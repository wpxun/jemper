{"categories":[{"title":"C","uri":"https://blog.jemper.cn/categories/c/"},{"title":"DevOps","uri":"https://blog.jemper.cn/categories/devops/"},{"title":"Docker","uri":"https://blog.jemper.cn/categories/docker/"},{"title":"Golang","uri":"https://blog.jemper.cn/categories/golang/"},{"title":"HTTP","uri":"https://blog.jemper.cn/categories/http/"},{"title":"云原生","uri":"https://blog.jemper.cn/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/"},{"title":"日记","uri":"https://blog.jemper.cn/categories/%E6%97%A5%E8%AE%B0/"}],"posts":[{"content":"图灵在数理逻辑方面的理论研究，间接地创造了机器指令设计的基本方法，计算机界最高奖项也因此称图灵奖；现代计算机之父冯诺依曼，于1945年提出了“存储程序通用电子计算方案”，提出了指令数据存储思想，并在 1951 年成功研制出了冯诺依曼结构的 IAS 计算机（现在计算机的原型机），奠定了现代计算机的微体系设计和程序结构设计。\n1 基础知识 1.1 冯诺依曼的“存储程序”思想 任何要计算完成的工作都要先被编写成程序，然后将程序和原始数据送入主存并启动。一旦程序被启动，计算机应能在不需操作的人员干预下，自己完成逐条取出指令和执行指令的任务。\n1.2 计算机系统抽象层 计算机就是一层层的抽象设计，比如 ISA 指令集体系结构是对硬件的抽象，机器语言、汇编语言、高级语言都是通过 ISA 使用硬件的。计算机专业课程基本上是围绕其中一个或多个抽象层开展教学，但从业人员要知道你所工作的层处在整个层次的哪一部分，跟哪些相关，要有整个的背景知识。 对程序员来说，硬件部分要理解微体系架构的实现方式，这部分我们放在第二部分讲；软件部分大致可以分为语言处理系统和操作系统这两大系统。\n1.3 语言处理系统 语言处理系统由“语言处理程序”和“语言运行时系统”组成：\n 语言处理程序：高级语言源程序 \u0026ndash;编译-\u0026gt; 汇编文本 \u0026ndash;汇编-\u0026gt; 目标二进制 \u0026ndash;链接-\u0026gt; 目标二进制。一般来说，编译有广义的编译（高级语言源代码转成机器语言）；也有狭义的编译（高级语言源代码转成汇编文本），这需要根据上下文理解就可以了，一般情况都是指狭义的编译。除了编译方式之外，还有解释方式运行的解释程序（如 PHP）。 要理解编译器、汇编器和链接器都是具有相应功能的软件，比如汇编器是将汇编语言翻译成硬件理解的二进制执行文件的软件。 语言运行时系统：GC、协程调度、优化、库函数、初始化等。  1.4 操作系统 汇编语言及以上层都通过操作系统抽象层使用指令集，操作系统主要由“人机接口”和“操作系统内核”组成：\n 人机接口：界面操作，GUI 操作和 CUI 操作 操作系统内核：输入输出等系统调用、虚拟内存和物理内存分配，进程和内核线程调度，多任务多用户等。  1.5 什么叫自举 初学者可能会误认为是鸡和蛋的问题，要理解自举，就是清楚编译器本身就是一个程序，它的功能就是把源代码翻译为计算机可执行的程序，它和被翻译的语言并没有关系。下面以 Go 语言作为例子：\n Go 语言：发明了 Go 语言，Go 源代码由 Go 程序和 Go 汇编（plan9 语法）组成； C 写的编译器程序：前期得用 C 语言写一个 Go 源程序的编译器，用来把 Go 源代码编译成汇编文本； Go 写的编译器程序：用 Go 写一个把 Go 源代码编译成汇编文本的程序，并用“步骤 2”生成的 C 编译器编译成可执行程序； 用 “Go 写的编译器程序” 去编译 “Go 源代码”，不再用“步骤 2”生成的 C 编译器去编译。  其中第 3 步就是实现了自举，因为编译器既是一个复杂的工程，也有一套成熟的评价体系，一般能实现自举的程序可以说是完备成熟的编程语言了。\n1.6 既简单又复杂的汇编 简单是因为汇编语言采用助记符来编写程序。复杂是因为汇编并不指代单一语言，相反，一种特定的汇编语言使用来自单一处理器指令集和操作数。因此，存在着多种汇编语言，每种都对应一类处理器。程序员可能会讨论 MIPS 汇编语言，也可能是 Intel x86 汇编语言。 总之，由于汇编语言是包含了特定处理器特性（如指令集、操作数寻址、寄存器数量和种类、寄存器存储的数值）的低级语言，因此存在着多种汇编语言。即使是同一 CPU，也可以采用不同的汇编语法并与之对应的汇编器，翻译成机器语言；\n汇编语言这一特性程序员造成的影响是显然的：当编程工作从一种处理器迁移到另一种处理器时，汇编语言程序员必须学习新的语言。不利的一面是，指令集、操作数类型、寄存器在不同的汇编语言中通常是不同的；有利的一面是，大多数汇编语言倾向于遵从相同的基本模式。因此，一旦程序员学会了一种汇编语言，就能够迅速学会其他汇编语言，并且通常是学习新的细节，而不是学习新的编程风格。一个了解了汇编语言基本范式的程序员可以快速学会新的汇编语言。\npep/9 汇编、plan9 汇编、x86 汇编、MIPS 汇编，相当于不同的语言，Go 用的就是 plan9 汇编。\n1.7 汇编与反汇编 汇编语言和机器语言基本上是一一对应的，将汇编语言编写的程序转化成机器语言的过程称为汇编；反之，机器语言程序转化成汇编语言程序的过程则称为反汇编。这一点和高级编程语言有很大不同，因此懂得汇编必然就懂得 CPU 指令集和内存的微体系架构。\n反汇编工具有 objdump、otool，与 objdump -Sl 能力接近的命令是 otool -tV，表示列出指令段。 对于 Go 自己提供了汇编与反汇编工具，\n go tool objdump \u0026lt;binary\u0026gt; 相当于 objdump \u0026lt;binary\u0026gt;、otool -t \u0026lt;binary\u0026gt;，不过输出格式都不一样； go tool compile -S \u0026lt;gofile\u0026gt; 是没有连接的汇编结果，地址可能会跟反汇编结果不太一样； Go 汇编和前两者其实是不同的，Go 汇编其实算是 Go 源代码，和前面两种格式也不尽相同。  1.8 函数与过程  术语“过程”或“子程序”指代一段可以被调用、执行计算并将控制权归还调用者的代码。术语“过程调用”或“子程序调用”指代了这样的调用过程。过程可以有参数。 术语“函数”指代返回以单一值为结果的过程，与过程类似，函数可以有参数。  所以说函数的概念隶属于过程。\n2 微体系架构 2.1 内存 首先有必要了解一下随机存取存储器（RAM） ，顾名思义，RAM 是针对随机（而不是顺序）访问进行优化的，可以直接认为随机访问过程是直接命中的；此外，RAM 提供读写功能，一般写入时间比读取时间要长得多，这需要选择合适的内存技术；最后 RAM 是易失性的，在计算机断电后，值不会持续存在。内存结构，有以下四种引脚\n VCC、GND：电源 A0-A9：地址信号，决定多少个地址编号，现在一般都是64位了 D0-D7：数据信号，决定一次可读取的数据，一直以来都是8位 RD、WR：控制信号   64位内存可以存储 2^64 个 1 字节的数据，所以最大容量为 2^64B 多于 1 字节的数据就要区分大小端了 寄存器的大小才是决定了数据是否要编写分割处理程序，比如寄存器是 64 位的，则大于8个字节的数据运算才需要编写分割处理程序 64 位一个指针就是 8 字节  2.2 寄存器 CPU 我们只认识寄存器就可以，其它的对编程来说相对透明：\n 一般叫法（不同CPU架构叫法可能不同）：程序计数器、累加寄存器、标志寄存器、指令寄存器和栈寄存器都只有一个，其它的一般有多个（基址寄存器、变址寄存器、通用寄存器都不止一个） 8088 有8个通用寄存器（适用于存储任意数据），x86也有8个，32位都在前面加了e(extended)，如果仅利用32位寄存器的低16位，此时只需把要指定的寄存器名开头的字母e去掉即可（但是plan9都寄存器不加前缀），amd64前面都加了R IR 指令寄存器是 CPU 内部使用，程序员无法通过程序对寄存器进行读写操作。  3 ISA 指令集 ISA 也正是介于软硬件的交界处，是高级语言和操作系统操作的抽象硬件（对硬件的抽象）。\n这里自行去查找不同的架构指令集即可（cat /proc/cpuinfo | grep flags | head -1） macOS 可以用 sysctl -a | grep machdep.cpu\n4 汇编 4.1 汇编基本范式 前面我们讲了，学汇编语言要学其基本范式，这部分建议学习《计算机体系结构精髓》，它是站在专业程序开发人员的角度进行讲解的。\n 汇编语句一般格式： label: opcode operand1, operand2, \u0026hellip; 操作数顺序，不同汇编，操作数顺序经常是不一样的 寄存器名称，由于寄存器是汇编语言编程的基础，每种汇编语言都提供了标识寄存器的方法。在一些语言中，专门的名字被保留；在另一些语言中，程序员可以为寄存器分配名字。以字母 r 开头并在其后跟随一至两位数字的名字会被保留，用于指代寄存器。有些语言 $10 指代寄存器 10；其他的汇编器则更灵活：允许程序员指定寄存器的名字。  listhd\tregister 6\t# 保存列表的起始地址 listptr\tregister 7\t# 在列表中移动  操作数类型，汇编语言需要为处理器支持的每种操作数类型提供的语法形式，包括寄存器引用、立即数(即一个常数)，以及对内存的间接引用  mov\tr2,r1 # 将寄存器1的内容复制到寄存器2 mov\tr2,(r1)\t# 将寄存器1的内容变作指针，指向一个内存，然后将该位置处的值拷贝到寄存器2。（todo）好像有些汇编用到方括号，具体作用有待查证。 movl eax, ebp+8\t# 有些汇编在 mov 后加 b、w、l、q 分别表示 1、2、4、8 个字节 mov eax, dword ptr [ebp+8]\t# 有些汇编用 dword 等表示长度  实现带参数的过程调用或函数调用  处理器使用内存中的栈保存参数，如 x86 Plan9 汇编 处理器使用寄存器传递参数，如 x86 C 汇编，前 6 个参数保存在寄存器，第7个开始保存在栈中 处理器使用专用的参数寄存器   变量和存储，一些汇编语言使用伪指令 .word 声明 16位存储空间，.long声明32位存储空间  4.2 调用规约 程序寄存器组是唯一能被所有函数共享的资源。虽然某一时刻只有一个函数在执行，但需保证当某个函数调用其他函数时，被调函数不会修改或覆盖主调函数稍后会使用到的寄存器值。因此，IA32采用一套统一的寄存器使用约定，所有函数(包括库函数)调用都必须遵守该约定。主要是要知道哪些寄存器是需要 caller-save，哪些是 callee-save，在编写汇编程序时应注意遵守惯例。\n根据惯例：\n 寄存器 %eax、%edx 和 %ecx 为主调函数保存寄存器(caller-saved registers)，当函数调用时，若主调函数希望保持这些寄存器的值，则必须在调用前显式地将其保存在栈中；被调函数可以覆盖这些寄存器，而不会破坏主调函数所需的数据。 寄存器 %ebx、%esi 和 %edi 为被调函数保存寄存器(callee-saved registers)，即被调函数在覆盖这些寄存器的值时，必须先将寄存器原值压入栈中保存起来，并在函数返回前从栈中恢复其原值，因为主调函数可能也在使用这些寄存器。此外，被调函数必须保持寄存器 %ebp 和 %esp，并在函数返回后将其恢复到调用前的值，亦即必须恢复主调函数的栈帧。  plan9 调用规约可以参见参考资料[6]。\n4.3 不同汇编的差异 程序员经常要在不同的汇编语言中切换调试，所以了解常见汇编语言的差异是很有必要的。不过这部分我也无法体系的讲，也找不比较全面的资料，只能凭经验列举一些差异，以后遇到不会感到迷惑。\n 操作数顺序问题，plan9 的汇编的操作数的方向是和 intel 汇编相反的，与 AT\u0026amp;T 类似，不过凡事总有例外，如果想了解这种意外，可以参见参考资料[1]。  MOVQ $0x10, AX ===== mov rax, 0x10 | |------------| | |------------------------|  下面是通用通用寄存器的名字在 IA64 和 plan9 中的对应关系：     IA64 RAX RBX RCX RDX RDI RSI RBP RSP R8 R9 R10 R11 R12 R13 R14 RIP     Plan9 AX BX CX DX DI SI BP SP R8 R9 R10 R11 R12 R13 R14 PC     intel 或 AT\u0026amp;T 汇编提供了 push 和 pop 指令族，plan9 中没有 push 和 pop，栈的调整是通过对硬件 SP 寄存器进行运算来实现的，例如:  subq $0x18, SP // 对 SP 做减法，为函数分配函数栈帧 ... // 省略无用代码 addq $0x18, SP // 对 SP 做加法，清除函数栈帧 4.4 C 语言实例 本次测试基于darwin系统(macOS)，amd64架构（intel i5），测试 C 程序如下：\nint AddNum(int a, int b) { return a + b; } int main() { int c; c = AddNum(123, 456); return 0; } 内存指令或数据是分段的，比如： _TEXT：指令的段定义 _DATA：是被初始化（有初始值）的数据段定义 _BSS：尚未初始化的数据的段定义\n我们主要关注指令段，即 _TEXT，对编译或者反汇编来说差异是不大的，实例分析我们就以编译结果进行分析。\n 编译结果如下：  adadeMacBook-Pro:debug ada$ cat main.s .section\t__TEXT,__text,regular,pure_instructions .build_version macos, 10, 14\tsdk_version 10, 14 .globl\t_AddNum ## -- Begin function AddNum .p2align\t4, 0x90 _AddNum: ## @AddNum .cfi_startproc ## %bb.0: pushq\t%rbp .cfi_def_cfa_offset 16 .cfi_offset %rbp, -16 movq\t%rsp, %rbp .cfi_def_cfa_register %rbp movl\t%edi, -4(%rbp) movl\t%esi, -8(%rbp) movl\t-4(%rbp), %esi addl\t-8(%rbp), %esi movl\t%esi, %eax popq\t%rbp retq .cfi_endproc ## -- End function .globl\t_main ## -- Begin function main .p2align\t4, 0x90 _main: ## @main .cfi_startproc ## %bb.0: pushq\t%rbp .cfi_def_cfa_offset 16 .cfi_offset %rbp, -16 movq\t%rsp, %rbp .cfi_def_cfa_register %rbp subq\t$16, %rsp movl\t$0, -4(%rbp) movl\t$123, %edi movl\t$456, %esi ## imm = 0x1C8 callq\t_AddNum xorl\t%esi, %esi movl\t%eax, -8(%rbp) movl\t%esi, %eax addq\t$16, %rsp popq\t%rbp retq .cfi_endproc ## -- End function 反汇编结果如下：  adadeMacBook-Pro:debug ada$ otool -tV a.out a.out: (__TEXT,__text) section _AddNum: 0000000100000f60\tpushq\t%rbp 0000000100000f61\tmovq\t%rsp, %rbp 0000000100000f64\tmovl\t%edi, -0x4(%rbp) 0000000100000f67\tmovl\t%esi, -0x8(%rbp) 0000000100000f6a\tmovl\t-0x4(%rbp), %esi 0000000100000f6d\taddl\t-0x8(%rbp), %esi 0000000100000f70\tmovl\t%esi, %eax 0000000100000f72\tpopq\t%rbp 0000000100000f73\tretq 0000000100000f74\tnopw\t%cs:(%rax,%rax) 0000000100000f7e\tnop _main: 0000000100000f80\tpushq\t%rbp 0000000100000f81\tmovq\t%rsp, %rbp 0000000100000f84\tsubq\t$0x10, %rsp 0000000100000f88\tmovl\t$0x0, -0x4(%rbp) 0000000100000f8f\tmovl\t$0x7b, %edi 0000000100000f94\tmovl\t$0x1c8, %esi 0000000100000f99\tcallq\t_AddNum 0000000100000f9e\txorl\t%esi, %esi 0000000100000fa0\tmovl\t%eax, -0x8(%rbp) 0000000100000fa3\tmovl\t%esi, %eax 0000000100000fa5\taddq\t$0x10, %rsp 0000000100000fa9\tpopq\t%rbp 0000000100000faa\tretq 4.5 实例分析 我们对编译结果进行简化，以便于我们分析。在 amd64 中，C 语言中 int 是 4 位，在 Go 中 int 是 8 位。\n_AddNum:\t#（编号2） pushq\t%rbp\t#（编号3） movq\t%rsp, %rbp\t#（编号4） movl\t%edi, -4(%rbp)\t# 在 5d0 之后把 123 压入栈 movl\t%esi, -8(%rbp)\t# 继续把 456 压入栈 movl\t-4(%rbp), %esi addl\t-8(%rbp), %esi movl\t%esi, %eax popq\t%rbp # (编号5) retq ## -- End function _main: pushq\t%rbp movq\t%rsp, %rbp\t#（编号0） subq\t$16, %rsp\t#（编号1） movl\t$0, -4(%rbp) movl\t$123, %edi movl\t$456, %esi callq\t_AddNum\t#（编号6） xorl\t%esi, %esi movl\t%eax, -8(%rbp) movl\t%esi, %eax addq\t$16, %rsp\t#（编号7） popq\t%rbp retq ## -- End function 接下来附上一张内存图及 rsp 和 rbp 的指针，图片的编号对应代码标识的编号，图中编号所在列表示编号中的代码执行后的状态。调试工具的使用请参见本文下一小节的介绍。 C 语言函数规约一般开头和结尾是固定的，那为什么要把 rbp 压入栈临时保存，并用 rbp 来代替 rsp 进行偏移呢？主要是 rsp 受 push、pop、subq、addq 等的影响，所以一般都用 rbp 来进行偏移。\n_main pushq\t%rbp\t;将 rbp 寄存器的值存入栈中 movq\t%rsp, %rbp\t;将 rsp 寄存器的值存入 rbp 寄存器 ...\t;函数操作 popq\t%rbp\t;读出栈中的数值存入 rbp 寄存器 retq\t;结束 main 函数，返回到调用源，返回 rbp 指令处 ## -- End function 系统调用就是按规定把该系统调用的数据存放到寄存器，然后执行系统调用就会去读取依赖的寄存器进行调用。 注意上面函数操作处，最后一定要进行栈清理处理，即把 esp 指回 ebp 那个单元，以便下一步 pop ebp 比如函数操作如下：\n\tsubq\t$16, %rsp\t;这里分配 16 个字节的内存 ...\t;加法操作 addq\t$16, %rsp\t;这里就是把 rsp 指回 push 数据之前（没用的数据并不会清空），也就是指向存放 rbp 的那个单元 4.6 lldb 调试汇编代码 最方便的是直接在 xcode 里调试，选中菜单 Debug/Debug Workflow/Always Show Disassembly，在 C 源码中设置断点。调试的时候要注意，像 step over 其实有三个可选项step over/step over instruction(hold Control)/step over thread(hold Control-Shift)，最好还是直接在 lldb 命令提示符中执行调试指令。对汇编的单步执行，要用 step over instruction 而非高级语言的 step over。下面列举一些常用的调试指令及其简写。\nstep over instruction/ni step into instruction/si register read/reg read thread list thread backtrace x/16xb 0x00007ffeefbff5f0-16 显示16个单元，每个单元占1个内存单元（g，1*8=8位），以16进制（x）显示，可以进行加减操作\n参考文献 [1] Go assembly language complementary reference. https://quasilyte.dev/blog/post/go-asm-complementary-reference/#external-resources [2] 矢泽久雄（日）. 程序是怎样跑起来的. 版次：2015年4月第1版 [3] J.Stanley Warford. 计算机系统 核心概念及软硬件实现. 版次：2019年1月第1版 [4] 南京大学 袁春风教授. 计算机系统基础(一)：程序的表示、转换与链接 https://www.icourse163.org/learn/NJU-1001625001?tid=1206622249#/learn/content?type=detail\u0026amp;id=1211390547\u0026amp;cid=1214042236\u0026amp;replay=true [5] 汇编 is so easy. https://github.com/cch123/asmshare/blob/master/layout.md [6] 调用规约. https://github.com/cch123/llp-trans/blob/master/part3/translation-details/function-calling-sequence/calling-convention.md [7] Douglas Comer(美). 计算机体系结构精髓. 版次：2019年6月第1版\n","id":0,"section":"posts","summary":"图灵在数理逻辑方面的理论研究，间接地创造了机器指令设计的基本方法，计算机界最高奖项也因此称图灵奖；现代计算机之父冯诺依曼，于1945年提出了","tags":["C","编程","Golang"],"title":"汇编语言","uri":"https://blog.jemper.cn/2019/08/assembly/","year":"2019"},{"content":"/usr/local/go/bin 下提供了三个命令 go、gofmt、godoc。主命令 Go 下分二级和三级子命令，本文档将迭代完善，争取列出所有功能点，目前还处于更新状态。\n1 go 二级子命令  go fmt 简单封装了gofmt，后者更多设置 go doc 是简单版的 godoc，后者可以做项目文档服务器 go get 获取网上包 go run 只能运行 Go 文件 go build 可以编译包或 Go 文件，对 Goland 来说表示编译加运行 go test 单元测试和性能测试 go install 安装到 $GOPATH/bin 目录下，无法指定到其它目录 go tool 放在第二部分讲解  常见的参数\n n print the command that would be executed but not execute it  2 go tool 三级子命令 go tool [-n] \u0026lt;command\u0026gt; 下又有很多命令，代码基本都在 /src/cmd 下，不加参数表示直接打印可以看到三级子命令：\n addr2line asm buildid cgo compile 这里会直接编译成 .o 文件，一般用 -S 打印出汇编文本 cover dist doc fix link 进行链接 nm objdump 反汇编 pack pprof 性能监控 test2json tour trace 和 -trace 一样的功能，进行竞争条件追踪，如果有竞争条件默认会输出到 stderr。 vet  参考文献 [1] Command go https://golang.org/cmd/go/\n","id":1,"section":"posts","summary":"/usr/local/go/bin 下提供了三个命令 go、gofmt、godoc。主命令 Go 下分二级和三级子命令，本文档将迭代完善，争取列出所有功能点，目前还处于更新状态。 1 go","tags":["Golang"],"title":"go command 命令","uri":"https://blog.jemper.cn/2019/08/go-command/","year":"2019"},{"content":"1 错误相关的概念  编译错误：build的时候就报错，由于考虑不周或输入错误导致程序异常（Exception），比如数组越界访问，除数为零，堆栈溢出等等。是大意疏忽。 运行错误：run的时候才报错，由于程序设计思路的错误导致程序异常或难以得到预期的效果。运行错误可以是预期的，也可以是不可预期的，对于可预期的不要用 panic，panic 恐慌机制是意料之外，如果不恢复（recover）就会导致宕机。**宕机（panic）**不是一件很好的事情，可能造成体验停止、服务中断，就像没有人希望在取钱时遇到 ATM 机蓝屏一样。但是，如果在损失发生时，程序没有因为宕机而停止，那么用户将会付出更大的代价，这种代价可以是金钱、时间甚至生命。因此宕机有时是一种合理的止损方法。 error错误：错误是业务过程的一部分，而异常不是。错误是可预期的结果，error错误机制是意料之中。  基于上面概念，我们可以把异常归为以下两种分类：\n Bug：不可预期，例如：不可预期的 panic，说不可预期本质上是粗心大意导致的 已知信息：可预期，error 错误（例如：网络连接断开、磁盘写入失败等）、可预期的 panic（recover 捕获变成 error，或者不捕获直接宕机）。在其它语言里，宕机往往以异常的形式存在。底层抛出异常，上层逻辑通过 try/catch 机制捕获异常，没有被捕获的严重异常会导致宕机，捕获的异常可以被忽略，上代码继续运行。 Go 没有异常系统，其使用 panic 触发宕机类似于其它语言的抛出异常，那么 recover 的宕机恢复机制就对应 try/catch 机制，能够通过 recover 捕获的原因在 panic() 函数前面已经运行过的 defer 语句依然会在宕机发生时发生作用，可以在 defer 内继续调用 panic，进一步将错误抛出。   错误转恐慌，比如程序逻辑上尝试请求某个 URL，最多尝试三次，尝试三次的过程中请求失败是错误，尝试完第三次还不成功的话，失败就被提升为恐慌宕机了。 恐慌转错误，比如 panic 触发的异常被 recover 恢复后，将返回值中 error 类型的变量进行赋值，以便上层函数继续走错误处理流程。  基本上 goland 能提示的错误都是编译错误，此时如果进行编译，就会得到以下的错误信息 # github.com/wpxun/client\n.\\main.go:45:20: cannot convert nima (type xx) to type string 在 Goland IDE 中一般会对输出做区分：一般 stdout 用黑色字体，stderr 用红色字体\n2 fmt 包 打印本质上就是把流写到 writer 对象上，并为了方便调用，提供了一系列函数。比如 Printf 并定制了 format 格式输出：\n %p 以地址的形式打印  var a int = 123 var b []int = []int{1,2} fmt.Printf(\u0026quot;%p, %p\u0026quot;, a, \u0026amp;a) // 前者不是地址无效，后者是 a 变量的地址 fmt.Printf(\u0026quot;%p, %p\u0026quot;, b, \u0026amp;b) // 前者为 b 内容的地址，后者为 b 变量的地址  %v、%#v，打印对象，%v 会去寻找 Stringer 接口，而 %#v 直接打印对象  type myStruct struct{ name1 int name map[int]string } func (myStruct) String () string { return \u0026quot;Stringer echo\u0026quot; } //打印结构体 var my myStruct = myStruct{1, map[int]string{1:\u0026quot;123\u0026quot;, 2:\u0026quot;345\u0026quot;}} fmt.Printf(\u0026quot;%#v，%v\u0026quot;, my, my) 3 log 包 log.Logger 结构体核心属性是三个：文件描识符、前缀和标签。默认初始化var std = New(os.Stderr, \u0026quot;\u0026quot;, LstdFlags)，就是对三个属性的设置，可以看出默认是错误输出，并已经设置默认的 flags。包的使用方式主要有两种：\n 使用默认的 std 变量，用于测试 使用自定义的 log.Logger 变量，调用 log.New 初始化，运用于产品  log 公开的函数或 Logger 方法都是调用了 Logger.Output() 方法，有两个参数：\n calldepth：输出文件名的时候指向的函数栈的深度，仅在 Llongfile or Lshortfile 被设置时才起作用；0 层表示 Logger.Output() 处的 runtime.Caller(calldepth)，1 层表示调用Output那一行，也就是当前行，2 层表示再往上一层，也就是调用输出的函数栈；一般都设置为指向当前调用行或其上一个栈，所以层层调用要算好栈的深度。 字符串，可以使用 fmt.Sprintf 进行格式化后再输入  4 经验总结  调用方具有更多关于正在运行的程序的上下文，并且可以做出关于如何处理错误的更明智的决定（比如错误发生3次才做处理），所以谁调用，谁处理；这样的原则有很多，比如 molloc/free集中在同一个函数，谁发起 goroutine，谁就保证该 goroutine 不会泄漏，谁生成了 channel，谁就负责关闭 channel，channel 重复关闭会引发恐慌。 一般以模块为边界设计 Error，不同边界（或者不同层）采用 wrap 的方式嵌套，将异常进行传递，直到传递到调用方进行处理，这样的好处是各层的异常信息都传递了，并且每一层还可以附加最明了的错误信息，这有两个好处：  明了的信息，这个明了的信息对用户友好，一般是“直接上层”传递过来的异常的友好说明，而不会再追究“上层以上的层”的错误信息 完整的错误链，因为嵌套的原因，所以可以把结构体输出却可   设计要异常传递的信息，不要等到最后才去优化，就如同“超时取消”的设计一样，应该在最开始的阶段就设计好 设计需要的信息：区分项目、区分日志类型，日志格式，用标准的语言描述：  发生了什么 发生在什么时间、什么位置 对用户友好的信息 告诉用户如何获得更多的信息    ","id":2,"section":"posts","summary":"1 错误相关的概念 编译错误：build的时候就报错，由于考虑不周或输入错误导致程序异常（Exception），比如数组越界访问，除数为零，堆栈","tags":["Golang"],"title":"go log 包","uri":"https://blog.jemper.cn/2019/08/go-log/","year":"2019"},{"content":"流量管理是 istio 最核心的问题，涉及 v1alpha3 中的配置资源，包括Gateway、VirtualSerice、DestinationRule 等。本文主要是基于 Istio 基础一文已经部署的实例来进行讲解。\n1 概述 对于入口流量管理，您可能会问： 为什么不直接使用 Kubernetes Ingress API ？ 原因是 Ingress API 无法表达 Istio 的路由需求。 Ingress 试图在不同的 HTTP 代理之间取一个公共的交集，因此只能支持最基本的 HTTP 路由，最终导致需要将代理的其他高级功能放入到注解（annotation）中，而注解的方式在多个代理之间是不兼容的，无法移植。\nIstio Gateway 通过将 L4-L6 配置与L7配置分离的方式克服了 Ingress 的这些缺点。 Gateway 只用于配置 L4-L6 功能（例如，对外公开的端口，TLS 配置），所有主流的L7代理均以统一的方式实现了这些功能。 然后，通过在 Gateway 上绑定 VirtualService 的方式，可以使用标准的 Istio 规则来控制进入 Gateway 的 HTTP 和 TCP 流量。\n所谓四层负载均衡就是使用IP加端口的方式进行路由转发；七层负载均衡一般是基于请求URL地址的方式进行代理转发。同理，还有基于MAC地址信息(虚拟MAC地址到真实MAC地址)进行转发的二层负载均衡和基于IP地址(虚拟IP到真实IP)的三层负载均衡。\n四层负载均衡具体实现方式为：通过报文中的IP地址和端口，再加上负载均衡设备所采用的负载均衡算法，最终确定选择后端哪台下游服务器。以TCP为例，客户端向负载均衡发送SYN请求建立第一次连接，通过配置的负载均衡算法选择一台后端服务器，并且将报文中的IP地址信息修改为后台服务器的IP地址信息，因此TCP三次握手连接是与后端服务器直接建立起来的。\n七层服务均衡在应用层选择服务器，只能先与负载均衡设备进行TCP连接，然后负载均衡设备再与后端服务器建立另外一条TCP连接通道。因此，七层设备在网络性能损耗会更多一些。\n从安全视角上： 四层负载均衡与服务器直接建立起TCP连接，很容易遭受SYN Flood攻击。SYN Flood是一种广为人知的DDoS（分布式拒绝服务攻击）的方式之一，这是一种利用TCP协议缺陷，发送大量伪造的TCP连接请求，从而使得被攻击方资源耗尽的攻击方式。从技术实现原理上可以看出，四层负载均衡很容易将垃圾流量转发至后台服务器，而七层设备则可以过滤这些恶意并清洗这些流量，但要求设备本身具备很强的抗DDOS流量的能力。\n2 Gateway 典型的网格将具有一个或多个负载均衡器（我们称之为网关），它们从外部网络终止TLS并允许流量进入网格。逻辑上相当于网格边缘的一个负载均衡器，用于接收和处理网格边缘出站和入站的网络连接，其中包括 TCP/TLS 等相关的配置内容。有两类网关\n 内部虚拟 Mesh 网关：代表网格内部的所有 Sidecar，换句话说：所有网格内部服务之间的互相通信，都是通过这个网关进行的。承载了所有 Kubernetes Service 的流量，只要是注射了 Istio 的服务均可。 自定义 Gateway 网关：如果要对外提供服务，就需要定义 Gateway 对象，并在 VirtualService 的 gateways 字段中进行赋值。流经用 selector(一般直接用 istio: ingressgateway) 指定的 Pods 流量，也就是只能通过 ingressgateway 服务 IP 访问的流量（这里是最终通过该 IP 访问的，可以设置域名，只要最终通过该 IP 即可）   selector：Istio Gateway 告诉 k8s 的 istio-ingressgateway pods 可以打开哪些主机和端口，它这是通过使用Kubernetes创造的标签选择器( label selector)模式来实现，以指定由哪些 Gateway Pods 来负责这个 Gateway 对象的运行。该网关要应用到的指定的 pods 上，通过 kubectl get po -n istio-system --show-labels 可以查看到 istio-ingressgateway-前缀的 pod 具有 app=istio-ingressgateway,chart=gateways,heritage=Tiller,istio=ingressgateway,pod-template-hash=5d8d989c76,release=istio 标签。 servers：通过 kubectl get svc -n istio-system -o wide，可以看到 istio-ingressgateway 是一个 LoadBalancer 类型（默认），映射了很多服务端口在宿主机比如： 15020:31966/TCP,80:31380/TCP,443:31390/TCP，以 80:31380/TCP 为例，宿主机:31380 -\u0026gt; 通过 kube-prox -\u0026gt; istio-ingressgateway CLUSTER-IP:80，它选择 app=istio-ingressgateway,istio=ingressgateway,release=istio，也就是选择到了 istio-ingressgateway-前缀的 pod port: 用来指定 istio-ingressgateway service 的某个端口开放给该 Gateway 资源  number: 这个就是服务的端口号，比如选择了 443，因为 443:31390/TCP，那么就可以在宿主机用 31390 访问到此服务 name: http protocol: HTTP   hosts: 数组，可以包含 *  3 VirtualService 3.1 理解 所有流量无论从哪里进来，从 Gateway 网关进来，从 Service 资源进来，都由 VirtualService 提供服务，VirtualService 通过 DestinationRule 转发到实际的 Service 里的的 Pod（Pod 标签匹配的 DestinationRule 设置的 subset 标签）。可以看出，流量进来后，不再直接把 Kubernetes service 放在前面，而是把 istio 的 VirtualService 放在前线提供服务。\n hosts: 数组，表示对哪些域名起作用，如果是 Gateway 网络进来的，需要指定 Gateway 服务端口号，不需要指定 pods 的服务端口号，如果不是 Gateway 进来的，需要指定 pods 所对应的服务端口号。 gateways: 把它绑定到某些网关下，如果没有指定，默认情况会绑定到 mesh 网关；一旦在 gateways 中填写了 mesh 之外的对象名称，就要继续对内部通信进行流量控制，并必须显式地将内置的 mesh 对象名称也加入列表中，但是注意，如果使用了 wildcard host * is not allowed for virtual services bound to the mesh gateway，而自定义网关没有此限制。 http: 指定7层网络，它不能指定服务的端口  match：http 层的匹配规则，比如 uri、headers、scheme、method、authority、端口、来源标签和 gateway 等，匹配方式有 exact、prefix 和 regex。 route：  destination：通过服务名用于筛选服务，通过版本或者端口号进行一点缩小目标  host：指定的服务名 port：  number：Endpoint Pod 端口   subset：指向 DestinationRule 配置     redirect：重定向，发生在客户端 rewrite：重写，rewrite 和 redirect 不能共存，它们不同之处在于，在 rewrite 方法的 match 一节必须包含对目标的定义   tcp: 指定4层网络，它可以指定服务的端口  VirtualService 是基于7层流量的，对指定的 hosts 有效；当然提供服务可以是 http、tcp 等\n3.2 实例 我们以下面的为例进行解释：\n[yhdodo19@instance-1 ~]$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE goapi NodePort 10.97.35.169 \u0026lt;none\u0026gt; 81:30001/TCP 30h goapisec NodePort 10.111.145.131 \u0026lt;none\u0026gt; 82:30002/TCP 30h [yhdodo19@instance-1 ~]$ kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) istio-ingressgateway LoadBalancer 10.99.215.11 \u0026lt;pending\u0026gt; 15020:31966/TCP,80:31380/TCP,443:31390/TCP,31400:31400/TCP,15029:32293/TCP,15030:30857/TCP,15031:30499/TCP,15032:31537/TCP,15443:32082/TCP 服务的映射关系可以直接替换访问，比如 10.99.217.250:443 等同于 10.142.0.3:31390、127.0.0.1:31390，可以直接替换\nGateway example-gateway 的 hosts 配置如下：\n hosts: - \u0026quot;*\u0026quot; 以该 Gateway 配置，以两种情况的 VirtualService 的理解它们之间的关系。\n3.2.1 配置一 VirtualService 的 hosts 配置如下：\n hosts: - goapi.default.svc.cluster.local - goapisec.default.svc.cluster.local - kube.jemper.cn gateways: - example-gateway - mesh 并准备以下命令语句：\nfor i in `seq 10`; do curl http://10.106.118.207:81/index; done for i in `seq 10`; do curl http://10.101.139.70:82/sec/index; done for i in `seq 10`; do curl http://goapi:81/index; done for i in `seq 10`; do curl http://goapisec:82/sec/index; done for i in `seq 10`; do curl http://10.99.217.250:443/index -H 'host: kube.jemper.cn'; done for i in `seq 10`; do curl http://10.99.217.250:443/sec/index -H 'host: kube.jemper.cn'; done  如果在网格外执行，前两个命令可以执行但没有匹配到 VirtualService；中间两个命令返回 “Could not resolve host”，后两个可以执行并匹配到 VirtualService。 如果在网格内执行，都可以执行，且都能匹配到 VirtualService，其中前两个 IP 地址访问时会经由某一控制解析为域名。  因此可以看出，Kubernetes Service 的 IP 在网格内被导向了域名，而域名因为在 VirtualService 配置所以受到其控制，所以体现出网格内外不一致的表现；而网格外不会被导向域名，所以不受其控制。\n3.2.1 配置二 VirtualService 的 hosts 配置如下：\n hosts: - “*” gateways: - example-gateway 并准备以下命令语句：\nfor i in `seq 10`; do curl http://10.106.118.207:81/index; done for i in `seq 10`; do curl http://10.101.139.70:82/sec/index; done for i in `seq 10`; do curl http://goapi:81/index; done for i in `seq 10`; do curl http://goapisec:82/sec/index; done for i in `seq 10`; do curl http://10.99.217.250:443/index; done for i in `seq 10`; do curl http://10.99.217.250:443/sec/index; done  因为没有 mesh 网关，没有通过 example-gateway 网关的流量都不会匹配 VirtualService。所以前面四个命令网格内外都匹配不到 VirtualService。 对后两个命令，在网格内外都能匹配到 VirtualService。  4 DestinationRule DestinationRule 是为 VirtualService 服务的，可以细分 Kubernetes Service 的 pods，以便一个 VirtualService 对应 Service 的多个版本。建议为每个网格都设置明确的目标访问规则。\n host：注意这里不是数组，是字符串，只能对单个 host 起作用，这里就是 Kubernetes Service 的域名了 subsets：这里是对上面 Kubernetes Service 的选择。 trafficPolicy：流量策略。在 DestinationRule 和 Subsets 两级中都可以定义 trafficPolicy，在 Subset 中设置的级别更高。  5 总结  Gateway： Istio Gateway是负责打开k8s上相关Istio的pods上的端口并接收主机的流量，是接收流量与路由之间的关键链接。L4-L6层配置，比如 TLS。Gateway 指向 Pods VirtualService： Istio VirtualService是“附加”到 Gateway 上的，并负责定义 Gateway 应实现的路由。可以将多个VirtualServices连接到Gateway，但不适用于同一个域。L7层配置，比如 CorsPolicy、HTTPRewrite、HTTPRedirect、HTTPMatchRequest 等。VirtualService 指向注射了 istio 的 Kubernetes Service。 DestinationRule 依赖于 Kubernetes 的 Service 和其关联的 pods，并服务于 VirtualService。由此可以，istio 对 pods 要求必须关联 Service，而无论其是否开放端口，DestinationRule 的 host 是依赖于 Kubernetes Service。 Gateway 的 hosts 是决定因素，VirtualService 的 hosts 是依赖于前者的。对于 Kubernetes 服务其 host 可以简写或全写比如：goapi、goapi.default.svc.cluster.local，建议写全。但访问的方式就更多了，比如 goapi、goapi.default、goapi.default.svc.cluster.local 都可以。  kubectl -n istio-system edit deployment istio-ingressgateway\nkubectl -n istio-system edit svc istio-ingressgateway\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml\nkubectl get pods \u0026ndash;all-namespaces -l app.kubernetes.io/name=ingress-nginx \u0026ndash;watch kubectl get pods -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx -o jsonpath=\u0026rsquo;{.items[0].metadata.name}\u0026rsquo;\nkubectl exec -it nginx-ingress-controller-5694ccb578-w6wn5 -n ingress-nginx \u0026ndash; /nginx-ingress-controller \u0026ndash;version\ncurl -v http://34.74.171.148 -H \u0026lsquo;host: kube.jemper.cn\u0026rsquo; curl -v http://35.227.70.245 -H \u0026lsquo;host: kube.jemper.cn\u0026rsquo; curl -v http://35.237.188.250 -H \u0026lsquo;host: kube.jemper.cn\u0026rsquo;\nfor i in seq 10; do curl http://35.237.188.250:30001/index; done for i in seq 10; do curl http://10.109.74.93:81/index; done for i in seq 10; do curl http://10.99.215.11/index -H \u0026lsquo;host: kube.jemper.cn\u0026rsquo;; done\ncurl -v http://35.237.188.250:30001/index curl -v http://10.109.74.93:81/index curl http://10.99.215.11:443/sec/index -H \u0026lsquo;host: kube.jemper.cn\u0026rsquo; curl http://10.99.215.11:443/index -H \u0026lsquo;host: kube.jemper.cn\u0026rsquo; curl http://10.142.0.2:31390/sec/index -H \u0026lsquo;host: kube.jemper.cn\u0026rsquo; curl http://10.99.215.11/index -H \u0026lsquo;host: kube.jemper.cn\u0026rsquo; for i in seq 10; do curl http://10.99.215.11:443/index -H \u0026lsquo;host: kube.jemper.cn\u0026rsquo;; done for i in seq 10; do curl http://10.99.215.11:443/sec/index -H \u0026lsquo;host: kube.jemper.cn\u0026rsquo;; done\nkubectl explain DestinationRule.spec \u0026ndash;api-version=networking.istio.io/v1alpha3\nscp -r -i /home/feixin10/.ssh/dodo /home/feixin10 yhdodo19@35.196.205.161:/home/yhdodo19\nfor i in seq 10; do curl http://35.237.188.250:30001/index; done for i in seq 10; do curl http://35.237.188.250:30002/sec/index; done\nfor i in seq 10; do curl http://10.97.35.169:81/index; done for i in seq 10; do curl http://10.111.145.131:82/sec/index; done\nfor i in seq 10; do curl http://goapi:81/index; done for i in seq 10; do curl http://goapisec:82/sec/index; done\nsudo tcpdump port 8080 -n sudo tcpdump -i eth0 src host 10.2.200.11 or dst host 10.2.200.11\nsudo tcpdump -i eth0 -s 80 -w /tmp/tcpdump.cap\n参考文献 [1] Istio v1aplha3 路由 API 介绍. https://istio.io/zh/blog/2018/v1alpha3-routing/ [2] Traffic Management. https://istio.io/docs/reference/config/networking/\n","id":3,"section":"posts","summary":"流量管理是 istio 最核心的问题，涉及 v1alpha3 中的配置资源，包括Gateway、VirtualSerice、DestinationRule 等。本文主要是基","tags":["Kubernetes","Service Mesh"],"title":"Istio 流量管理","uri":"https://blog.jemper.cn/2019/06/istio-traffic-management/","year":"2019"},{"content":"微服务是多进程、多服务部署，无法通过 IPC 进程内调用，必然通过网络调用，这将带来很多问题：不可靠、有带宽、协议设计。无论是 TCP、HTTP、RPC，无论是东西流量还是南北流量，涉及限流、熔断、域名及路径上下文，都需要 Kubernetes 或者第三方产品给出解决方案。网络是 Kubernetes 的难点之一。\n一、Kubernetes 自家解决方案 1 三个网络  Node Pod，同一Pod 可以和进程间 IPC 通信，可以直接用 localhost 访问同一 Pod 中的其它容器。但不同 Pods 采用了不同的虚拟 IP。共同点是都不需要 NAT 下通信，他们共享网络命名空间，他们之间的通信不需要宿主机的端口映射，Pod ip 对 Kubernetes 网络内部还是宿主机都是一样的。 Service，服务发现与负载均衡。ClusterIP 虚拟网络只对 Kubernetes 内部可见  以上三种网络是互不相交的。Kubernetes 并没有原生内置某种 Pod 网络实现，而是开放给了第三方厂商依据 CNI（Container Network Interface）规则接口实现。这是不同的容器接口可以调用相同的网络组件实现通信。 安装 Docker 容器后会有 /opt/cni/bin 目录。一般有三种实现方式：\n 二层交换 三层路由 Overlay 网络，在原有的网络上设计虚拟网络，实现解耦，但传输性能二法与二层和三层网络方案相比，实现上是分成 underlay 和 overlay 实现数据包地分发。 备注：via 是导向网络 Default via 192.168.1.1，选择下一跳；dev 是导向设备 10.1.0.0/16 dev bridge，进行分发。  以三层路由网络举例，有 node1 和 node2 节点，有以下两种方案：\n 方案一，网关路由：   Gateway(192.168.1.1): RouteTable(10.1.1.0/24 via 192.168.1.100, 10.1.2.0/24 via 192.168.1.101) node1(192.168.1.100/24): Container bridge(10.1.1.1/24), RouteTable(Default via 192.168.1.1) pod1: 10.1.1.2 pod2: 10.1.1.3 node2(192.168.1.101/24): Container bridge(10.1.2.1/24), RouteTable(Default via 192.168.1.1) pod1: 10.1.2.2 pod2: 10.1.2.3  方案二，主机路由：   Gateway(192.168.1.1): node1(192.168.1.100/24): Container bridge(10.1.1.1/24), RouteTable(Default via 192.168.1.1, 10.1.2.0/24 via 192.168.1.101) pod1: 10.1.1.2 pod2: 10.1.1.3 node2(192.168.1.101/24): Container bridge(10.1.2.1/24), RouteTable(Default via 192.168.1.1, 10.1.1.0/24 via 192.168.1.100) pod1: 10.1.2.2 pod2: 10.1.2.3  2 服务发现 为了承担起 DNS 解析任务，Kubernetes 环境 master 节点都会有一个 DNS 服务（一般多个 Pod），其 selector 为 k8s-app: kube-dns，\n[yhdodo19@instance-1 ~]$ kubectl get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP,9153/TCP 27d [yhdodo19@instance-1 ~]$ kubectl get po -n kube-system --show-labels | grep k8s-app=kube-dns coredns-fb8b8dccf-2rdns 1/1 Running 6 27d k8s-app=kube-dns,pod-template-hash=fb8b8dccf coredns-fb8b8dccf-nkqtv 1/1 Running 6 27d k8s-app=kube-dns,pod-template-hash=fb8b8dccf 3 Service 网络 3.1 NodePort 3.1.1 原理  创建 socket 对外监听，但要求监听端口大于 30000 基于 iptables 的流量转换，也可以认为是 socket 转发，通过 iptables Chain 实现 基于 iptables 的简单负载均衡，通过 iptables 概率实现  {% img http://img.jemper.cn/2019/06/nodeport.png 300 %}\n3.1.2 实现 外部请求 -\u0026gt; NodeIP:NodePort -\u0026gt; ClusterIP:Port -\u0026gt; PodIP:TargetPort，它是通过 kube-proxy 实现：\n kube-proxy 是以 daemonset pod 的形式运行在集群内的每个节点上，监听服务(Service)和端点(Endpoint)的变化设定和更新 iptables 规则 kube-proxy 通过 iptables 实现 nodePort 到集群内部 Service Port 再到 Pod 中的 Container targetPort 的流量转发 kube-proxy 是通过 iptables 实现 Service 流量转发到 Pod 的负载均衡  由以上的处理方式可见 NodePort 暴露端口方案是一个 4 层网络方案，无法处理 7 层网络，不能设置 80 等常用端口，它只能用来进行一些开发、测试工作，比如映射 3306:30001 进行数据操作或者数据迁移。\n3.1.3 分析 kube-proxy 是基于 iptable 来实现的，它是防火墙的一部分；Linux 防火墙可以对数据进行过滤、更改、转发操作，它由两个组件组成：核心层 netfilters 和用户层 iptables。 iptables 在用户层设置、变更和维护过滤规则，并最终由 netfilters 执行，iptables 主要由 一组 table 和 table 里的 Chain 组成，chain 有默认也可自定义。\n接下来我们以 外部请求 -\u0026gt; NodeIP:NodePort(35.237.188.250:30001) -\u0026gt; ClusterIP:Port(10.96.191.193:81) -\u0026gt; PodIP:TargetPort(10.36.0.10:80, ···) 来分析 iptables 是怎样实现转发和负载均衡的。\n[yhdodo19@instance-1 ~]$ kubectl describe svc goapi Name: goapi Namespace: default Labels: env=goapi Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026quot;apiVersion\u0026quot;:\u0026quot;v1\u0026quot;,\u0026quot;kind\u0026quot;:\u0026quot;Service\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;annotations\u0026quot;:{},\u0026quot;labels\u0026quot;:{\u0026quot;env\u0026quot;:\u0026quot;goapi\u0026quot;},\u0026quot;name\u0026quot;:\u0026quot;goapi\u0026quot;,\u0026quot;namespace\u0026quot;:\u0026quot;default\u0026quot;},\u0026quot;spec\u0026quot;:{\u0026quot;p... Selector: app=goweb Type: NodePort IP: 10.96.191.193 Port: http 81/TCP TargetPort: 80/TCP NodePort: http 30001/TCP Endpoints: 10.36.0.10:80,10.36.0.11:80,10.36.0.5:80 + 2 more... Session Affinity: None External Traffic Policy: Cluster Events: \u0026lt;none\u0026gt; [yhdodo19@instance-1 ~]$ sudo lsof -i tcp:30001 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME kube-prox 5307 root 21u IPv6 9386976 0t0 TCP *:pago-services1 (LISTEN) 可以看到 NodePort 就是 kube-proxy 创建并监听的，该服务一样有 5 个 Endpoints。接下执行命名 iptables -t nat -vnL --line-number \u0026gt; iptables-nat.txt 导出查看：\nChain PREROUTING (policy ACCEPT 34 packets, 3918 bytes) num pkts bytes target prot opt in out source destination 1 2796K 327M KUBE-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service portals */ Chain KUBE-SERVICES (2 references) 47 23 2744 KUBE-NODEPORTS all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL Chain KUBE-NODEPORTS (1 references) num pkts bytes target prot opt in out source destination 1 0 0 KUBE-MARK-MASQ tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* default/goapi:http */ tcp dpt:30001 2 0 0 KUBE-SVC-4MT5SRJPZGU2FACQ tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* default/goapi:http */ tcp dpt:30001 3 0 0 KUBE-MARK-MASQ tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* istio-system/istio-ingressgateway:tls */ tcp dpt:30614 4 0 0 KUBE-SVC-S4S242M2WNFIAT6Y tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* istio-system/istio-ingressgateway:tls */ tcp dpt:30614 5 0 0 KUBE-MARK-MASQ tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* default/web: */ tcp dpt:32063 6 0 0 KUBE-SVC-BIJGBSD4RZCCZX5R tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* default/web: */ tcp dpt:32063 7 0 0 KUBE-MARK-MASQ tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* istio-system/istio-ingressgateway:https-prometheus */ tcp dpt:32105 8 0 0 KUBE-SVC-VCO3RXEEJXVGNRLL tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* istio-system/istio-ingressgateway:https-prometheus */ tcp dpt:32105 9 0 0 KUBE-MARK-MASQ tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* istio-system/istio-ingressgateway:https-grafana */ tcp dpt:30523 10 0 0 KUBE-SVC-MZX34IYCYJRMNTMQ tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* istio-system/istio-ingressgateway:https-grafana */ tcp dpt:30523 11 0 0 KUBE-MARK-MASQ tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* istio-system/istio-ingressgateway:http2 */ tcp dpt:31380 12 0 0 KUBE-SVC-G6D3V5KS3PXPUEDS tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* istio-system/istio-ingressgateway:http2 */ tcp dpt:31380 13 0 0 KUBE-MARK-MASQ tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* istio-system/istio-ingressgateway:https */ tcp dpt:31390 14 0 0 KUBE-SVC-7N6LHPYFOVFT454K tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* istio-system/istio-ingressgateway:https */ tcp dpt:31390 15 0 0 KUBE-MARK-MASQ tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* istio-system/istio-ingressgateway:tcp */ tcp dpt:31400 16 0 0 KUBE-SVC-62L5C2KEOX6ICGVJ tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* istio-system/istio-ingressgateway:tcp */ tcp dpt:31400 17 0 0 KUBE-MARK-MASQ tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* istio-system/istio-ingressgateway:https-kiali */ tcp dpt:32227 18 0 0 KUBE-SVC-Y4Y3QMSBONEWNEDG tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* istio-system/istio-ingressgateway:https-kiali */ tcp dpt:32227 19 0 0 KUBE-MARK-MASQ tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* istio-system/istio-ingressgateway:status-port */ tcp dpt:30642 20 0 0 KUBE-SVC-TFRZ6Y6WOLX5SOWZ tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* istio-system/istio-ingressgateway:status-port */ tcp dpt:30642 21 0 0 KUBE-MARK-MASQ tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* istio-system/istio-ingressgateway:https-tracing */ tcp dpt:32308 22 0 0 KUBE-SVC-U67ZB3ILROLSW2OD tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* istio-system/istio-ingressgateway:https-tracing */ tcp dpt:32308 可以找到 KUBE-SVC-4MT5SRJPZGU2FACQ tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* default/goapi:http */ tcp dpt:30001， 继续找 KUBE-SVC-4MT5SRJPZGU2FACQ Chain\nChain KUBE-SERVICES (2 references) num pkts bytes target prot opt in out source destination 2 0 0 KUBE-SVC-4MT5SRJPZGU2FACQ tcp -- * * 0.0.0.0/0 10.96.191.193 /* default/goapi:http cluster IP */ tcp dpt:81 Chain KUBE-SVC-4MT5SRJPZGU2FACQ (2 references) num pkts bytes target prot opt in out source destination 1 0 0 KUBE-SEP-2GRZXFB4YOVSQMTA all -- * * 0.0.0.0/0 0.0.0.0/0 statistic mode random probability 0.20000000019 2 0 0 KUBE-SEP-FHKPE4W4K4BU4T6A all -- * * 0.0.0.0/0 0.0.0.0/0 statistic mode random probability 0.25000000000 3 0 0 KUBE-SEP-X6ZXAKDCXPAUMTPF all -- * * 0.0.0.0/0 0.0.0.0/0 statistic mode random probability 0.33332999982 4 0 0 KUBE-SEP-GXKTNAJ66D3R7K2Y all -- * * 0.0.0.0/0 0.0.0.0/0 statistic mode random probability 0.50000000000 5 0 0 KUBE-SEP-2JFIICOQDPEEFOFI all -- * * 0.0.0.0/0 0.0.0.0/0 以上 5 条规则对应了 5 个 Endpoints，并且每一条都有概率设置实现的负载均衡。到此分析结束了。\n3.2 LoadBalancer 需要第三方的负载均衡支持，以 GCE 为例，在“网络服务 -\u0026gt; 负载均衡”提供了三种类型的负载均衡：\n HTTP(S) 负载平衡：适用于 HTTP 和 HTTPS 应用的第 7 层负载平衡 TCP 负载平衡：适用于依赖 TCP/SSL 协议的应用的第 4 层负载平衡或代理 UDP 负载平衡：适用于依赖 UDP 协议的应用的第 4 层负载平衡  {% img http://img.jemper.cn/2019/06/loadbalancer.png 300 %}\n3.3 Ingress 我们知道 Service 的表现形式为 IP:Port，即工作在 TCP/IP 层，仅适用于依赖 TCP/SSL 协议的应用的第 4 层负载平衡或代理。而对于基于 HTTP 的服务来说，不同的 URL 地址经常对应到不同的后端服务或者虚拟服务器，这些应用的转发机制仅通过 Kubernetes 的 Service 机制是无法实现的。Kubernetes 提供了 Ingress 适用于 HTTP 和 HTTPS 应用的第 7 层负载平衡：\n 外部可访问 URL 负载均衡 SSL / TLS 基于域名的虚拟主机  其实就是实现了 nginx 的功能，更重要的是可以通过 ingress 配置文件直接控制 nginx。\n{% img http://img.jemper.cn/2019/06/ingress.png 300 %}\n2.3.1 Ingress Controller 控制器监听 Kubernetes API Ingress 资源的变化（反向代理和负载均衡），并实时的感知后端 service、pod 等变化（服务发现），对内置的反向代理进行设置和更新。 这里我们使用官方提供的 ingress-nginx-controller 进行实践，首先是安装 ingress-nginx-controller。只需要运行 kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml 但因为上面的配置文件没有开放 80、443 端口，所以必须得下载下来进行修改， 添加以下信息，添加 nodeSelector 是因为它是 deployment 部署 1 个副本，为了配合域名解析的稳定性所以固定某一个 node 节点上运行 ingress-nginx。\n... spec: hostNetwork: true nodeSelector: kubernetes.io/hostname: instance-3 ... kubectl apply -f mandatory.yaml，等待 pod 启动完成。\n3.3.2 Ingress 配置  ingress 配置的命名空间必须得在服务所在的空间，毕竟 ingress 的 backend 无法指定服务的命名空间。 如果域名和路径相同，则先部署的 ingress 优先。  apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-nginx spec: rules: - host: kube.jemper.cn http: paths: - backend: serviceName: goapi servicePort: 81 path: / --- 二、Istio 解决方案 Istio社区意识到了Ingress和Mesh内部配置割裂的问题，因此从0.8版本开始，社区采用了 Gateway 资源代替K8s Ingress来表示流量入口。\nIstio Gateway资源本身只能配置L4-L6的功能，例如暴露的端口，TLS设置等；但Gateway可以和绑定一个VirtualService，在VirtualService 中可以配置七层路由规则，这些七层路由规则包括根据按照服务版本对请求进行导流，故障注入，HTTP重定向，HTTP重写等所有Mesh内部支持的路由规则。\n三、API Gateway 解决方案 参考文献 [1] Kubernetes Ingress with Nginx Example. https://matthewpalmer.net/kubernetes-app-developer/articles/kubernetes-ingress-guide-nginx-example.html\n","id":4,"section":"posts","summary":"微服务是多进程、多服务部署，无法通过 IPC 进程内调用，必然通过网络调用，这将带来很多问题：不可靠、有带宽、协议设计。无论是 TCP、HTTP、RP","tags":["Kubernetes","Docker"],"title":"Kubernetes 网络","uri":"https://blog.jemper.cn/2019/06/kubernetes-network/","year":"2019"},{"content":"我选择 Istio 而非 Service Mesh 鼻祖 Linkerd 的原因，是 Istio 有大牌厂商支持、社区生态圈优势、重点支持 Kubernetes。\n1 遥测插件的远程访问 这里有很多种方法：\n 直接用 pod ip + pod port 去访问， 把 pod ip + pod port 映射到 istio 的 virtualservice 把 pod ip + pod port 映射到宿主机的端口  比如这些 Pod 内部开放的端口：Prometheus：9090、Grafana：3000、Kiali：20001、Tracing：80，后两者我们演示一下。\n1.1 istio istio-ingressgateway 服务开放了 15020:31966/TCP,80:31380/TCP,443:31390/TCP,31400:31400/TCP,15029:32293/TCP,15030:30857/TCP,15031:30499/TCP,15032:31537/TCP,15443:32082/TCP 这些端口到宿主机，可以新建 istio 资源进行访问，官方有文档“遥测插件的远程访问”\n1.2 port-forward 命令转发 比如 Prometheus： kubectl -n istio-system port-forward prometheus-d8d46c5b5-vb87x 9090:9090 \u0026amp; ssh -N -f -L 0.0.0.0:9091:127.0.0.1:9090 yhdodo19@0.0.0.0 -i ~/.ssh/googledodo http://35.237.188.250:9091\nGrafana： kubectl -n istio-system port-forward grafana-67c69bb567-qfzm5 3000:3000 \u0026amp; ssh -N -f -L 0.0.0.0:9092:127.0.0.1:3000 yhdodo19@0.0.0.0 -i ~/.ssh/googledodo http://35.237.188.250:9092\nkubectl -n istio-system port-forward istio-ingressgateway-5d8d989c76-cctpl 15000:15000 \u0026amp; ssh -N -f -L 0.0.0.0:9093:127.0.0.1:15000 yhdodo19@0.0.0.0 -i ~/.ssh/googledodo http://35.237.188.250:9093/listeners 查看该pod的监听器，默认只有 0.0.0.0:15090，比如我添加了 0.0.0.0:443，就可以接受 istio-ingressgateway svc 的 443 接口 http://35.237.188.250:9093/config_dump\nssh -N -f -L 0.0.0.0:80:127.0.0.1:31390 yhdodo19@0.0.0.0 -i ~/.ssh/googledodo\n{ \u0026quot;configs\u0026quot;:[ { \u0026quot;@type\u0026quot;:\u0026quot;type.googleapis.com/envoy.admin.v2alpha.BootstrapConfigDump\u0026quot;, \u0026quot;bootstrap\u0026quot;:Object{...}, \u0026quot;last_updated\u0026quot;:\u0026quot;2019-06-25T10:15:56.689Z\u0026quot; }, { \u0026quot;@type\u0026quot;:\u0026quot;type.googleapis.com/envoy.admin.v2alpha.ClustersConfigDump\u0026quot;, \u0026quot;version_info\u0026quot;:\u0026quot;2019-06-28T03:17:42Z/49\u0026quot;, \u0026quot;static_clusters\u0026quot;:Array[3], \u0026quot;dynamic_active_clusters\u0026quot;:Array[100] }, { \u0026quot;@type\u0026quot;:\u0026quot;type.googleapis.com/envoy.admin.v2alpha.ListenersConfigDump\u0026quot;, \u0026quot;version_info\u0026quot;:\u0026quot;2019-06-28T03:17:42Z/49\u0026quot;, \u0026quot;static_listeners\u0026quot;:Array[1], \u0026quot;dynamic_active_listeners\u0026quot;:Array[1] }, { \u0026quot;@type\u0026quot;:\u0026quot;type.googleapis.com/envoy.admin.v2alpha.RoutesConfigDump\u0026quot;, \u0026quot;static_route_configs\u0026quot;:Array[1], \u0026quot;dynamic_route_configs\u0026quot;:[ { \u0026quot;version_info\u0026quot;:\u0026quot;2019-06-28T03:17:42Z/49\u0026quot;, \u0026quot;route_config\u0026quot;:{ \u0026quot;name\u0026quot;:\u0026quot;http.443\u0026quot;, \u0026quot;virtual_hosts\u0026quot;:[ { \u0026quot;name\u0026quot;:\u0026quot;kube.jemper.cn:443\u0026quot;, \u0026quot;domains\u0026quot;:[ \u0026quot;kube.jemper.cn\u0026quot;, \u0026quot;kube.jemper.cn:443\u0026quot; ], \u0026quot;routes\u0026quot;:[ { \u0026quot;match\u0026quot;:{ \u0026quot;prefix\u0026quot;:\u0026quot;/sec\u0026quot; }, \u0026quot;route\u0026quot;:{ \u0026quot;cluster\u0026quot;:\u0026quot;outbound|82|v3|goapisec.default.svc.cluster.local\u0026quot;, \u0026quot;timeout\u0026quot;:\u0026quot;0s\u0026quot;, \u0026quot;retry_policy\u0026quot;:{ \u0026quot;retry_on\u0026quot;:\u0026quot;connect-failure,refused-stream,unavailable,cancelled,resource-exhausted,retriable-status-codes\u0026quot;, \u0026quot;num_retries\u0026quot;:2, \u0026quot;retry_host_predicate\u0026quot;:[ { \u0026quot;name\u0026quot;:\u0026quot;envoy.retry_host_predicates.previous_hosts\u0026quot; } ], \u0026quot;host_selection_retry_max_attempts\u0026quot;:\u0026quot;5\u0026quot;, \u0026quot;retriable_status_codes\u0026quot;:[ 503 ] }, \u0026quot;max_grpc_timeout\u0026quot;:\u0026quot;0s\u0026quot; }, \u0026quot;metadata\u0026quot;:{ \u0026quot;filter_metadata\u0026quot;:{ \u0026quot;istio\u0026quot;:{ \u0026quot;config\u0026quot;:\u0026quot;/apis/networking/v1alpha3/namespaces/default/virtual-service/goapi-default-xixi\u0026quot; } } }, \u0026quot;decorator\u0026quot;:{ \u0026quot;operation\u0026quot;:\u0026quot;goapisec.default.svc.cluster.local:82/sec*\u0026quot; }, \u0026quot;per_filter_config\u0026quot;:{ \u0026quot;mixer\u0026quot;:{ \u0026quot;disable_check_calls\u0026quot;:true, \u0026quot;forward_attributes\u0026quot;:{ \u0026quot;attributes\u0026quot;:{ \u0026quot;destination.service.host\u0026quot;:{ \u0026quot;string_value\u0026quot;:\u0026quot;goapisec.default.svc.cluster.local\u0026quot; }, \u0026quot;destination.service.uid\u0026quot;:{ \u0026quot;string_value\u0026quot;:\u0026quot;istio://default/services/goapisec\u0026quot; }, \u0026quot;destination.service.name\u0026quot;:{ \u0026quot;string_value\u0026quot;:\u0026quot;goapisec\u0026quot; }, \u0026quot;destination.service.namespace\u0026quot;:{ \u0026quot;string_value\u0026quot;:\u0026quot;default\u0026quot; } } }, \u0026quot;mixer_attributes\u0026quot;:{ \u0026quot;attributes\u0026quot;:{ \u0026quot;destination.service.host\u0026quot;:{ \u0026quot;string_value\u0026quot;:\u0026quot;goapisec.default.svc.cluster.local\u0026quot; }, \u0026quot;destination.service.uid\u0026quot;:{ \u0026quot;string_value\u0026quot;:\u0026quot;istio://default/services/goapisec\u0026quot; }, \u0026quot;destination.service.namespace\u0026quot;:{ \u0026quot;string_value\u0026quot;:\u0026quot;default\u0026quot; }, \u0026quot;destination.service.name\u0026quot;:{ \u0026quot;string_value\u0026quot;:\u0026quot;goapisec\u0026quot; } } } } } }, { \u0026quot;match\u0026quot;:{ \u0026quot;prefix\u0026quot;:\u0026quot;/\u0026quot; }, \u0026quot;route\u0026quot;:{ \u0026quot;cluster\u0026quot;:\u0026quot;outbound|81|v1|goapi.default.svc.cluster.local\u0026quot;, \u0026quot;timeout\u0026quot;:\u0026quot;0s\u0026quot;, \u0026quot;retry_policy\u0026quot;:{ \u0026quot;retry_on\u0026quot;:\u0026quot;connect-failure,refused-stream,unavailable,cancelled,resource-exhausted,retriable-status-codes\u0026quot;, \u0026quot;num_retries\u0026quot;:2, \u0026quot;retry_host_predicate\u0026quot;:[ { \u0026quot;name\u0026quot;:\u0026quot;envoy.retry_host_predicates.previous_hosts\u0026quot; } ], \u0026quot;host_selection_retry_max_attempts\u0026quot;:\u0026quot;5\u0026quot;, \u0026quot;retriable_status_codes\u0026quot;:[ 503 ] }, \u0026quot;max_grpc_timeout\u0026quot;:\u0026quot;0s\u0026quot; }, \u0026quot;metadata\u0026quot;:{ \u0026quot;filter_metadata\u0026quot;:{ \u0026quot;istio\u0026quot;:{ \u0026quot;config\u0026quot;:\u0026quot;/apis/networking/v1alpha3/namespaces/default/virtual-service/goapi-default-xixi\u0026quot; } } }, \u0026quot;decorator\u0026quot;:{ \u0026quot;operation\u0026quot;:\u0026quot;goapi.default.svc.cluster.local:81/*\u0026quot; }, \u0026quot;per_filter_config\u0026quot;:{ \u0026quot;mixer\u0026quot;:{ \u0026quot;disable_check_calls\u0026quot;:true, \u0026quot;forward_attributes\u0026quot;:{ \u0026quot;attributes\u0026quot;:{ \u0026quot;destination.service.uid\u0026quot;:{ \u0026quot;string_value\u0026quot;:\u0026quot;istio://default/services/goapi\u0026quot; }, \u0026quot;destination.service.host\u0026quot;:{ \u0026quot;string_value\u0026quot;:\u0026quot;goapi.default.svc.cluster.local\u0026quot; }, \u0026quot;destination.service.namespace\u0026quot;:{ \u0026quot;string_value\u0026quot;:\u0026quot;default\u0026quot; }, \u0026quot;destination.service.name\u0026quot;:{ \u0026quot;string_value\u0026quot;:\u0026quot;goapi\u0026quot; } } }, \u0026quot;mixer_attributes\u0026quot;:{ \u0026quot;attributes\u0026quot;:{ \u0026quot;destination.service.namespace\u0026quot;:{ \u0026quot;string_value\u0026quot;:\u0026quot;default\u0026quot; }, \u0026quot;destination.service.name\u0026quot;:{ \u0026quot;string_value\u0026quot;:\u0026quot;goapi\u0026quot; }, \u0026quot;destination.service.host\u0026quot;:{ \u0026quot;string_value\u0026quot;:\u0026quot;goapi.default.svc.cluster.local\u0026quot; }, \u0026quot;destination.service.uid\u0026quot;:{ \u0026quot;string_value\u0026quot;:\u0026quot;istio://default/services/goapi\u0026quot; } } } } } } ] } ], \u0026quot;validate_clusters\u0026quot;:false }, \u0026quot;last_updated\u0026quot;:\u0026quot;2019-06-28T03:17:42.772Z\u0026quot; } ] } ] } 参考文献 [1] 崔秀龙. 深入浅出 Istio | Service Mesh 快速入门与实践. 版次：2019年3月第1版 [2] 杨章显. Service Mesh 实战 | 基于 Linkerd 和 Kubernetes 的微服务实践. 版次：2019年1月第1版\n","id":5,"section":"posts","summary":"我选择 Istio 而非 Service Mesh 鼻祖 Linkerd 的原因，是 Istio 有大牌厂商支持、社区生态圈优势、重点支持 Kubernetes。 1 遥测插件的远程访问 这里有很多种方法： 直接用","tags":["Kubernetes","Service Mesh"],"title":"Istio Prometheus","uri":"https://blog.jemper.cn/2019/05/istio-prometheus/","year":"2019"},{"content":"我选择 Istio 而非 Service Mesh 鼻祖 Linkerd 的原因，是 Istio 有大牌厂商支持、社区生态圈优势、重点支持 Kubernetes。\n1 为什么需要 Service Mesh 有了 Kubernetes 的 Service，为什么还需要 Service Mesh ？相信这是初学者最大的疑问。\n微服务架构解决了单体应用的很多老问题，但同时也带来了一些新问题：\n 对部署和运维的自动化要求更高 对网络这一不可靠的基础设施依赖增强 调用链路变长 日志分散，跟踪和分析难度加大 服务分散，受攻击面积更广 跨服务控制协调能力要求更高 自动伸缩、路由管理、存储共享等  为了解决微服务架构产生的一些问题，以 Kubernetes 为代表的容器云系统出现了。这类容器云系统以容器技术为基础，在进程级别为微服务提供了一致的部署、调度、伸缩、监控、日志等功能。 然而，除了进程本身的问题，微服务之间的通信和联系更加复杂，其中的观测、控制和服务质量保障等都成为微服务方案的短板，因此随着 Kubernetes 成为事实标准，Service Mesh 顺势登场。\n一言蔽之：就是单体应用问题很多，用微服务架构来解决，但微服务架构实施部署难度加大，借助 Kubernetes 实现了，尔后发现 Kubernetes 只是解决了大部分问题，还有遗留的问题，于是第三方（Buoyant 公司，也就是 Linkerd 的开发商）在新的层次里（Kubernetes 层次外，也就是 Service Mesh）间接地解决。\n2 安装 2.1 工具准备 其安装都是直接复制命令程序即可：\n helm 工具安装 istio 工具安装  Istio 的安装目前有两种：快速安装和 Helm 安装。其原理都一样：\n 注册 crds 资源，需要生成“资源清单”后用 apply 打入 kubernetes 部署 Istio 组件，需要生成“组件清单”后用 apply 打入 kubernetes  Helm 是目前 Istio 官方推荐的安装方式，不过功能性学习时可以选择快速安装。\n2.2 方式1 - 快速安装  安装 Istio 定义的资源 Custom Resource Definitions (CRDs) for i in install/kubernetes/helm/istio-init/files/crd*yaml; do kubectl apply -f $i; done 我们可以看到，Istio v1.1.7 创建了 4 种 apiVersion，可以通过类似 /apis | grep 'istio.io' API 查看   authentication.istio.io/v1alpha1 rbac.istio.io/v1alpha1 config.istio.io/v1alpha2 networking.istio.io/v1alpha3  全部 53 个 Istio CRD 被提交到 Kubernetes api-server（如果你启用了 cert-manager,那么 CRD 的数目为58个），可以使用 kubectl get crds 查看所有的 CRD。需要注意的是，v1.2.0 版本已经调整成 23 个。\n 部署 Istio 核心组件 选择一个 配置文件，接着部署与你选择的配置文件相对应的 Istio 的核心组件，比如没有 tls 的 istio-demo.yaml 配置文件： kubectl apply -f install/kubernetes/istio-demo.yaml 我们看到，除了常见的 Deployment、Service、Configmap、ServiceAccount 等 Kubernetes 对象，这里还创建了各种 Istio CRD 的下属资源。\n  卸载\n  kubectl delete -f install/kubernetes/istio-demo.yaml for i in install/kubernetes/helm/istio-init/files/crd*yaml; do kubectl delete -f $i; done 2.3 方式2 - Helm 安装 这里我们推荐 helm template 安装。\n核心资源在 install/kubernetes/helm 目录中。\n  为 Istio 组件创建命名空间 istio-system： $ kubectl create namespace istio-system\n  使用 kubectl apply 安装所有的 Istio CRD，命令执行之后，会隔一段时间才能被 Kubernetes API Server 收到，并查询确保全部 53个（如果你启用了 cert-manager,那么 CRD 的数目为58个,在 install/kubernetes/helm/istio-init/values.yaml 配置）Istio CRD 被提交到 Kubernetes api-server： $ helm template install/kubernetes/helm/istio-init --name istio-init --namespace istio-system | kubectl apply -f - $ kubectl get crds | grep 'istio.io\\|certmanager.k8s.io' | wc -l\n  选择一个 配置文件，接着部署与你选择的配置文件相对应的 Istio 的核心组件，我们建议在生成环境部署中使用 default 配置文件: $ helm template install/kubernetes/helm/istio --name istio --namespace istio-system --values install/kubernetes/helm/istio/values-istio-demo.yaml --set gateways.istio-ingressgateway.type=NodePort | kubectl apply -f - 你可以添加一个或多个 --set \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; 来进一步自定义 helm 命令的安装选项，另外对于缺少 LoadBalancer 支持的平台，执行下面的安装步骤时，可以在 Helm 命令中加入 \u0026ndash;set gateways.istio-ingressgateway.type=NodePort 选项，使用 NodePort 来替代 LoadBalancer 服务类型。 但是推荐在配置文件中进行配置。 命令行参数解释：\n   \u0026ndash;name istio：代表生成的部署内容的基础名称为“istio”，比如 pod “istio-grafana-post-install-1.1.7-c4n6t”； \u0026ndash;namespace istio-system：代表将 Istio 部署到命名空间 istio-system 中； \u0026ndash;values(-f) values-istio-demo.yaml：代表从 values-istio-demo.yaml 文件中获取输入的内容，一般我们不直接修改原 values.yaml 文件，而是通过新增 yaml 文件为各种场景下的关键配置提供范本。  卸载   卸载组件 $ helm template install/kubernetes/helm/istio --name istio --namespace istio-system | kubectl delete -f - $ kubectl delete namespace istio-system 删除 CRD 和 Istio 配置 kubectl delete -f install/kubernetes/helm/istio-init/files  3 部署应用前准备 3.1 把 Envoy 容器注入 Pod 3.1.1 自动注入 配置在 install/kubernetes/helm/istio/values.yaml 中：\nautoInject: enabled sidecarInjectorWebhook: enabled: true replicaCount: 1 image: sidecar_injector enableNamespaceByDefault: false 需要同时满足以下三个条件才会开启自动注入功能：\n sidecarInjectorWebhook.enabled：总控制开头，为 true 表示开启自动注入特性，这里一定要设置为 true enableNamespaceByDefault：为 true 表示为所有的命令空间开启自动注入功能；为 false 表示只为标签 istio-injection: enabled 的命名空间才会开启自动注入。 autoInject：命名有点歧义，它其实是跟 pod 的 sidecar.istio.io/inject 的注解（annotations，注意不是标签）有关，以下两种情况会自动注入：   enabled + !sidecar.istio.io/inject: false disable + sidecar.istio.io/inject: true  因为 enableNamespaceByDefault 设置为 false，所以想要自动注入的命令空间都要打上 istio-injection=enabled，这样才满足条件2： kubectl label ns default istio-injection=enabled\n3.1.1 手动注入 如果目标命名空间中没有打上 istio-injection 标签， 可以使用 istioctl kube-inject 命令，在部署之前手工把 Envoy 容器注入到应用 Pod 之中\n3.2 Istio 对 Pod 和服务的要求 要成为服务网格的一部分，Kubernetes 集群中的 Pod 和服务必须满足以下几个要求：\n  需要给端口正确命名：服务端口必须进行命名。端口名称只允许是\u0026lt;协议\u0026gt;[-\u0026lt;后缀\u0026gt;-]模式，其中\u0026lt;协议\u0026gt;部分可选择范围包括 grpc、http、http2、https、mongo、redis、tcp、tls 以及 udp，Istio 可以通过对这些协议的支持来提供路由能力。例如 name: http2-foo 和 name: http 都是有效的端口名，但 name: http2foo 就是无效的。如果没有给端口进行命名，或者命名没有使用指定前缀，那么这一端口的流量就会被视为普通 TCP 流量（除非显式的用 Protocol: UDP 声明该端口是 UDP 端口）。\n  Pod 端口: Pod 必须包含每个容器将监听的明确端口列表。在每个端口的容器规范中使用 containerPort。任何未列出的端口都将绕过 Istio Proxy。\n  关联服务：Pod 不论是否公开端口，为了满足服务发现的需要，都必须关联到至少一个 Kubernetes 服务上，如果一个 Pod 属于多个服务，这些服务不能在同一端口上使用不同协议，例如 HTTP 和 TCP。\n  Deployment 应带有 app 以及 version 标签：在使用 Kubernetes Deployment 进行 Pod 部署的时候，建议显式的为 Deployment 加上 app 以及 version 标签。每个 Deployment 都应该有一个有意义的 app 标签和一个用于标识 Deployment 版本的 version 标签。app 标签在分布式追踪的过程中会被用来加入上下文信息。Istio 还会用 app 和 version 标签来给遥测指标数据加入上下文信息。\n  Application UID：不要使用 ID（UID）值为 1337 的用户来运行应用。\n  NET_ADMIN 功能: 如果您的集群中实施了 Pod 安全策略，除非您使用 Istio CNI 插件，您的 pod 必须具有NET_ADMIN功能。请参阅必需的 Pod 功能。\n  4 部署应用 在 Istio 中部署业务应用时，建议做到以下几点：\n 使用 app 标签表明应用身份 使用 version 标签表明应用版本 创建目标规则 创建默认路由规则  4.1 启动应用 本实例在 kubernetes 资源一文中延伸，不同之处在于应用资源时需要注入 sidecar，前面我们分析过了可以是手动注入，也可以是自动注入；\n 手动注入：istioctl kube-inject -f gorpc-kube.yaml | kubectl apply -f - 自动注入：kubectl apply -f gorpc-kube.yaml  4.2 创建目标规则和默认路由规则 gorpc-istio.yaml 放在 github 项目上 配置清单内容纲目：\n DestinationRule, goapi.default DestinationRule, goapisec.default VirtualService, goapisec.default -\u0026gt; v3, prefix: /sec; goapi.default -\u0026gt; v1, prefix: /; Gateway, number: 443, hosts: - \u0026ldquo;*.jemper.cn\u0026rdquo;  直接部署即可：kubectl apply -f gorpc-istio.yaml，这些资源没有注入的概念。\n4.3 访问服务 注意要点：\n Service 对象中的 Port 部分必须以“协议名”为前缀，目前支持的协议名包括 http、http2、mongo、redis 和 grpc。Istio 会根据这些命名来确定为这些端口提供什么样的服务，不符合命名规范的端口会被当作 TCP 服务，其功能支持范围会大幅缩小。 另外官方建议为 Pod 模板加入两个标签：app 和 version。 Istio 的注入要求：没有 Service 的 Deployment 是无法被 Istio 发现并进行操作的。为了满足服务发现的需要，所有的 Pod 都必须有关联的服务，如果没有 Service 则容器只能启动一个 DNS 解析是在网格内，外部流量无法解析服务名 Istio 用于控制网格内的访问，不能控制外部流量的访问。在容器内使用服务 IP 访问受到 Istio 控制，在宿主机使用服务 IP 进行访问不受 Istio 控制。 注入只会对部分资源起作用，比如 Deployment 等，对 Service、DestinationRule、VirtualService 等没有注入操作  所以应该要创建一个在网格内的客户端应用来访问服务。 for i in seq 10; do curl http://goapi:81/index; done for i in seq 10; do curl http://10.102.19.38:81/index; done for i in seq 10; do curl http://35.237.188.250:30001/index; done\n4.4 总结 总的来说，可以把上述两部分合成一条命令：\n 手动注入：istioctl kube-inject -f gorpc-kube.yaml | kubectl apply -f gorpc-istio.yaml -f - 自动注入：kubectl apply -f gorpc-istio.yaml -f gorpc-kube.yaml  删除应用：kubectl delete -f gorpc-istio.yaml -f gorpc-kube.yaml\n参考文献 [1] 崔秀龙. 深入浅出 Istio | Service Mesh 快速入门与实践. 版次：2019年3月第1版 [2] 杨章显. Service Mesh 实战 | 基于 Linkerd 和 Kubernetes 的微服务实践. 版次：2019年1月第1版\n","id":6,"section":"posts","summary":"我选择 Istio 而非 Service Mesh 鼻祖 Linkerd 的原因，是 Istio 有大牌厂商支持、社区生态圈优势、重点支持 Kubernetes。 1 为什么需要 Service Mesh 有了 Kubernetes 的 Service，为什","tags":["Kubernetes","Service Mesh"],"title":"Istio 基础","uri":"https://blog.jemper.cn/2019/05/istio/","year":"2019"},{"content":"这是一个架构概念爆发的时代，是架构本身的复杂度已经开始超越业务逻辑本身的时代，同时也是越来越接近 DevOps 工作方式的时代。\n1 互联网架构变迁 从历史观来看，一般是用更优的方案代替旧的方案，但同时引来新的问题，在新的方案完善直接解决或者在方案外新的层次里间接地解决问题，直到下一个更优的方案出现。架构地发展也是如此，互联网架构从集中式架构（单体应用）到分布式架构（服务化），再到云原生架构（微服务）变迁，架构的升级的目的是追求更高的质量和效率。\n1.1 集中式架构 集中式架构又称为单体式架构，B/S架构的后端系统大都采用该架构，分为标准的三层：\n Web 层：也就是典型的 MVC 服务层：用于对应用业务逻辑进行处理，整个系统的核心 数据访问层：实现对真实数据库的访问，面向对象的域模型 ORM，如 MyBatis  存在的问题：\n 单体应用代码库庞大，不易于理解和修改 持续部署困难，耦合性强 扩展应用困难，只能从一个维度进行扩展，即增加实例副本提供处理能力，即只能通过增加服务器的配置有限度地提升系统的处理能力，这种伸缩方式被称为垂直伸缩；另一方面，单体应用各个组件对资源的使用情况需求不同，一些是 CPU 密集型，另一些是内存密集型，但是不能独立地扩展单个组件 阻碍快速开发，团队需要花更多的时间在部门间协调和统一功能特性 迫使开发人员长期专注于一种技术栈 Cattle vs Cat 争论中，服务器的特点是有功能、有个性、难替换的 Cat  1.2 分布式架构 分布式架构解决了集中式架构中的问题，实现了服务器的水平伸缩，但也产生了新的问题：\n 服务部署：部署基于微服务的应用也要复杂得多，当前，云计算、云原生技术地快速发展比如容器、Kubernetes 技术使得开发和落地微服务更加便捷，从这一点看，运行成本是越来越低的 服务间通信的管理，因为网络不可靠、不安全、延迟丢包等客观存在的事实让服务间通信变得复杂，另外当一个来自 Web 的查询触发了几十个跨越了很多不同机器的内部远程过程调用时（HTTP 或 RPC），我们如何能达到最低的延迟呢？ 分区的数据库体系和分布式事务的管理，不同服务可能拥有不同的数据库。CAP 原理的约束，使得我们不得不放弃传统的强一致性，而转而追求最终一致性，这个对开发人员来说是一个挑战。 微服务架构对测试也带来了很大的挑战。传统的单体 Web 应用只需测试单一的 REST API 即可，而对微服务进行测试，需要启动它依赖的所有其他服务。这种复杂性不可低估。 对服务的依赖设计需要参考一些成熟的设计原则，如：SRP（单一职责原则）、OCP（开闭原则）、LSP（里氏替换原则）、ISP（接口隔离原则）、DIP（依赖反转原则） 服务发现机制 日志分散严重，跟踪和分析难度加大  随着分布式架构的普及，越来越多的互联网公司在重新审视一个并不崭新但却一直难于落地的概念，那就是面向服务架构（SOA）。一些大企业的分布式框架也相继出现，服务发现、负载均衡、失效转移、动态扩容、数据分片、调用链路监控等分布式系统的核心功能也一个个趋于成熟。 随着应用的规模越来越大，服务器的数量也急速增加，运维工程师越来越难以远程登录每一台服务器云进行管理，包括流程管理（搭建环境、部署应用、服务器伸缩等）和监控管理（查看服务状态、日志跟踪等）。自动化运维工具主要包括两大类：流程自动化工具和监控自动化工具；于是成堆的自动化运维工具就出来了。此时运维和开发的鸿沟越来越明显，急需自动化运维体系与开发技术体系配合（DevOps）。\n备注：分布式系统可以理解为并发系统，不同服务器进程空间的并行系统。像集群和分布式都是并行系统的概念，都是多台机器部署，而且都是为了实现一个业务功能。这里有一个简单的区分标准：一般我们将同一个程序单元用负载的方式将多台独立的服务器通过网络连接组合成一个有效的整体对外提供服务称为集群，而将多程序单元（将业务拆分成多个子业务）通过网络连接组合成一个有效的整体对外提供服务称为分布式。与之对应，单进程空间的并发复杂性可以看我另一篇文章《并发的复杂性》。\n1.3 云原生架构 1.3.1 微服务 容器的出现，使原有的基于虚拟机的云主机应用，彻底转变为更加灵活和轻量的“容器+编排调度”的云平台应用。基于容器的微服务架构解决了单体应用的很多老问题，提供了更细粒度的服务化落地方案，但同时服务化的很多问题还是存在或者产生了新的问题(微服务本质上也是服务化)：\n 对部署和运维的自动化要求更高 对网络这一不可靠的基础设施依赖增强 调用链路变长 日志分散，跟踪和分析难度加大 服务分散，受攻击面积更广 在不同的服务之间存在协作关系，对跨服务控制协调能力要求更高 自动伸缩、路由管理、故障控制、存储共享等  1.3.2 云原生平台 为了解决微服务架构产生的一些问题，以 Kubernetes 为代表的容器云系统出现了。这类容器云系统以容器技术为基础，在进程级别为微服务提供了一致的部署、调度、伸缩、监控、日志等功能。Cattle vs Cat 争论中，服务器的特点转变为有功能、无个性、可替代的 Cattle。\n1.3.3 后 Kubernetes 时代 对多数企业而言架构本身的复杂度已经开始超越业务逻辑本身，如果不加以统一管理与规划，那么只是维护成本就已经很高了。在 Kubernetes 成为事实的标准之后，围绕 Kubernetes 未解决的问题，第三方机构基于 Kubernetes 相继开发相应的中间件完善。 比如，除了进程本身的问题，微服务之间的通信和联系更加复杂，其中的观测、控制和服务质量保障等都成为微服务方案的短板，因此随着 Kubernetes 成为事实标准，Service Mesh 顺势登场。它让分布式应用无须关注服务（全球）发现与路由、限流、降级、熔断、安全等通用问题。\n我们看到 CNCF 云原生计算基金会 提供了很多开源软件栈致力于大型微服务快速应用部署，国内企业也纷纷加入该组织，并在国内对微服务架构落地提供强大的支持。CNCF 提供的软件栈未来会成为企业软件栈的趋势。像 Kubernetes、Container、Linkerd、gRPC、etcd 都榜上有名，从下图可以了解整个软件栈，其中标注 CNCF 的项目来自于 CNCF。 1.4 一言蔽之 单体应用问题很多，用微服务架构来解决，但微服务架构实施部署难度加大，借助 Kubernetes 实现了，尔后发现 Kubernetes 只是解决了大部分问题，还有遗留的问题；比如在服务间通信上，第三方（Buoyant 公司，也就是 Linkerd 的开发商）在新的层次里（Kubernetes 层次外，也就是 Service Mesh）间接地解决。\n2 微服务 我们先从下面的两种说法来理解微服务和面向服务的架构（SOA）的区别：\n 将微服务架构称为 SOA2.0 或 SOA++。 就像认为 XP 或者 Scrum 是敏捷软件开发的一种特定方法一样，你也可以认为微服务架构是 SOA 的一种特定方法。  微服务化是不同的程序单元（子系统）互相协作形成完整的大系统对外服务，微服务化更多是指业务框架的拆分，将单体应用的进程内调用变为服务间的网络通信，意义在于解耦，而非指部署方式（虽然不同子系统一般是采用分布式部署）。微服务本质上等同于服务化，只是强调了服务化拆分的粒度或程度——“微”，比如大型电商网站都会拆分出首页、用户、搜索、广告、购物、订单、商品、收益结算、日志埋点等子系统。\nMartin Fowler 写于 2014 年的著名文章 《Microservices》，对微服务做出了纲领性的定义，总结了微服务应该具备的特点，如下所述：\n 在结构上，将原有的从技术角度拆分的组件，升级为从业务角度拆分的独立运行的服务，这些服务具备各自的实现平台，并且独占自有数据，在服务之间以智能端点和哑管道的方式通信。 在工程上，从产品而非项目的角度进行设计，强调迭代、自动化和面向故障的设计方法。  下面可能是目前或未来炙热的微服务架构： 3 服务治理 服务如何拆分，粒度如何把控，以及服务与服务之间的 RPC 调用应该如何实现，这些问题解决之后，就要思考大规模服务化之前应该如何实施服务治理。 服务治理并不是一个统一的概念，主要包括服务发现、负载均衡、限流、熔断、超时、重试、服务追踪等。可以用城市治理的思路来思考服务治理，\n 请求网关：就是整个整体的守门人， 信息采集：日志采集，追踪工具，服务注册发现都是用来采集信息的，常用的工具有：   ElasticSearch，虽然是一个搜索引擎和分析框架，但因为提供很好的存储和查询性能，所以经常用于日志的采集和存储 logstash，Elastic的日志分析工具  信息分析：监控平台来展现这些采集的信息，并进行监控和分析，常用的工具有：   Kibana，Elastic的可视化插件，可以配合Elastic使用可视化查询日志 grafna，时序性分析工具，提供漂亮的图形化界面 Promethues，强大系统监控和报警框架，提供多维度数据模型，灵活强大的查询语句，有多种可视化图形界面  治理策略：根据分析的结果采取治理策略，有的服务快撑不住了要限流，有的服务坏了要熔断，并且还能够及时的调整这些服务的配置。  当然有的书提到服务治理的三个基础：\n 服务的动态注册与发现； 服务的扩容评估； 服务的升降级处理；  其中第一点就是信息采集，后面两点就是治理策略。目前已经有很多成熟的服务治理解决方案，它们是保证微服务顺利实施的中流砥柱。在云原生的浪潮中，服务治理更多情况下与容器调度平台结合，共同形成一站式的自动化调度治理平台。无论是否使用基于容器的调度系统，服务治理的原理和范畴都不会发生改变，只是实现方式不同而已。\n4 事务 只有落地的数据才需要考虑事务，从简到繁，先讨论单库事务，再考虑分布式事务；我们进行微服务化的时候，一般会同步进行分库分表操作，分表还是单库事务，常规数据库事务就可以解决，分库就产生分布式事务了。比如文章系统，把用户中心（注册、登录等）、积分系统和发布内容拆分成微服务，同时也进行分库（用户库、积分库和内容库）；用户注册成功时写入用户库后异步进行用户积分初始化（如果这一步比较慢或者出错），并马上返回注册成功给用户，用户登录后发布内容，服务器发现用户积分没有初始化完成而报错，这就是分布式事务不一致的例子。\n4.1 数据库事务 这里是指单个数据库事务，数据库事务的几个特性：原子性(Atomicity )、一致性( Consistency )、隔离性或独立性( Isolation)和持久性(Durabilily)，简称就是ACID。数据库是由两个文件组成的，一个数据库文件和一个日志文件，通常情况下，日志文件都要比数据库文件大很多，数据库进行任何写入操作的时候都是要先写日志的；同样的道理，我们在执行事务的时候数据库首先会记录下这个事务的redo操作日志，然后才开始真正操作数据库，在操作之前首先会把日志文件写入磁盘，那么当突然断电的时候，即使操作没有完成，在重新启动数据库时候，数据库会根据当前数据的情况进行undo回滚或者是redo前滚，这样就保证了数据的强一致性。\n4.2 分布式事务 分布式事务主要还是要达到最终一致性，分布式事务要解决的问题是如何使多次操作，对外部看起来是一个整体的操作。本质上来说，分布式事务就是为了保证不同数据库的数据一致性。区别于单库，在单库中使用本地事务保证数据库的一致性已经习以为常，但在分布式环境下处理一致性较复杂。 要保证BASE理论是对CAP中的一致性和可用性进行一个权衡的结果，理论的核心思想就是：我们无法做到强一致，但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。\n5 中间件 非底层操作系统软件、非业务应用软件，不是直接给最终用户使用的，不能直接给客户带来价值的软件统称为中间件。消息中间件关注于数据的发送和接收，利用高效、可靠的异步消息传递机制集成分布式系统。中间件是一种独立的系统软件或服务程序，分布式应用系统借助这种软件在不同的技术之间共享资源，管理计算资源和网络通信。中间件为开发者提供了公用于所有环境的应用程序接口当在应用程序中嵌入其函数调用时，它便可利用其运行的特定操作系统和网络环境的功能，为应用执行通信功能。 目前中间件的各类有很多，比如交易管理中间件（IBM 的 CICS）、Web 应用服务器中间件、消息中间件。\n业界通常有两种方式来实现系统间通信，其中一种是 RPC 调用，一种是基于消息队列的方式。消息队列是异步处理，用于解耦、削峰、日志收集、事务最终一致性等问题。\n6 调优 分布式系统应对大流量和高并发，业界多年的技术及经验总结，已经提出很多有助于提高系统可用性和弹性的通用技术指南或模式，通常做法如通过扩容、动静分离、缓存、服务降级和限流五种常规手段来保护系统的稳定运行。归纳为纵向优化和横向优化，扩容相当集群，是横向优化。而动静分享、缓存、服务降级和限流则是纵向优化，本质上一层一层过滤流量。\n6.1 扩容 或者叫扩展，中央基础设施的设计主要有两种扩展方法：\n 水平扩展：分布式并发模式，比如 NoSQL 数据库、分析处理系统、消息队列都使用水平扩展  MapReduce 模式 容错模式，比如 redis 的主从复制保证容错   垂直扩展：在单台计算机中安装更多内存或提高处理能力  6.2 动静分离 静态数据缓存在 CDN 上，或者只是通过 nginx 代理，而不走程序处理。 CDN：用户的请求并不会直接落到 ICP (ICP备案的企业的数据中心)，而是请求到离用户最近的 ISP（互联网服务提供商）上，因此可以大幅提升系统整体的响应速度，CDN 是一种廉价的加速方式。\n6.3 缓存 读请求尽量在缓存中命中\n6.4 消峰和限流   消峰不是通过技术手段而是通过在业务上做调整，本质上就是降低高并发，比如活动分时间段进行，通过答题验证等。12306 都在这方面做了处理，比如去程是早上9点抢，回程是下午14点抢，并加了复杂验证码过滤掉一部分用户。\n  限速节流限定服务在固定的时间内只处理一定数量的请求，确保系统有足够的能力优雅地处理其他请求，可避免峰值流量导致系统崩溃，与第三方系统集成时可以提供安全保障。好比高峰期地铁站的流量管理，排队进站。限流第一保证满负荷运行，第二保证有序运行。数据库的写服务一般就要进行限流控制负载压力。限流的方案：\n   计数器算法：比如池化技术（数据库连接池、redis连接池、线程池、对象池等），长连接和控制并发环境下的连接数使得流量在能够承载的最大上限内，保证系统不被击垮。 令牌桶算法：桶的容量固定不变，控制均衡流入量，放开流出量；nginx 限流就是采用此方式。 漏桶算法：桶的容量固定不变，控制均衡流出量，放开流入量，和令牌桶算法相反。原理上只要保证一个方向是均衡的即可。  6.5 超时、重试和熔断  超时（Timeout）使得如果访问下游服务缓慢或失败时，上游服务应快速失败而不是无限或者长时间等待，以此避免级联故障，隔离故障范围。 重试（Retry）有效地解决访问服务时发生的间隙性故障，有助于减少服务恢复时间。 熔断（Circuit Breaker）机制避免将请求继续发送给已经失败或者不健康的下游服务处理，而是等待它们恢复，避免级联故障。  参考文献 [1] Susan J. Fowler. Microservices in Production. 版次：2016年09月第1版 [2] 张亮 吴晟 敖小剑 宋净超. 未来架构 | 从服务化到云原生. 版次：2019年4月第1版 [3] Robert C.Martin, 孙宇聪（译）. 架构整洁之道. 2018年9月第1版 [4] 高翔龙. 人人都是架构师 分布式系统架构落地与瓶颈突破 版次：2017年5月第1版 [5] 微服务 https://www.cnblogs.com/liuning8023/p/4493156.html 原文作者：Martin Fowler（马丁·福勒） [6] 聊聊分布式事务，再说说解决方案 https://www.cnblogs.com/savorboard/p/distributed-system-transaction-consistency.html 杨晓东 2017-10-17 [7] 分布式事务 CAP 理解论证 解决方案 https://blog.csdn.net/weixin_40533111/article/details/85069536 小太阳 2018年12月28日 [8] 事务隔离级别浅析 http://geyifan.cn/2016/07/17/talk-about-transaction/ 葛一凡 2016-07-17 [9] 微服务并非Spring Cloud和Dubbo，下一代微服务是什么？https://cloud.tencent.com/info/71a0b4783039214f7f633b3c5269451f.html [10] Spring Cloud 微服务架构学习笔记与示例 http://www.likecs.com/default/index/show?id=32356 [11] 当我们在说微服务治理的时候究竟在说什么 https://www.jianshu.com/p/dd818114ab4b [12] 后 Kubernetes 时代的微服务 https://www.infoq.cn/article/microservices-post-kubernetes\n","id":7,"section":"posts","summary":"这是一个架构概念爆发的时代，是架构本身的复杂度已经开始超越业务逻辑本身的时代，同时也是越来越接近 DevOps 工作方式的时代。 1 互联网架构变迁 从历史观来","tags":["架构","并发","Kubernetes"],"title":"微服务架构","uri":"https://blog.jemper.cn/2019/05/microservice-architecture-difficult/","year":"2019"},{"content":"最近在思考计算机的发展方向，它们跟现实世界的关系是紧密的、有迹可循的，架构设计过程中理应从社会活动中得到启示。\n组件 世界上千千万万的事件，电视、空调、保湿杯、书桌、手机、房子、玩具等，它们通常都是一个完整的个体，但是他们本质上就是由各个组件组装形成的，大部分企业将它组装好再出售，比如将空调组装成一个整体，并和安装安调的服务二次组装整体出售；而像宜家、电商大件商品偏向提供所有零部件，让用户自行安装。太多的太多都可以看出组装的身影，只有极少的东西才是一体的比如玻璃杯、陶瓷、水果，这些东西看不出组装的痕迹，我只能把它们归类到不可拆分的东西部分。\n但因为计算机世界是依据于现实世界而存在或设计的，其设计理念也必然适用这种现象，我们的库组件，模块组件，微服务的服务组件都是往这方面发展，这两种组件的区别只是组件的调用方式，但组件化越来越为人看好，而不是单体程序。\n文件 蒙昧时代跨入文明时代的象征一般认为两个：金属工具的作用和文字的发明。文字其实就是知识的记载工具，文字未发明之前只能靠记忆，后来人类用绳子打结来记录一件事件，但这种记录本质上也是记忆。而记忆的缺点就是无法传承、数据量有限、容易出错。\n在计算机世界里，人类和计算机交流的协议就是类似人类与人类交流的文字。协议是双方通信的一个约定，比如传过来的一段字符流数据，其中第1个字节表示什么，第2个字节表示什么，类似这样的约定。编程语言是协议，通信要协议（TCP是二进制协议，HTTP是文本协议），运行要协议。然而只有协议还不够，就如同只有文字还不够一样，还要知道怎么用这些协议去编写程序，描述操作，就如同用文字描述论文、描述科技、描述知识一样；以配置一个软件为例，先要安装，要点这个确定那个确定，要修改配置，要运行，要开机运行，要日志输出配置等，因此开发人员就要按照经验一步步输入指令操作（当然可以自己记录成流程文档，以后复制执行），但其本质上还是没有逃脱记忆的处理方式，于是 unix 提出了“一切皆文件”，Plan9 计划更是按照这一标准设计，到现在大大小小的工具软件都提出了这一要求，就是文件化。发展到现在，有一个词就是：xx即代码；基础设施即代码(vagrantfile)、流水线即代码(jenkinsfile)、依赖即代码(glide)、容器部署即代码(dockerfile)、Kubernetes 资源配置文件，这些努力无非就是记忆电子化，变成一篇一篇的“论文”，不需要再依靠人的记忆；总之所有的流水线操作都应该往自动化方向发展。\n限制 法国大革命口号是：“自由、平等、博爱”。孙中山的三民主义：“民族、民权、民生”。都有交织的地方，一般来说民权强调自由，民生着重平等；人类社会中有相对的平等，但不可能有绝对的自由，统治管理的目的就是“限制”人的自由，法律条文本质上也是对“限制“做出明文规定。以驾驶证来说，没有驾驶证限制你驾车，驾驶证的车型限制你开的车的类型，斑马线限制你走的路线。如果没有这些限制，那就是原始社会了。历来中国素被称为“礼乐之邦”，“礼”就是规范人的行为；发展到现代文明，对人的限制变得宽松——更自由，限制的政策不歧视——更平等，但限制一直存在。\n计算机也是由规则组成，否则无法与计算机交流，所谓规则也是限制。架构就是限制，限制源码位置，限制依赖，限制通信方式等；三种编程范式也是限制，它限制我们的控制流和数据流：结构化编程限制了控制权的直接转移，面向对象编程限制了控制权的间接转移，函数式编程限制了赋值。\n能集中控制的要集中，需分散配置的要分散 ","id":8,"section":"posts","summary":"最近在思考计算机的发展方向，它们跟现实世界的关系是紧密的、有迹可循的，架构设计过程中理应从社会活动中得到启示。 组件 世界上千千万万的事件，电视","tags":null,"title":"日记——从现实世界看计算机世界（2）","uri":"https://blog.jemper.cn/2019/05/hello-world/","year":"2019"},{"content":"本文重点理解 Kubernetes 资源的作用，并用一个简单的例子进行演示。\n1 版本、组和资源 1.1 版本 版本是 Kubernetes 管理资源的一种策略，这种策略的目的是为了更好地进化与调整 API，从而间接地升级资源。选择在API级别进行版本化，而不是在资源或字段级别进行版本化，以确保API提供清晰，一致的系统资源和行为视图，并控制对已废止的API和/或实验性API的访问。 JSON和Protobuf序列化模式遵循架构更改的相同准则。为了使删除字段或者重构资源表示更加容易，Kubernetes 支持多个API版本。每一个版本都在不同API路径下，例如 /api/v1 或者 /apis/extensions/v1beta1。 同一组的优先规则：v2 \u0026gt; v1 \u0026gt; v1beta2 \u0026gt; v1beta1 \u0026gt; v2alpha1 \u0026gt; v1alpha1，目前来说 v2 版本基本都还处于 alpha 阶段。 相关文档：\n Overview of the API for Kubernetes Kubernetes API Versions 1.14 也可以通过 kubectl api-versions 查看  1.2 组 为了更容易地扩展Kubernetes API，我们实现了API组。 API组在REST路径和序列化对象的 apiVersion 字段中指定。目前有几个API组正在使用中：\n 核心组（通常被称为遗留组）位于REST路径 /api/v1 并使用 apiVersion：v1。 指定的组位于REST路径 /apis/$GROUP_NAME/$VERSION，并使用 apiVersion：$GROUP_NAME/$VERSION（例如 apiVersion：batch/v1）。 在Kubernetes API参考中可以看到支持的API组的完整列表。  1.3 资源 首先预览下 Kubernetes 的整个资源对象框架： {% img http://img.jemper.cn/2019/05/kubernetes.png 600 %} 最底层是容器引擎如 Docker 或 rkt，然后是基于容器引擎创建的容器，其次 Pod 是 Kubernetes 的基础，其它资源都是 Pod 更高层次的封装，以此提供管理的便捷性及满足各种应用需求。所有支持的资源可通过 kubectl api-resources 查看。\n资源（Resource）是一个 API 集合的抽象，自定义一种新资源就等于是对 Kubernetes API 的扩展，定义之后可以使用 kubectl 直接操作，这是 Kubernetes 扩展性的一个体现。\n1.4 通过 API 调用分析 我们通过 curl -k \u0026ndash;header \u0026ldquo;Authorization: Bearer [value]\u0026rdquo; https://192.168.1.11:6443 方式查看相关的信息\n /version：查看 Kubernetes 版本 /api：查看核心组，其实它并没有组名，且只有 v1 一个版本 /apis：查看 apis 下的组列表（APIGroupList），每个组可以有多个版本，如 apps/v1、apps/v1beta2、apps/v1beta1，但要注意，一个资源理论上只属于一个组，但因为历史迁移的原因，可以还会遗留在旧的组里，比如 deployment 资源就同时属于 apps/v1、apps/v1beta2、apps/v1beta1、extensions/v1beta1，即是同时属于 apps 和 extensions 两个组，当然这种情况下应该用 apps/v1 这个组和版本。 /apis/apps/v1：查看这个组版本里的资源列表(APIResourceList)，及其所用的动词等信息 /apis/apps/v1/deployments：查看创建的 deployments 列表（DeploymentList），相当于 kubectl get deploy --all-namespaces --output=yaml、kubectl describe deploy nginx-deploy 命令，注意需要创建一个 ClusterRoleBinding 才有权访问 /apis | grep \u0026lsquo;istio.io\u0026rsquo;，查看 istio 创建的 CRD（Custom Resource Definitions）  1.5 基于不同版本或不同组创建同一资源 基于多个版本创建的 kind: Deployment 资源： apiVersion: apps/v1\n kubectl create -f nginx-deploy.yaml deployment.apps/nginx-deployment created\n apiVersion: apps/v1beta2\n kubectl create -f nginx-deploy.yaml deployment.apps/nginx-zhijun created\n apiVersion: extensions/v1beta1\n kubectl create -f nginx-deploy.yaml deployment.extensions/nginx-shangyue created\n 以资源来说并没有区别。\n2 深入理解资源 有了以上的基础，我们再来剖析一些重要的资源\n2.1 Pod Pod 是最基本、最小的部署单元，由一个或一组容器组成，所有这些容器共享存储资源，共享唯一 IP 地址以及包含如何运行容器的信息。\n2.2 RelicaSet Pod 被设计为最小的部署单元，但是由于 Pod 不具备自愈能力，所以不建议单独使用 Pod。为此，Kubernetes 通过提供更高层级的 Controller，使用 Controller 创建和管理多个 Pod 副本，并提供集群级别的自愈能力。Kubernetes 提供多种 Controller 以满足不同需求，ReplicaSet 就是其中一种，与其类似的 Replication Controller 因无法支持基于集合操作的 selector，故推荐使用 ReplicaSet。\n使用命令 kubectl run 创建\n2.3 Deployment Deployment Controller 为 Pod 和 ReplicaSet 提供声明式更新，可以定义期望状态，副本数，速率，应用场景：\n 定义 Deployment 创建的 Pod 和 ReplicaSet，开始会自动创建一个 ReplicaSet 用于自愈 Pod 的能力 滚动升级和回滚 扩容和缩容 暂停和继续 Deployment  创建方式：\n 用命令 kubectl run 快速创建 定义 Manifest 文件  2.4 Service Service 解决了 Pod、ReplicaSet 和 Deployment 没有解决的服务发现和负载等问题，因为 Pod 是没有状态的，可以被自动创建和销毁的，其不稳定性导致不能用来提供对外的服务。为了解决这些问题，Kubernetes 引入了一个新的对象 Service。它为运行相同服务的一系列 Pod 提供一个静态 IP 地址 ClusterIP 和端口，该 ClusterIP 和端口作为访问后端 Pod 的唯一不变入口。当然，也可以创建没有 ClusterIP 的服务，该服务称为 Headless Service，但发现 Headless Service 需要自己实现。 Service 通过 selector 与后端 Pod 进行关联，无论后端 Pod 怎么变化，Service 均会感知后端变化。\n创建方式：\n 用命令 kubectl expose 快速创建 定义 Manifest 文件  Service 定义中的关键字段是 ports 和 selector。selector 定义部分设置的是后端 Pod 所拥有的 label；ports 有几个端口数据：\n port：要暴露到 Service 的端口，以便通过 Service 的虚拟机 IP 地址和虚拟端口号访问到他们 targetPort：Service 关联的 Pod 的端口，如果没指定则会用 port 值；clusterIP=None 时会被忽略 nodePort：要暴露到宿主机的端口，需要指定 Service.spec.type=NodePort，将 Pod 或 Service 的端口号映射到宿主机，以使得客户端应用能够通过物理机访问容器应用。  从以上内容可以看出，targetPort(Pod) \u0026lt;-\u0026gt; port(Service) \u0026lt;-\u0026gt; nodePort(Node) 或者 containerPort(Pod) \u0026lt;-\u0026gt; hostPort(Node)，前者为 Services，后者为 Pods。\n3 标签和标签选择器 Events: Type Reason Age From Message\n Normal SuccessfulCreate 19m replicaset-controller Created pod: nginx-rs-wpxun-gzqhs Normal SuccessfulCreate 19m replicaset-controller Created pod: nginx-rs-wpxun-7d9sx Normal SuccessfulCreate 19m replicaset-controller Created pod: nginx-rs-wpxun-nwrxq Normal SuccessfulDelete 2s (x9 over 14m) replicaset-controller Deleted pod: nginx3\nEvents: Type Reason Age From Message\n Normal ScalingReplicaSet 10m (x7 over 69m) deployment-controller Scaled up replica set nginx-deploy-57cd6466f7 to 5 Normal ScalingReplicaSet 3m54s deployment-controller Scaled up replica set nginx-deploy-6f699d5b64 to 5 Normal ScalingReplicaSet 2m38s deployment-controller Scaled down replica set nginx-deploy-6f699d5b64 to 0 Normal ScalingReplicaSet 33s deployment-controller Scaled down replica set nginx-rs-wpxun to 0\nEvents记录的是对子资源的操作，包括创建、删除、收缩等，手动创建的 po 如果 Deployment 的 SELECTOR 和 ReplicaSet 的 LABELS 关联，不会删除失联的 ReplicaSet， ReplicaSet 的 SELECTOR 和 Pod 的 LABELS 关联，直接创建（非 rs 或 deploy 自动创建）的 Pod 会被收编，会对 Pod 进行增删，不过在 kubectl describe po rs_name 并不会显示那些收编的 Pod，只会显示由 rs_name 自动创建的 Pod SELECTOR 逗号表示 AND，必须满足 SELECTOR 的条件才能关联； Pod 的 NodeSelector 表示在哪个节点部署，如果找不到节点，则 Pod 处于 Pending 状态\n4 一个简单的例子 本实例和 Docker Stack 多服务采用类似的服务程序，不过增加了一个 RPC 服务。\n goapi：前端页面服务，代码位于 wpxun/blog/tree/master/gorpcweb，镜像已经推送到 DockerHub gorpcsvc：提供了一个服务给 gorpcweb 进行基于 HTTP 的 RPC 函数调用，代码位于 wpxun/blog/tree/master/gorpcsvc，镜像已经推送到 DockerHub redis：提供给 gorpcweb 存储，计数用，使用官方镜像  4.1 准备 Manifest 文件 每个服务都需要 deploy 和 service 资源配置文件，gorpc-kube.yaml 放在 github 项目上，文件结构纲目：\n  redis: 缓存访问人数 Deployment, redis:5.0.4-alpine3.9 Service, 6379\n  gorpcsvc: 提供给 goweb 的 rpc 数学函数 Deployment, wpxun/gorpcsvc:v1 Service, 1234\n  goapi: 对外提供 api Deployment, wpxun/gorpcweb:v1, name: goweb-deploy-v1 Deployment, wpxun/gorpcweb:v1, name: goweb-deploy-v2 Service, type: NodePort, name: http, port: 81, targetPort: 80, nodePort: 30001 Deployment, wpxun/gorpcweb:v2, name: goweb-deploy-v3 Deployment, wpxun/gorpcweb:v2, name: goweb-deploy-v4 Service, type: NodePort, name: http, port: 82, targetPort: 8087, nodePort: 30002\n  4.2 管理资源 kubectl create -f gorpc-kube.yaml kubectl delete -f gorpc-kube.yaml kubectl apply -f gorpc-kube.yaml 创建好资源后查看资源情况：\nkubectl get deploy -o wide --show-labels kubectl get rs -o wide --show-labels kubectl get po -o wide --show-labels kubectl get svc -o wide --show-labels 4.3 验证服务 端口转换：targetPort(80) \u0026lt;-\u0026gt; port(81) \u0026lt;-\u0026gt; nodePort(30001)\n 在 Pod 内 curl http://goapi:81/index ，goweb-service即为服务名，只能在 Pod 内解析 在集群节点中，注意不能用 localhost curl http://10.44.0.57/index ，直接访问某一个 pod，IP:80 curl http://10.111.15.152:81/index ，访问服务，IP:81 curl http://192.168.1.11:30001/index ，访问宿主机，IP:30001，这里的 IP 是 eth1 IP(仅主机网络) curl http://10.0.2.15:30001/index ，访问宿主机，IP:30001，这里的 IP 是 eth0 IP（NAT） 在集群外，因为我是用 virtulbox 如果主机和虚拟机网络互通则可以直接像在集群中访问一样，curl http://192.168.1.11:30001/index ，IP:30001，这里的 IP 是 eth1 IP，是仅主机（Host-Only）网络 还可以做端口映射  5 常用的命令 kubectl get componentstatuses，查看 Kubernetes 系统服务是否正常运行。 kubectl get deploy -o wide \u0026ndash;show-labels kubectl get rs -o wide \u0026ndash;show-labels kubectl get po -o wide -n kube-system \u0026ndash;show-labels kubectl explain Deployment.spec \u0026ndash;api-version=apps/v1, 资源字段解释 kubectl exec -it curlapp-74bc6c5496-gqxxb -c curl /bin/sh, 选择pod，选择容器名\n参考文献 [1] 龚正. 等. Kubernetes 权威指南. 版次：2017年9月第1版 [2] Docker Swarm or Kubernetes — Help me decide. https://stackshare.io/stackups/docker-swarm-vs-kubernetes [3] https://zwischenzugs.com/2019/03/25/aws-vs-k8s-is-the-new-windows-vs-linux/\n","id":9,"section":"posts","summary":"本文重点理解 Kubernetes 资源的作用，并用一个简单的例子进行演示。 1 版本、组和资源 1.1 版本 版本是 Kubernetes 管理资源的一种策略，这种策略的目的是为了更好地进化与调整","tags":["Kubernetes","Docker"],"title":"Kubernetes 资源","uri":"https://blog.jemper.cn/2019/05/kubernetes-resource/","year":"2019"},{"content":"有了 Docker Swarm 基础，再来学习 Kubernetes 会相对容易一些，不过安装 Kubernetes 还是挺繁琐的，对官方文档中需要访问谷歌外网的部分，还需要替换成国内源。如果目的为了学习，还可以使用 minikube 或 Vagrant 快速创建集群。\n1 环境 Swarm 对主机（节点）的要求并不高，但 Kubernetes 会进行预检，一般注意以下几点：\n master 节点需至少 2CPUs 和至少 2GB 内存 关闭 swap 交换区  安装环境以官方文档为准 install-kubeadm；\n2 工具准备 2.1 Docker 安装 yum install -y yum-utils \\  device-mapper-persistent-data \\  lvm2 yum-config-manager \\  --add-repo \\  https://download.docker.com/linux/centos/docker-ce.repo yum install -y docker-ce docker-ce-cli containerd.io usermod -aG docker yhdodo19 systemctl start docker systemctl enable docker //重新登录 docker version 2.2 kube 相关工具 采用 aliyun 源：\ncat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF setenforce 0 sed -i \u0026#39;s/^SELINUX=enforcing$/SELINUX=permissive/\u0026#39; /etc/selinux/config yum install -y kubelet kubeadm kubectl systemctl enable --now kubelet 此时 kubelet 处于 activating (auto-restart)状态，当运行 kubeadm init 或者 kubeadm join 时才会变成 active (running) 状态。\n注意，必须关闭 selinux，确保容器可访问宿主机文件系统；注意 setenforce 为能设置为[ Enforcing | Permissive | 1 | 0 ]，修改文件可以设置为 disable。 另外一些 RHEL/CentOS 7 的用户曾经遇到过：由于 iptables 被绕过导致网络请求被错误的路由。您得保证 在您的 sysctl 配置中 net.bridge.bridge-nf-call-iptables 被设为 1。\ncat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system 3 启动集群 3.1 初始化集群 [root@instance-2 ~]# kubeadm init [init] Using Kubernetes version: v1.14.1 [preflight] Running pre-flight checks [WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly [WARNING IsDockerSystemdCheck]: detected \u0026#34;cgroupfs\u0026#34; as the Docker cgroup driver. The recommended driver is \u0026#34;systemd\u0026#34;. Please follow the guide at https://kubernetes.io/docs/setup/cri/ [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Activating the kubelet service [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Generating \u0026#34;ca\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver-kubelet-client\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key [certs] apiserver serving cert is signed for DNS names [instance-2 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.150.0.2] [certs] Generating \u0026#34;front-proxy-ca\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-client\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/ca\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/server\u0026#34; certificate and key [certs] etcd/server serving cert is signed for DNS names [instance-2 localhost] and IPs [10.150.0.2 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/peer\u0026#34; certificate and key [certs] etcd/peer serving cert is signed for DNS names [instance-2 localhost] and IPs [10.150.0.2 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/healthcheck-client\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver-etcd-client\u0026#34; certificate and key [certs] Generating \u0026#34;sa\u0026#34; key and public key [kubeconfig] Using kubeconfig folder \u0026#34;/etc/kubernetes\u0026#34; [kubeconfig] Writing \u0026#34;admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;kubelet.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;controller-manager.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;scheduler.conf\u0026#34; kubeconfig file [control-plane] Using manifest folder \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-apiserver\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-controller-manager\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-scheduler\u0026#34; [etcd] Creating static Pod manifest for local etcd in \u0026#34;/etc/kubernetes/manifests\u0026#34; [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026#34;/etc/kubernetes/manifests\u0026#34;. This can take up to 4m0s [apiclient] All control plane components are healthy after 21.002951 seconds [upload-config] storing the configuration used in ConfigMap \u0026#34;kubeadm-config\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [kubelet] Creating a ConfigMap \u0026#34;kubelet-config-1.14\u0026#34; in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --experimental-upload-certs [mark-control-plane] Marking the node instance-2 as control-plane by adding the label \u0026#34;node-role.kubernetes.io/master=\u0026#39;\u0026#39;\u0026#34; [mark-control-plane] Marking the node instance-2 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: rtbkg0.b6xgzvz2x9xekegi [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] creating the \u0026#34;cluster-info\u0026#34; ConfigMap in the \u0026#34;kube-public\u0026#34; namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 10.150.0.2:6443 --token rtbkg0.b6xgzvz2x9xekegi \\  --discovery-token-ca-cert-hash sha256:b4659d6b5d8e6015a5a31da172225b625336c095760191b51f246bf34b113864 3.2 访问集群 有了 Kubernetes cluster，那么接下就是访问集群，官方文档《通过 Kubernetes API 访问集群》列有多种方式，当然都需要 Kubernetes API 的 endpoint(IP 和端口)、token 和证书，以下可以获取到：\n$ APISERVER=$(kubectl config view | grep server | cut -f 2- -d \u0026quot;:\u0026quot; | tr -d \u0026quot; \u0026quot;) $ TOKEN=$(kubectl describe secret $(kubectl get secrets | grep default | cut -f1 -d ' ') | grep -E '^token' | cut -f2 -d':' | tr -d '\\t')  直接访问 REST API，比如 curl、wget 等，curl $APISERVER/api --header \u0026quot;Authorization: Bearer $TOKEN\u0026quot; --insecure 通过编程方式访问 API，比如有 Go、Python 客户端工具 kubectl command-line tool，通过以下方式配置 Kubernetes API endpoint、token 和证书，且 master 节点和 worker 节点都可以操作集群，如果是 worker 节点，把需要把 /etc/kubernetes/admin.conf 分发到 worker 节点上，然后执行以下操作： 对普通用户：   mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config  对 root 用户： export KUBECONFIG=/etc/kubernetes/admin.conf  3.4 选择安装网络插件 这里选择 weave net，kubectl apply -f [podnetwork].yaml： kubectl apply -f \u0026quot;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\u0026quot;\n3.5 查看信息 \u0026gt; kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-fb8b8dccf-6r95r 1/1 Running 0 113m kube-system coredns-fb8b8dccf-lhmvl 1/1 Running 0 113m kube-system etcd-instance-2 1/1 Running 0 112m kube-system kube-apiserver-instance-2 1/1 Running 0 112m kube-system kube-controller-manager-instance-2 1/1 Running 0 112m kube-system kube-proxy-pnc9h 1/1 Running 0 113m kube-system kube-scheduler-instance-2 1/1 Running 0 112m kube-system weave-net-l9hps 2/2 Running 0 61s \u0026gt; kubectl get nodes NAME STATUS ROLES AGE VERSION instance-2 Ready master 113m v1.14.1 一般来说，coredns 是 Kubernetes 集群的关键核心服务，会使用 Deployment 进行自动扩缩容；kube-proxy 和 weave-net 会分别在各节点上运行。\n4 创建节点 node 的依赖关系就没那么多，没有要求 CPU 和 内存数，没有要求 selinux，不需要选择网络插件，一般也不需要安装 kubectl。 即只需要 docker 环境、kubelet、kubeadm，其中 kubelet 启动后一样会进入 activating (auto-restart)状态，kubeadm join 后才会转变成 active (running) 状态。\n[root@instance-4 ~]# kubeadm join 10.150.0.2:6443 --token rtbkg0.b6xgzvz2x9xekegi \\ \u0026gt; --discovery-token-ca-cert-hash sha256:b4659d6b5d8e6015a5a31da172225b625336c095760191b51f246bf34b113864 [preflight] Running pre-flight checks [WARNING IsDockerSystemdCheck]: detected \u0026quot;cgroupfs\u0026quot; as the Docker cgroup driver. The recommended driver is \u0026quot;systemd\u0026quot;. Please follow the guide at https://kubernetes.io/docs/setup/cri/ [preflight] Reading configuration from the cluster... [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [kubelet-start] Downloading configuration for the kubelet from the \u0026quot;kubelet-config-1.14\u0026quot; ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot; [kubelet-start] Writing kubelet environment file with flags to file \u0026quot;/var/lib/kubelet/kubeadm-flags.env\u0026quot; [kubelet-start] Activating the kubelet service [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap... This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. 5 用 minikube 创建集群 可以参考官方的教程文档 Tutorials; 原理就是：给一个启动一个安装有 Docker 和 minikube CLI 的 Ubuntu 虚拟机，使用 minikube start 就可以安装以上所有步骤。\n$ minikube start o minikube v0.34.1 on linux (amd64) \u0026gt; Configuring local host environment ... \u0026gt; Creating none VM (CPUs=2, Memory=2048MB, Disk=20000MB) ... - \u0026quot;minikube\u0026quot; IP address is 172.17.0.86 - Configuring Docker as the container runtime ... - Preparing Kubernetes environment ... @ Downloading kubeadm v1.13.3 @ Downloading kubelet v1.13.3 - Pulling images required by Kubernetes v1.13.3 ... - Launching Kubernetes v1.13.3 using kubeadm ... - Configuring cluster permissions ... - Verifying component health ..... + kubectl is now configured to use \u0026quot;minikube\u0026quot; = Done! Thank you for using minikube! 6 用 Vagrant 创建集群 参考文献 [1] 龚正. 等. Kubernetes 权威指南. 版次：2017年9月第1版 [2] Docker Swarm or Kubernetes — Help me decide. https://stackshare.io/stackups/docker-swarm-vs-kubernetes\n","id":10,"section":"posts","summary":"有了 Docker Swarm 基础，再来学习 Kubernetes 会相对容易一些，不过安装 Kubernetes 还是挺繁琐的，对官方文档中需要访问谷歌外网的部分，还需要替换成国内源。如果目的为了学习，还","tags":["Kubernetes","Docker"],"title":"Kubernetes 集群安装","uri":"https://blog.jemper.cn/2019/04/kubernetes-install/","year":"2019"},{"content":"今天看到《Jenkins2 权威指南》出来了，马上买了一本，DevOps 相关的书籍更新太快了，这包括 Docker、Jenkins，新特性快速推出，旧版本必然快速淘汰，可以看一下 Jenkins 的版本，Jenkins 1.x 相关书籍已经严重滞后了。\n1 基础 1.1 安装  作为独立的应用程序安装：（1）二进制包安装，（2）brew、yum 安装，（3）直接下载 war 包，java -jar jenkins.war。 更新：用 brew、yum 命令更新，或者用新版的 jenkins.war 替换旧版然后重启 应用服务器安装，比如 tomcat 更新：注意要删除一些缓存，避免过期页面被引用  1.2 主目录 不管你把 Jenkins 的 war 文件存放在哪儿，Jenkins 都把其所有重要的数据存放在一个专用的、隔离的，称为 Jenkins 主目录的目录下。在这里，Jenkins 存储关于构建服务器的配置信息、构建作业、构建产物和其他有用的信息，当然也包括你安装了的任何插件。且 Jenkins 主目录格式是跨版本向后兼容的，所以你可以自由地更新或者重新安装 Jenkins 而不影响 Jenkins 主目录。\n一般主目录是 ~/.jenkins 目录，一个好的做法是为 Jenkins 创建一个特殊的用户和用户组。sudo groupadd build，sudo useradd --create-home --shell /bin/bash --groups build jenkins，让主目录在用户空间下。 确保你的 Jenkins 主目录能定期备份是非常重要的，Jenkins 程序本身相对并不那么重要。\n workspace：是 Jenkins 对你的项目进行构建的地方 builds：构建历史  1.3 节点 节点代表了任何可以执行 Jenkins 任务的系统，包括了主节点，代理节点和容器，前两都是机器，后者是需要在机器上运行的容器，但都可以称为节点，从这点可以看出，节点是承载任务的，而不是节点机器。\n 主节点：运行 Jenkins 实例的主要控制系统，不推荐在主节点上执行高负载任务，而应该在代理节点上运行 代理节点：也叫从节点，代表非主节点的系统，且由主系统管理，对资源访问有限，降低安全风险 容器：容器也应该选择在代理节点上运行  1.4 凭证   类型：除了默认类型，还可以安装插件扩展\n  Username with password，用户名密码\n  Docker Host Certificate Authentication\n  SSH Username with private key，ssh 连接\n  Secret file\n  Secret text，比如token\n  Certificate\n  域：尽量匹配对应的域名，不过不强制\n  全局\n  自定义域\n  提供者：\n  jenkins\n  用户\n  1.5 Pipeline Jenkins 2 的新特性，通过插件获得流水线这一新功能，通过 Jenkins DSL 实现流水线代码，代码提取到一个 Jenkinsfile 文件中。\n 脚本式流水线：node 用于指定节点，偏向程序语言 声明式流水线：agent 用于指定节点，偏向于自然语言  2 声明式流水线 创建一个流水线项目后，项目左侧列表会有“流水线语法”链接，可以借助这一脚手架直接生成 DSL 代码，比如与“构建触发”相关的可以选择 properties 步骤生成。当然想要理解声明式还得系统性地学习。\n{% img http://img.jemper.cn/2019/05/declarative_pipeline.png 250 %}\n3 构建触发器 除了手动立即构建，还可以构建触发器自动的构建，这也是自动化部署的基础，常用的几种构建触发器如下：\n 其他工程构建后触发，如标题所述，允许在一个或者多个其它项目之后开发你的项目构建。依赖构建 定时构建，定时构建 Build when a change is pushed to GitLab，需要设置 webhook URL GitHub hook trigger for GITScm polling，和 GitLab 一样需要在 GitHub 设置 webhook URL 轮询 SCM，Jenkins 主动询问  4 通知 7 分布式部署实战 下面通过 vagrant 进行分布式部署，主要还是用于开发和测试使用，本次实战只部署机器和环境，采用手动运行 jenkins.war。首先来看一下目录结构：\nvagjenkins/ ├── Vagrantfile ├── certs │ ├── domain.crt │ └── domain.key ├── jdk-8u211-linux-x64.tar.gz ├── jenkins.war └── scripts ├── docker.sh ├── hosts.sh ├── java.sh └── registry.sh  certs 和 scripts/registry.sh 是安装 Docker registry 的脚本，具体可以查看 Docker Engine 一文。 jdk-8u211-linux-x64.tar.gz 是运行 jenkins 的 jdk 环境工具 jenkins.war，在环境创建好后启动，以 java -jar jenkins.war 手动运行 Vagrantfile 和 scripts 在接下来重点介绍  7.1 Vagrantfile  master 节点   需要安装 Docker 并运行 Docker Registry 容器，并连接到Docker Registry； 安装 JDK 环境，预备运行 jenkins.war 映射 8080 端口到宿主机，访问宿主机的 localhost:8080 即是访问 master 的8080   node 从节点   只需安装 Docker，并连接到 master 虚拟机的 Docker Registry  Vagrantfile 就是基础设置即代码文件，可以将上述的过程代码化：\ncat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; /Users/ada/vagjenkins/Vagrantfile # -*- mode: ruby -*- # vi:set ft=ruby sw=2 ts=2 sts=2: NODE_COUNT = 2 POD_NETWORK_CIDR = \u0026quot;10.244.0.0/16\u0026quot; MASTER_IP = \u0026quot;192.168.56.113\u0026quot; NODE_IP_PREFIX = \u0026quot;192.168.56.\u0026quot; Vagrant.configure(\u0026quot;2\u0026quot;) do |config| config.vm.box = \u0026quot;centos/7\u0026quot; config.vm.box_version = \u0026quot;1902.01\u0026quot; config.vm.provider \u0026quot;virtualbox\u0026quot; do |v| v.memory = 1536 v.cpus = 2 end config.vm.provision \u0026quot;install-docker\u0026quot;, type: \u0026quot;shell\u0026quot;, :path =\u0026gt; \u0026quot;scripts/docker.sh\u0026quot; config.vm.provision \u0026quot;setup-hosts\u0026quot;, type: \u0026quot;shell\u0026quot;, :path =\u0026gt; \u0026quot;scripts/hosts.sh\u0026quot; do |s| s.args = [\u0026quot;eth1\u0026quot;] end config.vm.define \u0026quot;jenkins-master\u0026quot; do |node| node.vm.hostname = \u0026quot;jenkins-master\u0026quot; node.vm.network :private_network, ip: MASTER_IP node.vm.network \u0026quot;forwarded_port\u0026quot;, guest: 8080, host: 8080 node.vm.provision \u0026quot;install-registry\u0026quot;, type: \u0026quot;shell\u0026quot;, :path =\u0026gt; \u0026quot;scripts/registry.sh\u0026quot; node.vm.provision \u0026quot;install-java\u0026quot;, type: \u0026quot;shell\u0026quot;, :path =\u0026gt; \u0026quot;scripts/java.sh\u0026quot; end (1..NODE_COUNT).each do |i| config.vm.define \u0026quot;jenkins-node0#{i}\u0026quot; do |node| node.vm.hostname = \u0026quot;jenkins-node0#{i}\u0026quot; node.vm.network :private_network, ip: NODE_IP_PREFIX + \u0026quot;#{113 + i}\u0026quot; end end # config all node to connect registry config.vm.provision \u0026quot;shell\u0026quot;, inline: \u0026lt;\u0026lt;-SHELL mkdir -p /etc/docker/certs.d/192.168.56.113:5000 cp /vagrant/certs/domain.crt /etc/docker/certs.d/192.168.56.113:5000/ca.crt SHELL end EOF 7.2 scripts  Docker 安装  cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; /Users/ada/vagjenkins/scripts/docker.sh #!/bin/bash set -ex # 安装 docker yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo yum install -y docker-ce docker-ce-cli containerd.io # docker 官方镜像阿里云加速 mkdir -p /etc/docker tee /etc/docker/daemon.json \u0026lt;\u0026lt;-'EOF' { \u0026quot;registry-mirrors\u0026quot;: [\u0026quot;https://r11fpimm.mirror.aliyuncs.com\u0026quot;] } EOF # 启动 usermod -aG docker vagrant systemctl start docker systemctl enable docker EOF 设置主机名  cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; /Users/ada/vagjenkins/scripts/hosts.sh #!/bin/bash set -xe IFNAME=$1 IP=\u0026quot;$(ip -4 a s $IFNAME | grep \u0026quot;inet\u0026quot; | head -1 |awk '{print $2}' | cut -d'/' -f1)\u0026quot; sed -e \u0026quot;s/^.*${HOSTNAME}.*/${IP} ${HOSTNAME} ${HOSTNAME}/\u0026quot; -i /etc/hosts EOF 启动 docker registry 容器  cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; /Users/ada/vagjenkins/scripts/registry.sh #!/bin/bash set -ex # run container docker pull registry:2.7.1 docker run -d \\ --restart=always \\ --name registry \\ -v `pwd`/registry:/var/lib/registry \\ -v /vagrant/certs:/certs \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \\ -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \\ -p 5000:5000 \\ registry:2.7.1 # setup firewall, 因为 vagrant 镜像默认防火墙是关闭的，所以不用设置 #firewall-cmd --permanent --add-port=8080/tcp #firewall-cmd --reload EOF 安装 jdk 和 git，建议用 ssh 连接 git，一般有两种办法   如果是把 ssh 密钥放在服务器的话，必须得用 ssh -T git@github.com 连接 github，把 git 服务器指纹写入 known_hosts，否则构建不会在指纹提示自动输入 yes 只是安装 git，在 jenkins 创建新凭据，只需要填写 ssh 密钥，但每次使用时都需要指定用凭据，方便的是会自动采集指纹写入 known_hosts  我这里选择了第二种方式，注意设置环境变量的 $ 符号不要被解析掉：\ncat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; /Users/ada/vagjenkins/scripts/java.sh #!/bin/bash set -ex # untar jdk mkdir -p /usr/local/java tar -zxf /vagrant/jdk-8u211-linux-x64.tar.gz -C /usr/local/java/ # set env cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; /home/vagrant/.bash_profile export JAVA_HOME=/usr/local/java/jdk1.8.0_211 export JRE_HOME=\\${JAVA_HOME}/jre export CLASSPATH=.:\\${JAVA_HOME}/lib:\\${JRE_HOME}/lib export PATH=\\${JAVA_HOME}/bin:\\$PATH EOF yum install -y git EOF 7.3 运行 master 节点的 Jenkins 完成以上步骤，启动虚拟机，用 java -jar /vagrant/jenkins.war 运行 Jenkins，因为 8080 已经映射到宿主机，所以可以在宿主机访问 Jenkins：localhost:8080。\n7.4 加入 node 从节点 这里主要就把从节点的 ssh 私钥（服务器鉴权放在 /home/vagrant/.ssh/authorized_keys）生成 jenkins 的凭据，节点的 ssh 私钥可以通过 vagrant ssh [name] \u0026ndash;debug 的输出日志中获取，一般放在 /Users/ada/vagjenkins/.vagrant/machines/[name]/virtualbox/private_key，通过 ssh、scp 去连接都需要该 private_key，例子如下：\n scp -i /Users/ada/vagjenkins/.vagrant/machines/jenkins-node01/virtualbox/private_key /Users/ada/.ssh/github/* vagrant@192.168.56.114:/home/vagrant/.ssh、 NAT 网络通过端口转发连接，ssh -i /Users/ada/vagjenkins/.vagrant/machines/jenkins-master/virtualbox/private_key vagrant@127.0.0.1 -p 2222 因为我创建了主机网络，所以还可以直接通过主机网络的 IP 地址连接，ssh -i /Users/ada/vagjenkins/.vagrant/machines/jenkins-master/virtualbox/private_key vagrant@192.168.56.113  主节点通过 ssh 连接到从节点，同样需要 known_hosts 验证，在可以在服务器手动执行 ssh vagrant@192.168.56.115 验证再到 jenkins 启动节点。\n8 Troubleshooting  Jenkins 出现更新出错的问题，一般就是 /etc/resolv.conf 解析出问题，可以用以下的方式查看： curl -I \u0026ndash;connect-timeout 10 -m 20 https://updates.jenkins.io/update-center.json  参考文献 [1] what-is-scrum. https://www.scrum.org/resources/what-is-scrum [2] 如何从零开始搭建 CI/CD 流水线 https://www.infoq.cn/article/WHt0wFMDRrBU-dtkh1Xp\n","id":11,"section":"posts","summary":"今天看到《Jenkins2 权威指南》出来了，马上买了一本，DevOps 相关的书籍更新太快了，这包括 Docker、Jenkins，新特性快速推","tags":["Jenkins"],"title":"Jenkins 持续集成","uri":"https://blog.jemper.cn/2019/04/jenkins/","year":"2019"},{"content":"个人操作系统并不需要配置化，但随着开发人员的复杂化，每个开发人员都需要同样的主机系统以保证开发环境的可移植性；另外一方面，随着微服务的推广需要多主机节点管理，重复性的指令配置消耗大量时间且容易出错，于是出现了多主机配置化，可以用现在流行的说法“基础设施即代码”，Infrastructure as code (IaC)，例如 docker machine、vagrant，本文将讲解 vagrant 的使用，并安装 Kubernetes 集群进行实战。\n1 安装 目前 vagrant 支持在常见操作系统 Linux, macOS 或 Windows 上安装，支持的虚拟机包括 VMware、VirtualBox 等。 macOS 分别下载安装即可：\n VirtualBox，安装最新 6.0.6 版本 vagrant，安装最新 2.2.4 版本 或者直接用 brew 安装： brew cask install virtualbox brew cask install vagrant  运行 vagrant version 查看版本信息。\n2 box 的管理 box 相当于 Docker 基础镜像操作系统，这些镜像一般比那些 ISO 小，并进行一些设置，比如创建 vagrant（1000） 用户，镜像 .box 作为文件后缀。官方 Box 源，注意选择 virtualbox 下面的 box。\nbox 的操作是以 vagrant box 加子命令，它的操作跟虚拟机是无关的。\n vagrant box list，查看本地上的 box vagrant box add，The box descriptor can be the name of a box on HashiCorp's Vagrant Cloud, or a URL, or a local .box file, or a local .json file containing the catalog metadata，可以用 \u0026ndash;box-version 指定版本，否则下载最新版本 vagrant box remove，即使有基于该 box 运行的虚拟机也是可以删除的  下面我们用 centos/7 进行演示：\n\u0026gt; vagrant box add centos/7 ==\u0026gt; box: Loading metadata for box 'centos/7' box: URL: https://vagrantcloud.com/centos/7 This box can work with multiple providers! The providers that it can work with are listed below. Please review the list and choose the provider you will be working with. 1) hyperv 2) libvirt 3) virtualbox 4) vmware_desktop Enter your choice: 3 ==\u0026gt; box: Adding box 'centos/7' (v1902.01) for provider: virtualbox box: Downloading: https://vagrantcloud.com/centos/boxes/7/versions/1902.01/providers/virtualbox.box box: Download redirected to host: cloud.centos.org ==\u0026gt; box: Successfully added box 'centos/7' (v1902.01) for 'virtualbox'! \u0026gt; vagrant box list centos/7 (virtualbox, 1902.01) 3 虚拟机管理 虚拟机管理相当于 Docker 的容器管理，是 box 镜像的运行时。\n虚拟机的操作是以 vagrant 直接加子命令，以下命令基本都可以指定某一个 [name|id] 虚拟机运行操作，可以通过 vagrant COMMAND -h 查看\n vagrant init，初始化，会创建一个 Vagrantfile，如果已经有配置文件就无需初始化 vagrant up，无界面启动(VBoxHeadless)虚拟机 启动时会把当前目录(Vagrantfile 所在目录) COPY（不是挂载）到虚拟机 /vagrant，显示 Rsyncing folder: /Users/ada/vagtest/ =\u0026gt; /vagrant， 该命令会创建 .vagrant 隐藏目录 vagrant up \u0026ndash;provision 强制运行 provision，适用于创建虚拟机路途失败但虚拟机已经创建而 provision 还没完全运行的情况 vagrant up [name|id] 可以指定启动某一个虚拟机 vagrant ssh，连接到虚拟机 端口转发：默认配置端口转发config.vm.network \u0026quot;forwarded_port\u0026quot;, guest: 22, host: 2222，启动时显示 22 (guest) =\u0026gt; 宿主机的 2222 (host) (adapter 1，即网卡1)，SSH address: 127.0.0.1:2222。登录时通过类似 ssh vagrant@127.0.0.1 -p 2222 进行连接（当然还需要 SSH 密钥信息才能登录，通常直接用 vagrant ssh 登录）。另外我们查看 VirtualBox 虚拟机设备/网络/网卡1/高级/端口转发，可以看到已经配置的 2222 到 22 的端口转发规则，且每次 up 都会重新设置端口转发规则。 ssh 密钥：可以用 --debug 参数打印执行信息，可以看到连接需要的 ssh 密钥 private_key 所在的位置，然后就可以直接用 ssh 命令连接 scp -i /Users/ada/vagjenkins/.vagrant/machines/jenkins-node01/virtualbox/private_key /Users/ada/.ssh/github/* vagrant@192.168.56.114:/home/vagrant/.ssh vagrant ssh [name|id] 可以指定连接某一个虚拟机 vagrant reload，重启，一般在修改 Vagrantfile 后重启 vagrant halt，关机 vagrant destroy，删除虚拟机 vagrant provision，等同于 vagrant up \u0026ndash;provision vagrant suspend，挂起（保存虚拟机状态） vagrant resume，对挂起状态(suspend)进行恢复，而不是像 up 一样的全新启动 vagrant status，查看状态  \u0026gt; mkdir vagtest \u0026gt; cd vagtest \u0026gt; vagrant init centos/7 \u0026gt; vagrant up \u0026gt; ls Vagrantfile \u0026gt; vagrant ssh Last login: Mon May 6 01:59:53 2019 from 10.0.2.2 [vagrant@localhost ~]$ \u0026gt; sudo -i [root@localhost ~]# 从上面可以看出，与宿主机通信路由是 10.0.2.2，也可以在虚拟机查看 ip route show，注意要把 /etc/resolv.conf 的 DNS IP 设置为与宿主机通信路由 IP\n4 Vagrantfile 上面的虚拟机管理还是基于单机的管理，只能将其文件化才能 box 打包和多虚拟机管理。\nvagrant init centos/7 --box-version 1902.01，默认创建的 Vagrantfile：\nVagrant.configure(\u0026quot;2\u0026quot;) do |config| config.vm.box = \u0026quot;centos/7\u0026quot; config.vm.box_version = \u0026quot;1902.01\u0026quot; end  端口转发：config.vm.network \u0026quot;forwarded_port\u0026quot;, guest: 22, host: 2222，也就是 Docker 发布端口 目录共享：有多种方式，基本用法 config.vm.synced_folder \u0026quot;src/\u0026quot;, \u0026quot;/srv/website\u0026quot;，对开发共享代码非常有用，其它还有 NFS(not work on Windows)、SMB（Windows or macOS）等  5 打包 必须先关闭虚拟机，然后运行打包 vagrant package --output xxx.box\n参考文献 [1] 官方文档 https://www.vagrantup.com/docs/index.html\n","id":12,"section":"posts","summary":"个人操作系统并不需要配置化，但随着开发人员的复杂化，每个开发人员都需要同样的主机系统以保证开发环境的可移植性；另外一方面，随着微服务的推广需","tags":["Kubernetes"],"title":"Vagrant 虚拟机管理","uri":"https://blog.jemper.cn/2019/04/vagrant/","year":"2019"},{"content":"我们从简到繁看一下 Docker 的学习路线：\n docker run：Single Engine(者称 Single-Host，单 Docker 节点)下单服务运行 docker-compose：Single Engine(或者称 Single-Host，单 Docker 节点)下多服务编排 docker swarm：Multi-Host(多 Docker 节点，集群)下单服务编排 docker stack：Multi-Host(多 Docker 节点，集群)下多服务编排  可以看到 docker stack 其实就是 docker-compose 多应用和 docker swarm 规模化两者的结合。\n1 节点初始化 从 swarm 我们得知环境要求并不高，那么对 stack 也一样，接下来我们用三台主机进行实战部署，跟 swarm 一样对节点进行初始化，成为 swarm 集群，但不需要创建网络，因为网络创建我们通过编排文件进行：\n姓名 地区 内部 IP binke01 asia-northeast1-a 10.146.0.2 (nic0) binke01-1 asia-northeast1-a 10.146.0.3 (nic0) binke01-3 asia-northeast1-a 10.146.0.5 (nic0) \u0026gt; docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION 0kjqucshibpm35zhq7kizldp0 binke01 Ready Active 18.09.5 qr7i763tagufpcyn4qf37b5fs * binke01-1 Ready Active Leader 18.09.5 ogvwkwq0zxw3s05shey2ruzqa binke01-3 Ready Active 18.09.5 2 应用容器化 我们先来看一下容器化上下文：\n\u0026gt; tree multigo multigo ├── config.yaml ├── Dockerfile ├── docker-stack.yml ├── main.go └── service ├── config.go └── redis.go 2.1 业务代码 我们继续使用计数器，需要两个应用，分别是Go web 服务器和 redis 记录数据应用，我们采用的代码和我们在用 Docker Compose 部署的多应用代码几乎一样：\ncat \u0026lt;\u0026lt;EOF \u0026gt; $GOPATH/github.com/wpxun/multigo/main.go package main import ( \u0026quot;fmt\u0026quot; \u0026quot;github.com/wpxun/multigo/service\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;os\u0026quot; \u0026quot;strconv\u0026quot; ) func IndexHandler(w http.ResponseWriter, r *http.Request) { redis := service.GetRedis() val, err := redis.Incr(\u0026quot;count\u0026quot;).Result() if err != nil { panic(err) } host, _ := os.Hostname() fmt.Fprintln(w, \u0026quot;hello world \u0026quot;+ host +\u0026quot;, visitors = \u0026quot; + strconv.FormatInt(val, 10) ) } func main() { http.Handle(\u0026quot;/pattern\u0026quot;, http.HandlerFunc(IndexHandler)) http.ListenAndServe(\u0026quot;:80\u0026quot;, nil) } EOF cat \u0026lt;\u0026lt;EOF \u0026gt; $GOPATH/github.com/wpxun/multigo/config.yaml Redis: DialTimeout: 2000000000 #连接超时设定(s)，默认200ms Network: tcp #网络连接协议 Address: redis:6379 #连接地址(带端口) Password: #密码 Database: 0 #数据库，默认0 EOF cat \u0026lt;\u0026lt;EOF \u0026gt; $GOPATH/github.com/wpxun/multigo/service/redis.go package service import ( \u0026quot;github.com/go-redis/redis\u0026quot; ) func GetRedis() *redis.Client { return redis.NewClient(\u0026amp;redis.Options { Addr: Conf.Redis.Address, Password: Conf.Redis.Password, DB: Conf.Redis.Database, Network: Conf.Redis.Network, DialTimeout: Conf.Redis.DialTimeout, }) } EOF cat \u0026lt;\u0026lt;EOF \u0026gt; $GOPATH/github.com/wpxun/multigo/service/config.go package service import ( \u0026quot;fmt\u0026quot; \u0026quot;gopkg.in/yaml.v2\u0026quot; \u0026quot;io/ioutil\u0026quot; \u0026quot;time\u0026quot; ) type confstruct struct { Redis struct { Address string `yaml:\u0026quot;Address\u0026quot;` Database int `yaml:\u0026quot;Database\u0026quot;` DialTimeout time.Duration `yaml:\u0026quot;DialTimeout\u0026quot;` Network string `yaml:\u0026quot;Network\u0026quot;` Password string `yaml:\u0026quot;Password\u0026quot;` } `yaml:\u0026quot;Redis\u0026quot;` } var Conf confstruct func init() { GetYaml(\u0026quot;config\u0026quot;, \u0026amp;Conf) } func GetYaml(filename string, out interface{}) { yamlFile, err := ioutil.ReadFile(fmt.Sprintf(\u0026quot;%s.yaml\u0026quot;, filename)) if err != nil { fmt.Println(\u0026quot;Read config file error:\u0026quot;, err.Error()) } err = yaml.Unmarshal(yamlFile, out) if (err != nil) { fmt.Println(\u0026quot;Unmarshal config file error:\u0026quot;, err.Error()) } } EOF 2.2 容器化 编写 Dockerfile，采用多阶段构建方式，使得镜像只有 12.9MB。另外 stack 要求提前构建好并推送到创建，也就是 docker-stack.yml 不能用在运行的时候才 build 镜像，原因是多节点部署中，其它节点并没有构建上下文。另外 redis 我们使用官方的 redis 镜像。\n# 多阶段构建 # 第一阶段，391MB，编译前准备：go 和 git 工具、代码依赖库 FROM golang:1.12.4-alpine3.9 AS front RUN set -xe \u0026amp;\u0026amp; \\ apk add git \u0026amp;\u0026amp; \\ go get -v github.com/go-redis/redis \u0026amp;\u0026amp; \\ go get -v gopkg.in/yaml.v2 # 分成两次 RUN 目的是可复用上面的缓存，编译 go 代码 COPY . /go/src/github.com/wpxun/multigo RUN set -xe \u0026amp;\u0026amp; \\ go install github.com/wpxun/multigo # 第二阶段，14.6MB；仅仅复制了可执行程序和程序的配置文件 FROM alpine:3.9 ENV GOM_VERSION 1904.1 COPY --from=front /go/bin /go/src/github.com/wpxun/multigo/config.yaml /go/bin/ EXPOSE 80 WORKDIR /go/bin CMD [\u0026quot;/go/bin/multigo\u0026quot;] 3 分析 Stack 文件 Stack 一直是期望的 Compose——完全集成到 Docker 中，并能管理应用的整个生命周期。\ncat \u0026lt;\u0026lt;EOF \u0026gt; $GOPATH/github.com/wpxun/multigo/docker-stack.yml version: \u0026quot;3.7\u0026quot; services: goweb: image: \u0026quot;wpxun/multigo:v1\u0026quot; ports: - target: 80 published: 80 networks: - counter-net deploy: restart_policy: condition: on-failure delay: 5s max_attempts: 3 window: 120s replicas: 8 update_config: parallelism: 2 failure_action: rollback redis: image: \u0026quot;redis:5.0.4-alpine3.9\u0026quot; networks: - counter-net deploy: restart_policy: condition: on-failure delay: 5s max_attempts: 3 window: 120s placement: constraints: - 'node.role == worker' networks: counter-net: volumes: counter-vol: EOF 在该文件整体结构中，定义了 4 种顶级关键字：\n version: 其要求的 version ≥ 3.0 services: 定义了两个服务，这部分也是核心内容，接下来会讲解 networks: 创建一个网络，驱动为默认。stack 编排文件的默认驱动是 overlay(swarm)，而 compose 编排文件的默认驱动是 bridge(local)。 volumes: 创建一个卷  4 部署应用 docker stack deploy -c docker-stack.yml multigo\n\u0026gt; docker stack ls NAME SERVICES ORCHESTRATOR multigo 2 Swarm \u0026gt; docker service ls ID NAME MODE REPLICAS IMAGE PORTS 8vcqoh0yry5v multigo_goweb replicated 8/8 wpxun/multigo:v1 *:80-\u0026gt;80/tcp cygtoce4mfkt multigo_redis replicated 1/1 redis:5.0.4-alpine3.9 5 管理应用 部署成功之后，所有的 node 节点的 IP 都可以访问到服务，而非仅仅 Leader 节点。\n6 删除 Stack docker stack rm，一定要谨慎，删除 Stack 不会进行二次确认，服务和网络会删除，但卷不会删除，这是因为卷的设计初衷是保存持久化数据，其生命周期独立于容器、服务以及 Stack 之外。\n参考文献 [1] Nigel Poulton. 深入浅出 Dokcer. 版次：2019年4月第1版 [2] Docker Swarm or Kubernetes — Help me decide. https://stackshare.io/stackups/docker-swarm-vs-kubernetes\n","id":13,"section":"posts","summary":"我们从简到繁看一下 Docker 的学习路线： docker run：Single Engine(者称 Single-Host，单 Docker 节点)下单服务运行 docker-comp","tags":["Docker","容器"],"title":"Docker Stack 多服务","uri":"https://blog.jemper.cn/2019/04/docker-stack/","year":"2019"},{"content":"概括来说，Swarm 有两个核心组件：\n 企业级的 Docker 安全集群 微服务应用编排引擎  1 环境 Swarm 对主机（节点）的要求并不高，我在4台 f1-micro（1 个 vCPU，0.6 GB 内存）+ 10G磁盘上操作，开启三个小型的服务共40个副本（采取逐步调整 scale，每个副本 13M 左右，平均一个节点10个副本），副本太多容易导致 Leader 节点编排的时候 Docker 服务宕掉，所以 Leader 性能要好一点。总之对学习来说并不存在环境障碍。\n 用四台机器进行部署：  姓名 地区 内部 IP binke01 asia-northeast1-a 10.146.0.2 (nic0) binke01-1 asia-northeast1-a 10.146.0.3 (nic0) binke01-2 asia-northeast1-a 10.146.0.4 (nic0) binke01-3 asia-northeast1-a 10.146.0.5 (nic0)  开放端口，一般云服务器内网不需要此操作  firewall-cmd --permanent --add-port=2377/tcp firewall-cmd --permanent --add-port=7946/tcp firewall-cmd --permanent --add-port=7946/udp firewall-cmd --permanent --add-port=4789/tcp firewall-cmd --reload  准备应用代码，我是用 Go 写的一个小应用，一共两个版本，只是把“this is version 1”中的改为2即可，build 完后必须推到 hub 库，创建服务的时候节点在本地查找镜像，没找到会去拉取；代码读取容器的 HOSTNAME（容器的 HOSTNAME 其实就是容器的 ID），可以检验基于 Ingress 的容器负载均衡：  cat \u0026lt;\u0026lt;EOF \u0026gt;main.go package main import ( \u0026quot;fmt\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;os\u0026quot; ) func IndexHandler(w http.ResponseWriter, r *http.Request) { host := os.Getenv(\u0026quot;HOSTNAME\u0026quot;) fmt.Fprintln(w, \u0026quot;hello world \u0026quot;+ host +\u0026quot;, this is version 1.\u0026quot; ) } func main() { http.Handle(\u0026quot;/pattern\u0026quot;, http.HandlerFunc(IndexHandler)) http.ListenAndServe(\u0026quot;:80\u0026quot;, nil) } EOF  编写 Dockerfile，采用多阶段构建方式，使得镜像只有 12.9MB  cat \u0026lt;\u0026lt;EOF \u0026gt;Dockerfile FROM golang:1.12.4-alpine3.9 AS front COPY main.go /go/src/github.com/wpxun/onlygo/ RUN set -xe \u0026amp;\u0026amp; go install github.com/wpxun/onlygo FROM alpine:3.9 ENV GOM_VERSION 1904.1 COPY --from=front /go/bin /go/bin/ EXPOSE 80 CMD [\u0026quot;/go/bin/onlygo\u0026quot;] EOF 2 集群初始化 只能在管理节点上初始化 Swarm 集群。我把 binke01-1 节点初始化：\n\u0026gt; docker swarm init Swarm initialized: current node (qr7i763tagufpcyn4qf37b5fs) is now a manager. To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-33x2n6mrudf0x4s4j3zque3lec9c2dq5jeiaj17b7yujuaklns-ayc7mxqui49frdgb8hadcmmyf 10.146.0.3:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. 按上面的提示把其它节点 join 进来，查看节点列表：\n\u0026gt; docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION 0kjqucshibpm35zhq7kizldp0 binke01 Ready Active 18.09.3 qr7i763tagufpcyn4qf37b5fs * binke01-1 Ready Active Leader 18.09.5 186b6a4dpvy8alzdfuf3jtrv4 binke01-2 Ready Active 18.09.5 ogvwkwq0zxw3s05shey2ruzqa binke01-3 Ready Active Reachable 18.09.5 其中 * 表示当前命令所在的节点，工作节点不能查看或者管理集群状态（node、service 等子命令不能用）。 Docker Swarm 推荐奇数个管理节点（1，3，5，不要大于7个）。\n创建一个覆盖网络（即驱动类型：overlay） bridge 是 single-host network，其 SCOPE 只能是 local；而 overlay 是 multi-host network，其 SCOPE 则是 swarm。且只能在 manager node 创建，此时只有当前的 Swarm 管理节点可见，其它节点如果有容器接入该网络后也是可见的。 如果是 docker-compose 创建的，其名称是“目录名_网络名”。\n\u0026gt;docker network create -d overlay onlygo-net fe1wzq8dljvcqzd8o5vhw7bma \u0026gt; docker network ls NETWORK ID NAME DRIVER SCOPE 079de4d69078 bridge bridge local 41c3503514c2 docker_gwbridge bridge local e6f7a28b3b05 host host local njzh016c5qrl ingress overlay swarm cf1660673a83 none null local fe1wzq8dljvc onlygo-net overlay swarm 3 创建服务 无论哪种模式，即使主节点（Leader）挂掉，其它没挂掉的节点还是可以访问。而挂掉的节点其 IP 一定不能访问。\n 入站模式（Ingress Mode），也是默认的模式，是在 Swarm 中的所有节点开放端口：   即使节点上没有服务的副本，通过 IP 也可以访问，所有的节点都配置有映射，因此会将请求转发给运行有服务副本的节点，相当有了负载的能力，即基于 Ingress 的容器负载均衡；当然如果某个节点 Down 掉，则不可能再访问那个节点的 IP。 一个节点可以有多个副本，如下的 binke01-2 节点有两个副本 通过端口显示（为空或 80/tcp）就可以看出是服务提供统一端口，单引擎和 Swarm 端口冲突时不管谁先启动，以 Swarm 优先，即如果宿主机端口已经占用，docker 会把它分配回给 Swarm，从这一点看如果已经是 Swarm 就不建议同时作为单引擎使用了，当然如果端口不冲突那是可以共用的。  \u0026gt; docker service create --name onlygo-svc -p 80:80 --replicas 5 --network onlygo-net wpxun/onlygo:v1 # PORTS 看出服务提供端口 \u0026gt; docker service ls ID NAME MODE REPLICAS IMAGE PORTS zai8apyhcv0c onlygo-svc replicated 5/5 wpxun/onlygo:v1 *:80-\u0026gt;80/tcp # PORTS 看出节点并没有提供端口 \u0026gt; docker service ps onlygo-svc ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS asda8ds4ae70 onlygo-svc.1 wpxun/onlygo:v1 binke01-2 Running Running 31 seconds ago 0dpg2041gjhv onlygo-svc.2 wpxun/onlygo:v1 binke01-2 Running Running 31 seconds ago h0a302tciz4d onlygo-svc.3 wpxun/onlygo:v1 binke01-3 Running Running 32 seconds ago c4zs50l7ji3m onlygo-svc.4 wpxun/onlygo:v1 binke01-1 Running Running 35 seconds ago lm5rvn4oc2h6 onlygo-svc.5 wpxun/onlygo:v1 binke01 Running Running 32 seconds ago \u0026gt; docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6389302152c1 wpxun/onlygo:v1 \u0026quot;/go/bin/onlygo\u0026quot; About a minute ago Up About a minute 80/tcp onlygo-svc.4.c4zs50l7ji3mw4ate9iz7m6bv 主机模式（Host Mode），即仅在运行有容器副本的节点上开放端口：   节点上如果没有服务的副本，就不能通过 IP 访问，节点不配置映射，相当于没有负载 一个节点最多只能有一个副本，如果副本多于节点，会提示 no suitable node (host-mode port already in use on 4 nodes) 通过端口显示（*:80-\u0026gt;80/tcp 和 0.0.0.0:80-\u0026gt;80/tcp）就可以看出是各节点各自提供端口；另外还要防止端口冲突：Bind for 0.0.0.0:80 failed: port is already allocated  \u0026gt; docker service create --name onlygo-svc --publish published=80,target=80,mode=host --replicas 5 --network onlygo-net wpxun/onlygo:v1 # PORTS 看出节点提供端口 \u0026gt; docker service ps --no-trunc onlygo-svc ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS u7a6xsh8si0uz9917j3i3gp3d onlygo-svc.1 wpxun/onlygo:v1@sha256:b98f1f36fe80f113f4e23932fc5a0314ced0ef87f5e1b33d0528a05acbd2c2f5 binke01-2 Running Running 8 minutes ago *:80-\u0026gt;80/tcp f51qf2f3cl5pt2jjda1323pux onlygo-svc.2 wpxun/onlygo:v1@sha256:b98f1f36fe80f113f4e23932fc5a0314ced0ef87f5e1b33d0528a05acbd2c2f5 binke01-3 Running Running 8 minutes ago *:80-\u0026gt;80/tcp n9rgob84kpa4d2b7fgswnt1ib onlygo-svc.3 wpxun/onlygo:v1@sha256:b98f1f36fe80f113f4e23932fc5a0314ced0ef87f5e1b33d0528a05acbd2c2f5 binke01-1 Running Running 8 minutes ago *:80-\u0026gt;80/tcp n3wjdse90bjp47cauozew8rny onlygo-svc.4 wpxun/onlygo:v1@sha256:b98f1f36fe80f113f4e23932fc5a0314ced0ef87f5e1b33d0528a05acbd2c2f5 binke01 Running Running 53 seconds ago *:80-\u0026gt;80/tcp m3k4wlocswfila6evuvrocydv onlygo-svc.5 wpxun/onlygo:v1@sha256:b98f1f36fe80f113f4e23932fc5a0314ced0ef87f5e1b33d0528a05acbd2c2f5 Running Pending 55 seconds ago \u0026quot;no suitable node (host-mode port already in use on 4 nodes)\u0026quot; [feixin10@binke01-3 ~]$ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 51b3c95d4772 wpxun/onlygo:v1 \u0026quot;/go/bin/onlygo\u0026quot; 30 minutes ago Up 30 minutes 0.0.0.0:80-\u0026gt;80/tcp onlygo-svc.2.f51qf2f3cl5pt2jjda1323pux 另外要注意几点： （1）通过 docker service ls 列表的 PORTS 表示服务的端口，而通过 docker service ps onlygo-svc 列表的 PORTS 表示节点的端口。 （2）拉取的镜像 tag 为 none(如下图正常 create 时 tag 为 v1 和 v2)，不要进行修改，否则运行的容器会退出。当然可以退出副本看管理节点自动控制能力，docker service ps 列表的 ERROR 行也可以看出哪些副本状态异常。\n\u0026gt; docker images -a REPOSITORY TAG IMAGE ID CREATED SIZE wpxun/onlygo \u0026lt;none\u0026gt; c36b4eaf86ea 6 hours ago 12.9MB wpxun/onlygo \u0026lt;none\u0026gt; bb89b247fa08 6 hours ago 12.9MB 4 Scale 规模 缩放 scale 和 create 是很像的，如果有副本异常会持续编排，可能通过 ctrl+c 中断（不会影响正常运行的副本）。\n\u0026gt; docker service scale onlygo-svc=3 onlygo-svc scaled to 3 overall progress: 3 out of 3 tasks 1/3: running [==================================================\u0026gt;] 2/3: running [==================================================\u0026gt;] 3/3: running [==================================================\u0026gt;] verify: Service converged 5 滚动升级 docker service update --image wpxun/onlygo:v2 --update-parallelism 2 --update-delay 80s onlygo-svc\n image 要升级为的镜像 update-parallelism 每次使用新镜像更新的副本数 update-delay 每次更新的延迟时间  下面是运行结果，初始是 5 个副本，每次更新 2 个副本，经过三轮更新，每轮 80s 延迟。\n# 初始 5 个副本 \u0026gt; docker service ps onlygo-svc ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS vrenb115qwe9 onlygo-svc.1 wpxun/onlygo:v1 binke01 Running Running 9 minutes ago w4sz99i5xfgm onlygo-svc.2 wpxun/onlygo:v1 binke01-2 Running Running 9 minutes ago fe5bh6j2o6jq onlygo-svc.3 wpxun/onlygo:v1 binke01-3 Running Running 9 minutes ago vyv2hj1q8tdn onlygo-svc.4 wpxun/onlygo:v1 binke01-1 Running Running 9 minutes ago usrztf48878j onlygo-svc.5 wpxun/onlygo:v1 binke01 Running Running 9 minutes ago # 第一轮更新 \u0026gt; docker service ps onlygo-svc ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS vrenb115qwe9 onlygo-svc.1 wpxun/onlygo:v1 binke01 Running Running 10 minutes ago obtas0qo4lq6 onlygo-svc.2 wpxun/onlygo:v2 binke01-3 Running Running less than a second ago w4sz99i5xfgm \\_ onlygo-svc.2 wpxun/onlygo:v1 binke01-2 Shutdown Shutdown 5 seconds ago xl3fldhit8s4 onlygo-svc.3 wpxun/onlygo:v2 binke01-2 Running Running 4 seconds ago fe5bh6j2o6jq \\_ onlygo-svc.3 wpxun/onlygo:v1 binke01-3 Shutdown Shutdown 6 seconds ago vyv2hj1q8tdn onlygo-svc.4 wpxun/onlygo:v1 binke01-1 Running Running 10 minutes ago usrztf48878j onlygo-svc.5 wpxun/onlygo:v1 binke01 Running Running 10 minutes ago # 等待 80s # 第二轮更新 \u0026gt; docker service ps onlygo-svc ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 4hrqsqc59stn onlygo-svc.1 wpxun/onlygo:v2 binke01 Running Running 28 seconds ago vrenb115qwe9 \\_ onlygo-svc.1 wpxun/onlygo:v1 binke01 Shutdown Shutdown 29 seconds ago obtas0qo4lq6 onlygo-svc.2 wpxun/onlygo:v2 binke01-3 Running Running about a minute ago w4sz99i5xfgm \\_ onlygo-svc.2 wpxun/onlygo:v1 binke01-2 Shutdown Shutdown about a minute ago xl3fldhit8s4 onlygo-svc.3 wpxun/onlygo:v2 binke01-2 Running Running about a minute ago fe5bh6j2o6jq \\_ onlygo-svc.3 wpxun/onlygo:v1 binke01-3 Shutdown Shutdown about a minute ago vyv2hj1q8tdn onlygo-svc.4 wpxun/onlygo:v1 binke01-1 Running Running 12 minutes ago 7jcq34xnxhpw onlygo-svc.5 wpxun/onlygo:v2 binke01-3 Running Running 32 seconds ago usrztf48878j \\_ onlygo-svc.5 wpxun/onlygo:v1 binke01 Shutdown Shutdown 33 seconds ago # 等待 80s # 第三轮更新 \u0026gt; docker service ps onlygo-svc ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS 4hrqsqc59stn onlygo-svc.1 wpxun/onlygo:v2 binke01 Running Running about a minute ago vrenb115qwe9 \\_ onlygo-svc.1 wpxun/onlygo:v1 binke01 Shutdown Shutdown about a minute ago obtas0qo4lq6 onlygo-svc.2 wpxun/onlygo:v2 binke01-3 Running Running 2 minutes ago w4sz99i5xfgm \\_ onlygo-svc.2 wpxun/onlygo:v1 binke01-2 Shutdown Shutdown 3 minutes ago xl3fldhit8s4 onlygo-svc.3 wpxun/onlygo:v2 binke01-2 Running Running 3 minutes ago fe5bh6j2o6jq \\_ onlygo-svc.3 wpxun/onlygo:v1 binke01-3 Shutdown Shutdown 3 minutes ago tmnwlqvydpgn onlygo-svc.4 wpxun/onlygo:v2 binke01-1 Running Running 11 seconds ago vyv2hj1q8tdn \\_ onlygo-svc.4 wpxun/onlygo:v1 binke01-1 Shutdown Shutdown 12 seconds ago 7jcq34xnxhpw onlygo-svc.5 wpxun/onlygo:v2 binke01-3 Running Running about a minute ago usrztf48878j \\_ onlygo-svc.5 wpxun/onlygo:v1 binke01 Shutdown Shutdown about a minute ago # 查看每个节点运行的容器，比如在 binke01-1 上查看 \u0026gt; docker container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 107b41631240 wpxun/onlygo:v2 \u0026quot;/go/bin/onlygo\u0026quot; 8 minutes ago Up 8 minutes 80/tcp onlygo-svc.4.tmnwlqvydpgng67an6hliqibx a5faabc04466 wpxun/onlygo:v1 \u0026quot;/go/bin/onlygo\u0026quot; 22 minutes ago Exited (2) 8 minutes ago onlygo-svc.4.vyv2hj1q8tdnedue98c2725z3 可以注意到，Swarm 在升级中会均衡的将副本分配给 Swarm 中的所有节点。\n查看 inspect 可以看到，更新的配置已经成为服务定义的一部分，之后的更新将会自动使用这些设置，直到使用参数覆盖它们。\n\u0026gt; docker service inspect --pretty onlygo-svc ID: p9p5hfije6krnjtrusz3ut1st Name: onlygo-svc Service Mode: Replicated Replicas: 5 UpdateStatus: State: completed Started: 19 minutes ago Completed: 14 minutes ago Message: update completed Placement: UpdateConfig: Parallelism: 2 Delay: 1m20s On failure: pause Monitoring Period: 5s Max failure ratio: 0 Update order: stop-first RollbackConfig: Parallelism: 1 On failure: pause Monitoring Period: 5s Max failure ratio: 0 Rollback order: stop-first ContainerSpec: Image: wpxun/onlygo:v2@sha256:5a4e11c60f0ff8956b7ae972e0588753c27fc0376025e1fc02582af21d84dab7 Init: false Resources: Endpoint Mode: vip Ports: PublishedPort = 80 Protocol = tcp TargetPort = 80 PublishMode = ingress 再次“升级”回 v1 版本 docker service update --image wpxun/onlygo:v1 onlygo-svc：\n# 第三轮升级等待中 \u0026gt; docker service update --image wpxun/onlygo:v1 onlygo-svc onlygo-svc overall progress: 4 out of 5 tasks 1/5: running [==================================================\u0026gt;] 2/5: running [==================================================\u0026gt;] 3/5: 4/5: running [==================================================\u0026gt;] 5/5: running [==================================================\u0026gt;] # 升级完成查看副本列表 \u0026gt; docker service ps onlygo-svc ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS xsry6d3b57qy onlygo-svc.1 wpxun/onlygo:v1 binke01 Running Running 2 minutes ago 4hrqsqc59stn \\_ onlygo-svc.1 wpxun/onlygo:v2 binke01 Shutdown Shutdown 3 minutes ago vrenb115qwe9 \\_ onlygo-svc.1 wpxun/onlygo:v1 binke01 Shutdown Shutdown 24 minutes ago 39g1ghfjc2up onlygo-svc.2 wpxun/onlygo:v1 binke01-3 Running Running about a minute ago obtas0qo4lq6 \\_ onlygo-svc.2 wpxun/onlygo:v2 binke01-3 Shutdown Shutdown about a minute ago w4sz99i5xfgm \\_ onlygo-svc.2 wpxun/onlygo:v1 binke01-2 Shutdown Shutdown 26 minutes ago jsbd3x38qouo onlygo-svc.3 wpxun/onlygo:v1 binke01-2 Running Running about a minute ago xl3fldhit8s4 \\_ onlygo-svc.3 wpxun/onlygo:v2 binke01-2 Shutdown Shutdown about a minute ago fe5bh6j2o6jq \\_ onlygo-svc.3 wpxun/onlygo:v1 binke01-3 Shutdown Shutdown 26 minutes ago ig89pa0ylmrf onlygo-svc.4 wpxun/onlygo:v1 binke01-1 Running Running 7 seconds ago tmnwlqvydpgn \\_ onlygo-svc.4 wpxun/onlygo:v2 binke01-1 Shutdown Shutdown 9 seconds ago vyv2hj1q8tdn \\_ onlygo-svc.4 wpxun/onlygo:v1 binke01-1 Shutdown Shutdown 23 minutes ago ud4kn771erry onlygo-svc.5 wpxun/onlygo:v1 binke01 Running Running 2 minutes ago 7jcq34xnxhpw \\_ onlygo-svc.5 wpxun/onlygo:v2 binke01-3 Shutdown Shutdown 3 minutes ago usrztf48878j \\_ onlygo-svc.5 wpxun/onlygo:v1 binke01 Shutdown Shutdown 24 minutes ago # 在某个节点上查看运行的容器，对应上面该节点上的副本的状态 [feixin10@binke01 ~]$ docker container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 619e375ca3f9 wpxun/onlygo:v1 \u0026quot;/go/bin/onlygo\u0026quot; 5 minutes ago Up 5 minutes 80/tcp onlygo-svc.5.ud4kn771erryzg2d0cansm3md 26ac4c19a78f wpxun/onlygo:v1 \u0026quot;/go/bin/onlygo\u0026quot; 5 minutes ago Up 5 minutes 80/tcp onlygo-svc.1.xsry6d3b57qy1bpc2x8ptbxyd 0f54cadb8f00 wpxun/onlygo:v2 \u0026quot;/go/bin/onlygo\u0026quot; 26 minutes ago Exited (2) 5 minutes ago onlygo-svc.1.4hrqsqc59stnioxgmo94h6gsa 75b3967ee0c9 wpxun/onlygo:v1 \u0026quot;/go/bin/onlygo\u0026quot; 38 minutes ago Exited (2) 26 minutes ago onlygo-svc.5.usrztf48878jn3490qeujwjc6 13ccf9d7fbaa wpxun/onlygo:v1 \u0026quot;/go/bin/onlygo\u0026quot; 38 minutes ago Exited (2) 26 minutes ago onlygo-svc.1.vrenb115qwe9qg488megfnq1j 6 节点管理 节点的选择可以用 ID 或 HOSTNAME。\n6.1 新增节点 docker swarm join --token 下面新增节点 binke01-4，ip:10.146.0.6，然后修改 scale = 9，然后查看编排结果：\n\u0026gt; docker swarm join --token SWMTKN-1-33x2n6mrudf0x4s4j3zque3lec9c2dq5jeiaj17b7yujuaklns-ayc7mxqui49frdgb8hadcmmyf 10.146.0.3:2377 \u0026gt; docker service scale onlygo-svc=9 \u0026gt; docker service ps onlygo-svc ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTS xsry6d3b57qy onlygo-svc.1 wpxun/onlygo:v1 binke01 Running Running 18 minutes ago 4hrqsqc59stn \\_ onlygo-svc.1 wpxun/onlygo:v2 binke01 Shutdown Shutdown 18 minutes ago vrenb115qwe9 \\_ onlygo-svc.1 wpxun/onlygo:v1 binke01 Shutdown Shutdown 40 minutes ago 39g1ghfjc2up onlygo-svc.2 wpxun/onlygo:v1 binke01-3 Running Running 17 minutes ago obtas0qo4lq6 \\_ onlygo-svc.2 wpxun/onlygo:v2 binke01-3 Shutdown Shutdown 17 minutes ago w4sz99i5xfgm \\_ onlygo-svc.2 wpxun/onlygo:v1 binke01-2 Shutdown Shutdown 41 minutes ago jsbd3x38qouo onlygo-svc.3 wpxun/onlygo:v1 binke01-2 Running Running 17 minutes ago xl3fldhit8s4 \\_ onlygo-svc.3 wpxun/onlygo:v2 binke01-2 Shutdown Shutdown 17 minutes ago fe5bh6j2o6jq \\_ onlygo-svc.3 wpxun/onlygo:v1 binke01-3 Shutdown Shutdown 41 minutes ago ig89pa0ylmrf onlygo-svc.4 wpxun/onlygo:v1 binke01-1 Running Running 15 minutes ago tmnwlqvydpgn \\_ onlygo-svc.4 wpxun/onlygo:v2 binke01-1 Shutdown Shutdown 15 minutes ago vyv2hj1q8tdn \\_ onlygo-svc.4 wpxun/onlygo:v1 binke01-1 Shutdown Shutdown 38 minutes ago ud4kn771erry onlygo-svc.5 wpxun/onlygo:v1 binke01 Running Running 18 minutes ago 7jcq34xnxhpw \\_ onlygo-svc.5 wpxun/onlygo:v2 binke01-3 Shutdown Shutdown 18 minutes ago usrztf48878j \\_ onlygo-svc.5 wpxun/onlygo:v1 binke01 Shutdown Shutdown 40 minutes ago lxjhy78nsn45 onlygo-svc.6 wpxun/onlygo:v1 binke01-4 Running Running 10 seconds ago nzc6wenlsl8o onlygo-svc.7 wpxun/onlygo:v1 binke01-4 Running Running 10 seconds ago fz51eign4ms1 onlygo-svc.8 wpxun/onlygo:v1 binke01-3 Running Running 15 seconds ago ma78euhri5p6 onlygo-svc.9 wpxun/onlygo:v1 binke01-1 Running Running 15 seconds ago 6.2 更新节点   docker node update --role worker binke01-3 把节点转成工作节点，转换后至少还存在一个管理节点就可以，比如有两个管理节点，把 leader 转成工作节点，则另一个管理节点会转成 leader；\u0026ndash;role 有两个选项 worker|manager\n  docker node update --availability drain binke01-4\n   availability string(\u0026ldquo;active\u0026quot;|\u0026quot;pause\u0026quot;|\u0026quot;drain\u0026rdquo;) 其中 drain 即是排空节点，把节点上的副本移到其它节点，同时阻止在该节点上分配新的副本；  6.3 删除节点  离开 Swarm 群，可以在任意节点上操作，使其连通状态为 Down；  [feixin10@binke01-4 ~]$ docker swarm leave Node left the swarm. 查看节点列表，status 表示节点连通状态有 Ready|Down；availability 表示可编排状态，有 active|pause|drain 三种状态。\n\u0026gt; docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION 0kjqucshibpm35zhq7kizldp0 binke01 Ready Active 18.09.3 qr7i763tagufpcyn4qf37b5fs * binke01-1 Ready Active Reachable 18.09.5 186b6a4dpvy8alzdfuf3jtrv4 binke01-2 Ready Active 18.09.5 ogvwkwq0zxw3s05shey2ruzqa binke01-3 Ready Active Leader 18.09.5 zadyw6n9o8h14x0flsjmrkoyl binke01-4 Down Drain 18.09.5 除此之外，节点的 Docker 服务未开启时也是 Down 状态。\n删除节点，只有 Down 状态下才可能删除(不在乎 availability 的状态)，另外还有强制删除但不建议使用； docker node rm binke01-4  7 删除服务 docker service rm onlygo-svc，没有任何警告，直接删除服务和所有的副本，但不会删除拉取过的镜像。\n参考文献 [1] Nigel Poulton. 深入浅出 Dokcer. 版次：2019年4月第1版 [2] Getting started with swarm mode. https://docs.docker.com/engine/swarm/swarm-tutorial/\n","id":14,"section":"posts","summary":"概括来说，Swarm 有两个核心组件： 企业级的 Docker 安全集群 微服务应用编排引擎 1 环境 Swarm 对主机（节点）的要求并不高，我在4台 f1-micro（1 个 v","tags":["Docker","容器"],"title":"Docker Swarm 集群","uri":"https://blog.jemper.cn/2019/04/docker-swarm-mode/","year":"2019"},{"content":"将应用整合到容器中并且运行起来的这个过程，或者把应用打包成为一个镜像的过程，称为容器化，有时也叫作“Docker 化”。容器化核心就是创建镜像，创建镜像有两种方式，一种是 commit 容器，还有一种是使用 Dockerfile 快速创建自定义镜像。\ndocker compose 能够在 Docker 节点上，以**单引擎模式(Single-Engine Mode)**进行多容器应用的部署和管理。它区别于 Swarm 和 Kubernetes 可以进行多引擎多容器应用部署(在 docker 中叫 swarm mode，Compose does not use swarm mode to deploy services to multiple nodes in a swarm)。compose 和 Kubernetes 其适用范围不同，所以不适合作对比。\n1 shell 基础   熟悉 shell 语法，比如$的应用规则：$?(上一个命令的返回值)、$0 $1 $2（表示指令，参数1、参数2）、$() = ` `、$NAME（引用变量）等，脚本经常会先执行 set -xe（e 表示单个命令执行返回非零时立即退出，包括函数返回非零，x 执行指令前会先显示该完整的命令）。\n  理解程序运行的原理，shell 是一个等待输入的程序，输入的命令有外部命令和内部命令之分；外部命令是通过系统调用或独立的程序实现的，如 sed、awk 等。内部命令是由特殊的文件格式（.def）所实现，如 cd、history、exec、source 等。其接收到指令后有三种方式运行： （1）在当前的 shell 上运行 （2）fork 新的 shell 运行，环境变量会从父进程传递给子进程 （3）系统调用 exec 函数簇执行，一般是 fork 父进程，父子进程拥有共同的地址空间，只有当子进程需要写入数据时(如向缓冲区写入数据),这时候会复制地址空间，复制缓冲区到子进程中去。 同理，运行一个脚本也有三种方式一一对应上面三种，当然前提是有一个已经在运行的 shell。 （1）source：也就是 . 命令，在当前上下文中执行脚本，不会生成新的进程。脚本执行完毕，shell 继续等待输入。影响上下文； （2）./script.sh（以 #!/bin/sh 开头） 与 sh script.sh（无需 #!/bin/sh 开头） 等效，当前shell是父进程，fork 子 shell 进程，在子 shell 进程中执行脚本。脚本执行完毕，退出子shell，回到当前shell。不影响上下文。 （3）执行完不返回 shell，直接退出 shell，关闭上下文。 如以下脚本，通过 . jump.sh 后返回到原来的 shell 其当前目录也变了成 /，而 ./jump.sh 或 sh jump.sh 则不会影响上下文。\n  cat \u0026lt;\u0026lt;EOF \u0026gt; jump.sh #!/bin/sh cd / pwd echo $HOME EOF 2 Dockerfile Dockerfile 具有众多的指令。一般分为四部分：基础镜像信息、维护者信息、镜像操作指令和容器启动时执行指令。\n2.1 命令 镜像常用 Dockerfile 指令文件创建，创建命令：docker image build -t NAME[:TAG] Dockerfile-Path。它几乎无需指定参数，但还是简单说明几个：\n \u0026ndash;rm 成功 build 后删除中间运行的容器，默认为true。还有一个 \u0026ndash;force-rm，无论成不成功都删除，不建议使用，因为失败了还可以通过容器调试。 注意这里要区别于中间镜像，在 build 中 ---\u0026gt; cdf98d1859c1 表示依赖的镜像或者中间镜像，---\u0026gt; Running in 1d2485ce71e9 表示中间容器。 \u0026ndash;no-cache 不走缓存。构建的时候会搜索开始到当前的指令是否有缓存，有则直接拿来用提升速度，但有注意 COPY、ADD 指定即使没变也会检查复制的文件有没有改动过。既然缓存会自动判断，那为什么要设置不走缓存，那是因为像 RUN，即使命令都没变，但可能因为时间、远程版本变化导致运行结果也有变化，这时候就可以指定不走缓存。 \u0026ndash;squash，压缩层，即把所有的层压缩成一个层，这对本地使用还好，对需要 pull、push 的增加了网络负担。所以尽量不用，而是在 Dockerfile 里选择性的合并指令达到压缩层的目的。像 git 也有该参数，同样各有利弊。  2.2 指令 有些指令会新建镜像层，有些只会增加元数据，关于如何区分命令是否会新建镜像层，一个基本的原则是，**如果指令的作用是向镜像中增添新的文件或者程序，那么这条指令就会新建镜像层；**如果只是告诉 Docker 如何完成构建或者如何运行应用程序，那么就只会增加镜像的元数据。所以并非所有的 RUN 都会有创建新层，比如 RUN echo \u0026quot;no data\u0026quot; 就不会创建新的层。\n需要注意的一点是镜像没有任何运行时的宿主机信息，比如不可能有端口映射，端口映射一定是在启动容器的时候才会指定，否则宿主机的端口未知是否可用，则容器也未知是否可用。\n下面列举一些常用的指令：\n FROM：指定基础镜像，推荐 Alpine，只有 5M 左右； RUN：有 shell 和 exec 两种执行方式：  RUN \u0026lt;command\u0026gt; //shell RUN [\u0026quot;executable\u0026quot;, \u0026quot;param1\u0026quot;, \u0026quot;param2\u0026quot;] //exec  COPY、ADD：COPY 只能复制宿主机文件，ADD 支持远端复制，并且会自动解压压缩文件，不过不会删除压缩文件。 EXPOSE、-p、-P：设置镜像暴露端口，容器启动时就会监听的端口，但是不导出（publish）端口到主机，不过容器之间 link 可以使用暴露的端口通信。docker run 命令的 -p 和 -P 表示是否设置容器的端口到宿主机的映射； 其中 -P 表示将 EXPOSE 暴露的端口映射到本地主机的随机端口；-p 设置容器新暴露端口并映射到宿主机的指定端口。  EXPOSE 80 //Dockerfile ------------------------ PORTS 80/tcp, 0.0.0.0:91-\u0026gt;8080/tcp //80端口只是暴露没有导出，只能用于容器之间的 link；-p 91:8080 0.0.0.0:32768-\u0026gt;80/tcp, 0.0.0.0:90-\u0026gt;8080/tcp // 80端口导出到宿主机随机端口；-P -p 90:8080  ENTRYPOINT、CMD、docker run 的命令： Dockerfile 中应至少一条 CMD 或 ENTRYPOINT 指令，如果有多条，他们都是最后一条生效；而且逻辑是 CMD 在后面，如果不写在后面也不会报错，不过还是会追回在 ENTRYPOINT 参数后面； CMD 和 docker run 本质上是一样的，只不过前者是默认，后者会覆盖前者；如果有 ENTRYPOINT 指令，则他们只能是 ENTRYPOINT 指令的追加参数； docker run 中加入 \u0026ndash;entrypoint，会覆盖镜像中的 ENTRYPOINT； 当使用容器作为一个程序容器时，应使用 ENTRYPOINT 定义入口程序。  CMD [\u0026quot;executable\u0026quot;, \u0026quot;param1\u0026quot;, \u0026quot;param2\u0026quot;] //exec， json数组格式，所有参数都必须有双引号 CMD [\u0026quot;param1\u0026quot;, \u0026quot;param2\u0026quot;] // 结合 ENTRYPOINT 指令追加参数 CMD command param1 param2 //shell ENTRYPOINT [\u0026quot;executable\u0026quot;, \u0026quot;param1\u0026quot;, \u0026quot;param2\u0026quot;] //exec， json数组格式，所有参数都必须有双引号 ENTRYPOINT command param1 param2 //shell   WORKDIR：需要注意如果是相对路径，则会以上一条绝对路径为前缀，像 cd 改变目录的功能。\n  USER：如果容器中的应用程序运行时不需要特殊权限，则可以通过 USER 指令把应用程序的所有者设置为非 root 用户。\n  RUN groupadd -r postgres \u0026amp;\u0026amp; useradd -r -g postgres postgres USER postgres   ENV：有两种方式，但推荐第二种减少中间镜像数量\n  ENV \u0026lt;key\u0026gt; \u0026lt;value\u0026gt;\n  ENV \u0026lt;key1\u0026gt;=\u0026lt;value1\u0026gt; \u0026lt;key2\u0026gt;=\u0026lt;value2\u0026gt;，这种情况字符串有空格一定要用双引号括起来\n  VOLUME、-v：挂载卷，启动容器的时候会把容器中的目录挂载到宿主机中。docker run 的 -v 是可以指定宿主机的目录名的。\n  2.3 Dockerfile 最佳实践  让层尽量的少，加快编译时间；但是保留共用层，避免 push 或 pull 重复的数据   RUN 时一般使用 \\ 把长的指令分成多行，把多个 RUN 指令合并成一个 RUN 指令，达到压缩镜像层的目的； ENV ENV \u0026lt;key1\u0026gt;=\u0026lt;value1\u0026gt; \u0026lt;key2\u0026gt;=\u0026lt;value2\u0026gt; 减少中间镜像层  让镜像的大小尽量的小，只留必要的文件，其它的如构建工具、依赖、代码等如对服务没有帮助则应该删除   运行结束后应该清理缓存和中间工具使得每一层的 SIZE 最小，这主要有两种方式：  编写命令清理不需要的数据，php 镜像就是这么干的，apk add \u0026ndash;no-cache \u0026ndash;virtual .build-deps 和 apk del .build-deps；phpize 和 docker-php-source delete等； 建造者模式：把有用的数据移到最小版本，需要多个 Dockerfile； 多阶段构建方式：利用 COPY \u0026ndash;from 参数指定要复制指定的数据，只需要一个 Dockerfile。    3 Compose 3.1 安装 三大版本的关系：docker compose 版本、Compose file format 版本和 Docker Engine 版本，可以参见 github 库 docker/compose。比如 docker compose v1.21.0 只能支持 Compose file format v3.6 基于 Docker Engine v18.02.0+，docker compose v1.22.0 才增加了 Compose file format v3.7，而且 docker-compose.yml specification v3.7 版本要求 Docker Engine 在 v18.06.0 以上；目前最新的 docker compose v1.24.0 只能支持 Compose file format v3.7 基于 Docker Engine v18.06.0+。 版本号如果写成 version: \u0026lsquo;3\u0026rsquo;，则表示为 3.0 版本。关于 docker compose file format 的差异可以看 Compose file versions and upgrading。\ndocker compose 是收购 fig，它是一个 python 工具，按官方下载就可以了；升级也很简单，重新下载一次就可以了。\n3.2 命令  up：启动，-d 表示后台运行； down：关闭，会把容器和网络删除，但不会删除卷； logs：如果加了 -d 参数，可以通过该命令查看日志，但日志的输出是依赖于服务内部的设计的； build：重建镜像用 docker-compose build or docker-compose up --build；  3.3 指令 需要注意的是可以在 Compose 文件中用$直接引用宿主机的变量，而 Dockerfile 文件是不行的，$只是引用 ENV 定义的变量。下面以 3.7 版本格式列举一些常见的指令：\n一级指令：\n version：版本号，规定版本的格式 services：服务 build：本地找，找不到就构建，如果指定 image 则用其值，如没有就用 “服务名:latest“ image：如指定 build 则其规则看 build；如未指定 build，则本地找，本地没有上 hub 拉取 environment：在 docker-compose 运行时导入容器，这极大的方便了引用宿主机环境变量 networks：网络 volumes：卷  4 实战部署 本节以一个计数器进行实战部署，目录结构：\n\u0026gt; tree gomicro gomicro ├── config.yaml ├── docker-compose.yml ├── Dockerfile ├── main.go └── service ├── config.go └── redis.go 除了 Dockerfile 和 docker-compose.yml，其余的都是业务代码。\n4.1 业务代码  使用 go 作为 web 服务器，开发路径为 $GOPATH/github.com/wpxun/gomicro，开发环境和生产环境保持一致，需要发布 80 端口； 使用 redis 存储计数，并作持久存储，该服务只供 go web 服务请求，所以暴露的端口(6379)不需要发布；  cat \u0026lt;\u0026lt;EOF \u0026gt; $GOPATH/github.com/wpxun/gomicro/main.go package main import ( \u0026quot;fmt\u0026quot; \u0026quot;github.com/wpxun/gomicro/service\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;os\u0026quot; \u0026quot;strconv\u0026quot; ) func IndexHandler(w http.ResponseWriter, r *http.Request) { redis := service.GetRedis() val, err := redis.Incr(\u0026quot;count\u0026quot;).Result() if err != nil { panic(err) } host := os.Getenv(\u0026quot;FROMHOSTNAME\u0026quot;) //读取 docker-compose.yml 中引入到容器的环境变量 fmt.Fprintln(w, \u0026quot;hello world \u0026quot;+ host +\u0026quot;, visitors = \u0026quot; + strconv.FormatInt(val, 10) ) } func main() { http.Handle(\u0026quot;/pattern\u0026quot;, http.HandlerFunc(IndexHandler)) http.ListenAndServe(\u0026quot;:80\u0026quot;, nil) } EOF cat \u0026lt;\u0026lt;EOF \u0026gt; $GOPATH/github.com/wpxun/gomicro/config.yaml Redis: DialTimeout: 2000000000 #连接超时设定(s)，默认200ms Network: tcp #网络连接协议 Address: redis:6379 #连接地址(带端口) Password: #密码 Database: 0 #数据库，默认0 EOF cat \u0026lt;\u0026lt;EOF \u0026gt; $GOPATH/github.com/wpxun/gomicro/service/redis.go package service import ( \u0026quot;github.com/go-redis/redis\u0026quot; ) func GetRedis() *redis.Client { return redis.NewClient(\u0026amp;redis.Options { Addr: Conf.Redis.Address, Password: Conf.Redis.Password, DB: Conf.Redis.Database, Network: Conf.Redis.Network, DialTimeout: Conf.Redis.DialTimeout, }) } EOF cat \u0026lt;\u0026lt;EOF \u0026gt; $GOPATH/github.com/wpxun/gomicro/service/config.go package service import ( \u0026quot;fmt\u0026quot; \u0026quot;gopkg.in/yaml.v2\u0026quot; \u0026quot;io/ioutil\u0026quot; \u0026quot;time\u0026quot; ) type confstruct struct { Redis struct { Address string `yaml:\u0026quot;Address\u0026quot;` Database int `yaml:\u0026quot;Database\u0026quot;` DialTimeout time.Duration `yaml:\u0026quot;DialTimeout\u0026quot;` Network string `yaml:\u0026quot;Network\u0026quot;` Password string `yaml:\u0026quot;Password\u0026quot;` } `yaml:\u0026quot;Redis\u0026quot;` } var Conf confstruct func init() { GetYaml(\u0026quot;config\u0026quot;, \u0026amp;Conf) } func GetYaml(filename string, out interface{}) { yamlFile, err := ioutil.ReadFile(fmt.Sprintf(\u0026quot;%s.yaml\u0026quot;, filename)) if err != nil { fmt.Println(\u0026quot;Read config file error:\u0026quot;, err.Error()) } err = yaml.Unmarshal(yamlFile, out) if (err != nil) { fmt.Println(\u0026quot;Unmarshal config file error:\u0026quot;, err.Error()) } } EOF 4.2 容器化 这一步我们只需要把 go web 服务器容器化，而 redis 我们直接用官方的容器。\ncat \u0026lt;\u0026lt;EOF \u0026gt; $GOPATH/github.com/wpxun/gomicro/Dockerfile # 多阶段构建 # 第一阶段，391MB，编译前准备：go 和 git 工具、代码依赖库 FROM golang:1.12.4-alpine3.9 AS front RUN set -xe \u0026amp;\u0026amp; \\ apk add git \u0026amp;\u0026amp; \\ go get -v github.com/go-redis/redis \u0026amp;\u0026amp; \\ go get -v gopkg.in/yaml.v2 # 分成两次 RUN 目的是可复用上面的缓存，编译 go 代码 COPY . /go/src/github.com/wpxun/gomicro RUN set -xe \u0026amp;\u0026amp; \\ go install github.com/wpxun/gomicro # 第二阶段，14.6MB；仅仅复制了可执行程序和程序的配置文件 FROM alpine:3.9 ENV GOM_VERSION 1904.1 COPY --from=front /go/bin /go/src/github.com/wpxun/gomicro/config.yaml /go/bin/ EXPOSE 80 WORKDIR /go/bin CMD [\u0026quot;/go/bin/gomicro\u0026quot;] EOF 我们采用多阶段构建，最终只需要 go web 服务器的可执行程序和启动时需要读取的配置文件，这里我把他们放在 /go/bin 目录下，因为 go 程序中基于当前目录读取的 config.yaml，所以需要设置工作目录为配置文件所在的目录 WORKDIR /go/bin。\n4.3 单引擎部署 这一步我们需要把 go web 服务和 redis 服务进行编排管理，两者的通信需要配置同一个 networks。而且还在运行容器的时候添加 FROMHOSTNAME 环境变量等于宿主机的 HOSTNAME 环境变量，这里要注意，运行时的 compose 里的环境变量是宿主机的环境变量，而构建时的 Dockerfile 不能包含宿主机的信息（build 的时候可以通过 \u0026ndash;build-arg 传变量）。\ncat \u0026lt;\u0026lt;EOF \u0026gt; $GOPATH/github.com/wpxun/gomicro/docker-compose.yml version: \u0026quot;3.7\u0026quot; services: gomicro: build: . image: wpxun/gomicro:v1 environment: FROMHOSTNAME: $HOSTNAME ports: - target: 80 published: 80 networks: - counter-net redis: image: \u0026quot;redis:5.0.4-alpine3.9\u0026quot; networks: - counter-net networks: counter-net: volumes: counter-vol: EOF 4.4 浏览器看结果 docker-compose up -d 运行\n\u0026gt; docker ps -a IMAGE COMMAND CREATED STATUS PORTS NAMES gomicro_gomicro \u0026quot;/go/bin/gomicro\u0026quot; 3 hours ago Up 2 hours 0.0.0.0:80-\u0026gt;80/tcp gomicro_gomicro_1 edis:5.0.4-alpine3.9 \u0026quot;docker-entrypoint.s…\u0026quot; 3 hours ago Up 2 hours 6379/tcp gomicro_redis_1 访问 http://\u0026lt;ip\u0026gt;/pattern 即可以看到打印的次数加 1。如果容器被 stop，计数次数保留，如果容器被 down 掉（也就是容器被删除），则计数丢失。当然可以把 redis 的数据保存在卷中，这样即使容器被删除，redis 持久化数据还在卷中，下次重启可以挂载。\n参考文献 [1] Nigel Poulton. 深入浅出 Dokcer. 版次：2019年4月第1版 [2] 廖煜 晏东. Docker 容器实战. 版次：2016年11月第1版 [3] Dockerfile 最佳实践. https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/\n","id":15,"section":"posts","summary":"将应用整合到容器中并且运行起来的这个过程，或者把应用打包成为一个镜像的过程，称为容器化，有时也叫作“Docker 化”。容器化核心就是创建镜像","tags":["Docker","容器"],"title":"Dockerfile 应用容器化及 Compose 部署应用","uri":"https://blog.jemper.cn/2019/04/docker-single-engine-mode/","year":"2019"},{"content":"目前，以容器技术为代表的应用形态和以虚拟化为代表的系统形态完美融合于 OpenStack 生态圈之上，Kubernetes 是用于自动部署，扩展和管理容器化应用程序的开源系统(基于容器技术的分布式架构)，Kubernetes 底层支持两种容器技术 Docker 的 containerd 和 CoreOS 的 rkt。 Docker 主要以 linux 内核的 namespace 和 cgroup 等特性为基础，保障进程或者进程组处于一个隔离、受限、安全的环境之中。Docker 用 Go 实现，并在容器技术之中有风靡之势。自 2007 年 cgroups 合并至 linux 内核 2.6.24 版本，2008 年 LXC 诞生，2009 年 Go 发布，2013 年 Docker 开源，2015 年 Kubernetes 发布。到 2018 年已经有不少企业应用于生产环境；当然不免有唱衰的 再见 docker。\n1 诞生与发展 Docker 和 Kubernetes 一样都是新生事物，处于快速迭代期，所以很多规范、架构、功能一直在更新中。书籍和网上的文章淘汰速度也很快，特别是 v1.13.0（2017年）前的版本其知识点并不一定适用于最新的版本。所以这里有必要对一些重要版本或时间点作一些说明：\n 2013-03，发布 Docker Engine 项目 v0.1.0； 2014-03，v0.9.0，用 libcontainerd 取代了早期 Docker 架构中的 LXC； 2014-12，CoreOS 由于与 Docker 对于容器的发展方向见解不合，自立门户建立了 rkt 容器项目和 AppC 容器标准，该标准为后来的容器统一标准 OCI 的诞生奠定了基础。 2015-04，CoreOS 成为 Kubernetes 的战略合作平台，并共同推出了 CoreOS+rkt+Kubernetes 的新项目 Tectonic(和 Swarm 同级产品)。 2015-06，Linux 基金会出面调和，成立开放容器计划（The Open Container Initiative，前身是 OCP），谷歌、CoreOS 及 Docker 都加入 OCI 开放标准，Docker 引擎往模块化设计方向走。 2015-07，Kubernetes v1.0 正式发布； 2015-08，Linux 基金会宣布成立 CNCF，也就是原生云计算基金会(Cloud Native Computing Foundation)，原生云应用和服务成为焦点。有毕业、孵化中、初级三个级别项目，比如 Kubernetes、containerd 是毕业项目（目前是6个），gRPC 是孵化项目；现有 300 多个成员，国内包括阿里、腾讯、百度。 2016-02，v1.10.0，对 images 和 layers 采用了新的 content-addressable storage 方式，镜像的结构变得更加灵活； 2016-04，v.1.11，Docker 由 4 个执行程序 docker, docker-containerd, docker-containerd-shim and docker-runc 组成； 2016-07，v1.12.0，把 docker 执行程序拆分成 docker(docker client)程序和 dockerd(docker daemon)程序。 2016-12，containerd 捐献给了云原生计算基金会(CNCF)，CorsOS 的 rkt 也捐献给 CNCF；containerd 和 rkt 都是 Container Runtime，不过后者是孵化中项目。 2017-01， v1.13.0 API 1.25 开始支持 docker stack； 2017-02，v1.13.1 版本之后分化成了 Community Edition(CE) 和 Enterprise Edition(EE) 两个版本，版本从 v17.04.0 开始，这也算是 docker 商业化的一个探索；而且对命令进行了整合，移除了 docker daemon 命令，并把之前的顶级命令归类到 Management Commands 中，使得命令的含意更加清晰。 2017-04，Docker 项目正式命名为 Moby 项目，并遵循开放容器计划（The Open Container Initiative, OCI）规范，比如容器运行时格式和镜像格式等。Docker 公司为了让 Docker Engine 项目得以生存，逐渐放弃自己独家控制权； 2017-12，containerd 并发布了 v1.0 版本。 2018-09，v18.09.0，The client and container runtime are now in separate packages from the daemon in Docker Engine 18.09. Users should install and update all three packages at the same time to get the latest patch releases. For example, on Ubuntu: sudo apt install docker-ce docker-ce-cli containerd.io，这里的 docker-ce 相当于 deamon，daemon 去掉了 client 和 containerd，可以说是历史以来最简化，目前 daemon 还剩下镜像管理、镜像构建、REST API、身份验证、安全、核心网络以及编排。Docker 引擎的模块化工作仍在进行中。  2 引擎架构 2.1 Docker vs 虚拟机 容器比虚拟机轻量级的原因是共享宿主机的操作系统内核；而虚拟机各自运行一个完整的 Guest OS 并通过 hypervisor 连接到宿主机。所以 Docker 的 linux 内核版本一定是跟宿主机一样，不同的是操作系统发行版本。下面引用 docker 文档图片：\n{% img http://img.jemper.cn/2019/04/Container@2x.png 300 %} {% img http://img.jemper.cn/2019/04/VM@2x.png 300 %}\n2.2 引擎模块 一般我们用 Docker(大写D)表示整个 Docker，docker 和 dockerd 表示执行程序，docker ≈ docker client，dockerd ≈ docker daemon，docker engine ≈ client + daemon + containerd + runc，daemon ≈ 镜像管理、镜像构建、REST API、身份验证、安全、核心网络以及编排。总体逻辑如下图： {% img http://img.jemper.cn/2019/04/docker_engine.jpeg 300 %}\n2.3 模块通信 综合 systemctl status docker、systemctl status containerd 等可以理解以下内容：\n /usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock，dockerd 通过 -H 连接到 fd://，并用 \u0026ndash;containerd 参数指定容器运行时； API listen on /var/run/docker.sock，dockerd 提供 API 接口并开启监听； docker -H unix:///var/run/docker.sock，docker 通过 -H 连接到 dockerd(docker daemon); docker 默认是 unix socket，所以只能同一宿主机调用，远程调用可以改为 TCP socket。 守护进程：dockerd -H tcp://0.0.0.0:2375 客户进程：docker -H tcp://\u0026lt;宿主机 IP\u0026gt;:2375 需要注意的时，守护进程改用 tcp 提供调用后，客户进程即使在本机也需要通过 tcp 去调用。  docker 和 dockerd 的通信可以用 socat 进行抓包。socat 是一个强大的代理命令，能让用户在两个几乎任意类型的通道之间中继数据。groups 包括 FD,SOCKET,LISTEN,CHILD,RANGE,IP4,IP6,UDP,TCP 等，操作如下：\nsocat -v unix-listen:/tmp/dockerapi.sock unix-connect:/var/run/docker.sock docker -H unix:///tmp/dockerapi.sock images 2.4 启动容器的过程 dockerd 监听 client 的处理请求，并连接到 containerd 管理容器在宿主机的生命周期：start、stop、pause、rm 等。启动容器的过程如下图： {% img http://img.jemper.cn/2019/04/docker_step_container.jpeg 300 %}\n3 安装 安装 docker 和 docker-compose 等都很简单，按官方文档安装就可以了。一般安装最新社区版（CE）即可，安装完成后 systemctl start docker、systemctl enable docker 自启动。macOS 和 Windows 抛弃了过时的 Docker Toolbox，采用桌面版安装。一般 linux 服务器只安装 Docker Engine(Docker 引擎)，Mac 的桌面版包括了 Docker 引擎、Compose、Machine、Notary。\n最好通过非 root 用户使用 Docker，如有提示：Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock; 处理方式文档中有说明 By default, a unix domain socket (or IPC socket) is created at /var/run/docker.sock, requiring either root permission, or docker group membership. sudo usermod -aG docker USER_NAME\nDocker 文档提供了命令的使用 Command-Line Interfaces：\n Docker CLI(docker)：客户端，很多子命令 Daemon CLI(dockerd)：Docker 服务器，单命令 Machine CLI(docker-machine)：机器，很多子命令 Compose CLI(docker-compose)：编排，很多子命令 Dockerfile：构建文件，很多指令  4 镜像 4.1 images 和 layers 镜像是由 layer 有序列表和一些元数据组成的配置对象，layer 才是实际数据存储的地方（比如文件等，镜像之间是完全独立的，并没有从属于某个镜像集合的概念）。\nlinux 从启动到运行需要两个 fs： bootfs：用 bootloader 引导加载 kernel, 当 boot 成功后 kernel 被加载到内存中后 bootfs 就被 umount。 rootfs：kernel 利用 aufs 等添加系统的 rootfs 文件系统。\n  基础镜像： 从 scratch 空镜像构建起，在此基础上添加一层 rootfs，比如 centos-7-docker.tar.xz、alpine-minirootfs-3.9.3-x86_64.tar.gz。 这里要特别提一下 Alpine 基础镜像，稳定性和安全性都是挺可靠的，大小也在 5M 左右，未来 docker 官方也会用 Alpine 取代 Ubuntu。 基础镜像在启动后只会启动前台进程 bash。\n  其它镜像构建于基础镜像之上，即 baseImage/image/image\u0026hellip;，即在基础镜像层上利用 UnionFS（联合文件系统） 构建一层一层的只读文件系统层。 一般这类镜像在启动后都会启动守护进程。\n  镜像 ID: 每个镜像都是用唯一 IMAGE ID 标识，并用可视化别名 NAME[:TAG] 分类显示，ID 和别名是一对多的关系；如同 IP 对域名一样。TAG 不指定的时候表示默认值 latest，但是不推荐用默认值。 镜像 digest: 镜像摘要 When pushing or pulling to a 2.0 registry, the push or pull command output includes the image digest. You can pull using a digest value. You can also reference by digest in create, run, and rmi commands, as well as the FROM image reference in a Dockerfile.\n4.2 通过命令输出理解镜像  下面实例通过 Dockerfile 文件创建一个镜像。Dockerfile 文件内容如下：  FROM alpine:3.9 ENV ABC_VERSION 20190416 RUN apk add --no-cache --virtual .persistent-deps xz COPY test_copy /usr/local/etc ADD test_add /usr/local/etc RUN adduser -u 1001 -D -s /sbin/nologin pub EXPOSE 80 CMD [\u0026quot;/bin/sh\u0026quot;] 看创建时输出信息：  \u0026gt; docker build -t dtest:1.0 . Sending build context to Docker daemon 324.1kB Step 1/8 : FROM alpine:3.9 ---\u0026gt; cdf98d1859c1 Step 2/8 : ENV ABC_VERSION 20190416 ---\u0026gt; Running in 44ed3f9df47b Removing intermediate container 44ed3f9df47b ---\u0026gt; a06b9552c5be Step 3/8 : RUN apk add --no-cache --virtual .persistent-deps xz ---\u0026gt; Running in fd75b4d2063c fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/main/x86_64/APKINDEX.tar.gz fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/community/x86_64/APKINDEX.tar.gz (1/3) Installing xz-libs (5.2.4-r0) (2/3) Installing xz (5.2.4-r0) (3/3) Installing .persistent-deps (0) Executing busybox-1.29.3-r10.trigger OK: 6 MiB in 17 packages Removing intermediate container fd75b4d2063c ---\u0026gt; c421ae62239c Step 4/8 : COPY test_copy /usr/local/etc ---\u0026gt; 123b7cf03238 Step 5/8 : ADD test_add /usr/local/etc ---\u0026gt; f63b6fdf1995 Step 6/8 : RUN adduser -u 1001 -D -s /sbin/nologin pub ---\u0026gt; Running in dc2be3fdbc00 Removing intermediate container dc2be3fdbc00 ---\u0026gt; cd4063d6d004 Step 7/8 : EXPOSE 80 ---\u0026gt; Running in df6d2ebc78ef Removing intermediate container df6d2ebc78ef ---\u0026gt; a948ed8f4770 Step 8/8 : CMD [\u0026quot;/bin/sh\u0026quot;] ---\u0026gt; Running in cc2b4eaca4a6 Removing intermediate container cc2b4eaca4a6 ---\u0026gt; b28b2394f41a Successfully built b28b2394f41a Successfully tagged dtest:1.0 查看所生成的镜像和中间镜像，注意这里上下并不是依赖关系，仅仅是时间排序。  \u0026gt; docker images -a dtest 1.0 b28b2394f41a 37 seconds ago 5.83MB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; a948ed8f4770 37 seconds ago 5.83MB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; cd4063d6d004 37 seconds ago 5.83MB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; f63b6fdf1995 38 seconds ago 5.82MB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 123b7cf03238 38 seconds ago 5.82MB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; c421ae62239c 38 seconds ago 5.82MB \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; a06b9552c5be 41 seconds ago 5.53MB 查看构建历史  \u0026gt; docker history dtest:1.0 IMAGE CREATED CREATED BY SIZE COMMENT b28b2394f41a 2 minutes ago /bin/sh -c #(nop) CMD [\u0026quot;/bin/sh\u0026quot;] 0B a948ed8f4770 2 minutes ago /bin/sh -c #(nop) EXPOSE 80 0B cd4063d6d004 2 minutes ago /bin/sh -c adduser -u 1001 -D -s /sbin/nolog… 4.82kB f63b6fdf1995 2 minutes ago /bin/sh -c #(nop) ADD file:b803a882fa128cb8c… 9B 123b7cf03238 2 minutes ago /bin/sh -c #(nop) COPY file:24b874c6ab361858… 17B c421ae62239c 2 minutes ago /bin/sh -c apk add --no-cache --virtual .per… 291kB a06b9552c5be 2 minutes ago /bin/sh -c #(nop) ENV ABC_VERSION=20190416 0B cdf98d1859c1 7 days ago /bin/sh -c #(nop) CMD [\u0026quot;/bin/sh\u0026quot;] 0B \u0026lt;missing\u0026gt; 7 days ago /bin/sh -c #(nop) ADD file:2e3a37883f56a4a27… 5.53MB 查看 所有镜像的 layers  \u0026gt; docker inspect -f '{{json .RootFS.Layers}}' `docker history -q dtest:1.0` [\u0026quot;sha256:a464c54f93a9e88fc1d33df1e0e39cca427d60145a360962e8f19a1dbf900da9\u0026quot;,\u0026quot;sha256:35b7153bfc8fd8e90ff45282bf7abb9c43046400287a912693fa409d4278818c\u0026quot;,\u0026quot;sha256:e7d271231802f2b45e74efcb14dfefc5dd952e0c3914fa823a5ce2c30c461043\u0026quot;,\u0026quot;sha256:3077c11a618ad8a5da0802c9228e4c17f1b0759950fbcca7256d2bdcdcf922a4\u0026quot;,\u0026quot;sha256:1e9e99d1a6a31105f60b0e466627405b653f1159d27d3c0f265e5f3c9f99e812\u0026quot;] [\u0026quot;sha256:a464c54f93a9e88fc1d33df1e0e39cca427d60145a360962e8f19a1dbf900da9\u0026quot;,\u0026quot;sha256:35b7153bfc8fd8e90ff45282bf7abb9c43046400287a912693fa409d4278818c\u0026quot;,\u0026quot;sha256:e7d271231802f2b45e74efcb14dfefc5dd952e0c3914fa823a5ce2c30c461043\u0026quot;,\u0026quot;sha256:3077c11a618ad8a5da0802c9228e4c17f1b0759950fbcca7256d2bdcdcf922a4\u0026quot;,\u0026quot;sha256:1e9e99d1a6a31105f60b0e466627405b653f1159d27d3c0f265e5f3c9f99e812\u0026quot;] [\u0026quot;sha256:a464c54f93a9e88fc1d33df1e0e39cca427d60145a360962e8f19a1dbf900da9\u0026quot;,\u0026quot;sha256:35b7153bfc8fd8e90ff45282bf7abb9c43046400287a912693fa409d4278818c\u0026quot;,\u0026quot;sha256:e7d271231802f2b45e74efcb14dfefc5dd952e0c3914fa823a5ce2c30c461043\u0026quot;,\u0026quot;sha256:3077c11a618ad8a5da0802c9228e4c17f1b0759950fbcca7256d2bdcdcf922a4\u0026quot;,\u0026quot;sha256:1e9e99d1a6a31105f60b0e466627405b653f1159d27d3c0f265e5f3c9f99e812\u0026quot;] [\u0026quot;sha256:a464c54f93a9e88fc1d33df1e0e39cca427d60145a360962e8f19a1dbf900da9\u0026quot;,\u0026quot;sha256:35b7153bfc8fd8e90ff45282bf7abb9c43046400287a912693fa409d4278818c\u0026quot;,\u0026quot;sha256:e7d271231802f2b45e74efcb14dfefc5dd952e0c3914fa823a5ce2c30c461043\u0026quot;,\u0026quot;sha256:3077c11a618ad8a5da0802c9228e4c17f1b0759950fbcca7256d2bdcdcf922a4\u0026quot;] [\u0026quot;sha256:a464c54f93a9e88fc1d33df1e0e39cca427d60145a360962e8f19a1dbf900da9\u0026quot;,\u0026quot;sha256:35b7153bfc8fd8e90ff45282bf7abb9c43046400287a912693fa409d4278818c\u0026quot;,\u0026quot;sha256:e7d271231802f2b45e74efcb14dfefc5dd952e0c3914fa823a5ce2c30c461043\u0026quot;] [\u0026quot;sha256:a464c54f93a9e88fc1d33df1e0e39cca427d60145a360962e8f19a1dbf900da9\u0026quot;,\u0026quot;sha256:35b7153bfc8fd8e90ff45282bf7abb9c43046400287a912693fa409d4278818c\u0026quot;] [\u0026quot;sha256:a464c54f93a9e88fc1d33df1e0e39cca427d60145a360962e8f19a1dbf900da9\u0026quot;] [\u0026quot;sha256:a464c54f93a9e88fc1d33df1e0e39cca427d60145a360962e8f19a1dbf900da9\u0026quot;] Error: No such object: \u0026lt;missing\u0026gt; 删除镜像  \u0026gt; docker rmi dtest:1.0 Untagged: dtest:1.0 Step Deleted: sha256:b28b2394f41a2793a39ef52d5dd9d2baee25577ca186ae418f40788953461feb 8 Deleted: sha256:a948ed8f4770e63f048bd70cfeed285282b8f00cabd1e1f533a87f108bc732b0 7 Deleted: sha256:cd4063d6d0042155a55656769d862f245e52c3b4850c0058b9df57a2efb14f7e 6 Deleted: sha256:4e93fada93b2cd4957de03f29afcef7a1902f32bcfc2c1dc18981589908d17cc 删除6引用layer Deleted: sha256:f63b6fdf19958d210f4025bd8e7712674100c258dc9cec1989b694f8ce584bb6 5 Deleted: sha256:137b3352b7489125cefce90a8090e65784b80f26cfd1d1d3cd476b8aaaa1ec25 删除5引用layer Deleted: sha256:123b7cf032389fe77fbf9ceb824b78d193245994f4363a0afe54c22f022ef979 4 Deleted: sha256:24763812613715916999c97b6701f9efad57b5279037ada7379173cc4d3c2743 删除4引用layer Deleted: sha256:c421ae62239c98dd587bc7f5a9acb03ea2566a72025b21006c25373ec0c186d9 3 Deleted: sha256:6a9da1e2e4d23c4323fb85d14fe8ca6d030a367d712189f193638296f5c82f68 删除3引用layer Deleted: sha256:a06b9552c5be5fa3546e5eb25db5dccc196dfdb1fc3cd6f90def57bd0bac9488 2    Step 指令 中间容器 ID IMAGE ID Layers(只截前5字符) 说明     1 FROM - cdf98d1859c1 a464c 基础镜像 layer   2 ENV 44ed3f9df47b a06b9552c5be a464c 元数据   3 RUN fd75b4d2063c c421ae62239c a464c,35b71 新 layer   4 COPY - 123b7cf03238 a464c,35b71 e7d27 新 layer   5 ADD - f63b6fdf1995 a464c,35b71,e7d27 3077c, 新 layer   6 RUN dc2be3fdbc00 cd4063d6d004 a464c,35b71,e7d27,3077c,ef463 新 layer   7 EXPOSE df6d2ebc78ef a948ed8f4770 a464c,35b71,e7d27,3077c,ef463 元数据   8 CMD cc2b4eaca4a6 b28b2394f41a a464c,35b71,e7d27,3077c,ef463 元数据    根据以上分析可以看出，所有涉及的镜像仅由 5 layers 组成，第一个就是基础镜像的 layer，RUN、COPY、ADD、RUN 新增加 4 个 layers，从 history 的 SIZE 字段也可以看出。而 ENV、EXPOSE、CMD 只增加了元数据。关于如何区分命令是否会新建镜像层，一个基本的原则是，如果指令的作用是向镜像中增添新的文件或者程序，那么这条指令就会新建镜像层；如果只是告诉 Docker 如何完成构建或者如何运行应用程序，那么就只会增加镜像的元数据。所以并非所有的 RUN 都会有创建新层，比如 RUN echo \u0026quot;no data\u0026quot; 就不会创建新的层。 每一个指令都有相应的镜像，但是否生成中间容器要看指令看是否需要运行验证。 删除镜像的时候，会把新生成的7个镜像和4个 layers 都删除掉。对于拉取的镜像没有中间镜像，删除的时候就只主镜像和 layers。\n4.3 inspect image inspect 命令可以列出与镜像相关的信息，但是这些信息并不都是镜像的属性，而是对同一个镜像 ID 信息的归纳。以下列举一些说明:\n Id ：即配置对象本身的散列值，一般叫 IMAGE ID；这个 Id 是跟仓库等属性无关的，只与 layer 和 元数据有关。 RepoTags ：数组类型，格式为 name:tag；按名称和标签分组； RepoDigests ：数组类型，格式为 name@sha256:hex；按名称分组；在 push 后才会生成； RootFS ：layer 相关的信息，列有 layer 本身内容的散列值；不过要注意的是，push 到仓库后会进行压缩，所以远端会用分发散列值。  5 容器 容器是以镜像为模板，在镜像上添加一层可写的容器层就成为容器：baseImage/image/image\u0026hellip;/container，对容器的修改仅限于该容器的可写层。可以比喻镜像是类，容器是实例化的对象。\n6 常用命令   pull 拉取镜像，如docker pull php:7.1.3-fpm-alpine\n  push 推送镜像，注意使用前需要 docker login 登录\n  inspect/network inspect 查看镜像、容器和网络\n  ps 查看运行中的容器，常用的属性有 -a，还有其它的一些命令，如 docker rm `docker ps -a -q`、docker network inspect $(docker network ls -q)\n  images 查看镜像，除了正常的镜像，还有三类 \u0026lt;none\u0026gt; 镜像：\n   正常镜像的中间镜像，无法删除，因为被上层依赖。docker images -a 命令才看得到； dangling 镜像，即重复 build 会把原来正常的镜像变成 dangling，可以用 docker rmi $(docker images -f “dangling=true” -q) 删除； 没用的镜像，可以用 docker image prune -f 删除。   run 常用参数 docker run \u0026ndash;name nginx_server -d -p 80:80 \u0026ndash;link php:php -v /Users/mylxsw/Dockers/php/nginx.conf:/etc/nginx/nginx.conf \u0026ndash;volumes-from php nginx，需要注意的是：Docker容器后台运行,就必须有一个前台进程.容器运行的命令如果不是那些一直挂起的命令（比如运行top，tail），就是会自动退出的，以 nginx 为例，默认以交互方式在前台运行，要在后台运行有两种方式： （1）nginx -g \u0026ldquo;daemon off\u0026rdquo;. CMD 或者 ENTRYPOINT 两种方式 （2）nginx.conf 加入 daemon off. 这样 run 指令就只要 nginx 就可以了 -e：设置环境变量，有三种方式：-e MYVAR1 \u0026ndash;env MYVAR2=foo \u0026ndash;env-file ./env.list  也可以不加-d,并运行/bin/sh，依次输入ctrl+p、ctrl+q，就不会终止容器而只是退出。 \u0026ndash;restart string Restart policy to apply when a container exits (default \u0026ldquo;no\u0026rdquo;)\n  exec 进入容器，如 docker exec -it e97e3208d019 /bin/sh，在运行中的容器内部额外启动进程，参数和 run 类似。直接 exit 或 ctrl+D 不会退出容器\n  attach 可以附着到容器上，就相当于 run 的方式进入，直接 exit 或 ctrl+D 会退出容器\n  restart 或者 start 可以重启容器、stop 停止守护容器、rm 删除容器、rmi 删除镜像\n  build 基于基础镜像和 Dockerfile 构建新镜像，如 docker build -t wpxun/php:7.1.3 .\n  tag 打上标签\n  上面的命令基本够用了，更多的命令可以查阅官方文档 Command-Line Interfaces。\n7 本地仓库 部署一个本地仓库可以查阅官方文档 Docker Registry，需要注意的几点是：\n 采用 registry 官方镜像部署 一般把容器的镜像目录映射到宿主机 远程访问时开启 HTTPS，一般可以用 Let’s Encrypt、insecure registry（包括 HTTP 和 self-signed certificates）  7.1 服务端 最简单可以用 docker run -d -p 5000:5000 --restart=always --name registry registry:2.7.1 这种情况只能在宿主机下操作，为了在其它主机连接上仓库，必须用 TLS。\n接下来用自签名证书演示：\n$ mkdir -p certs $ openssl req \\ -newkey rsa:4096 -nodes -sha256 -keyout certs/domain.key \\ -x509 -days 365 -out certs/domain.crt -subj \u0026quot;/CN=192.168.56.113\u0026quot; #Be sure to use the name myregistrydomain.com as a CN. 可以加 -subj '/C=CN/ST=ShenZhen/L=NanShan/CN=\u0026lt;Ipaddress\u0026gt;' $ docker run -d \\ --restart=always \\ --name registry \\ -v `pwd`/registry:/var/lib/registry \\ -v `pwd`/certs:/certs \\ -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt \\ -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key \\ -p 5000:5000 \\ registry:2.7.1 7.2 客户端配置 客户端配置其实就是公钥配置。\n 对宿主机可以用 localhost:5000 连接仓库的操作，无需配置公钥 对其它 Docker 主机，需要信任该证书，把公钥放到相应的目录下 对 CentOS，把公钥放在 /etc/docker/certs.d/ 目录下，无需重启 Docker。 Copy the domain.crt file to /etc/docker/certs.d/\u0026lt;MyRegistry\u0026gt;:\u0026lt;Port\u0026gt;/ca.crt on every Docker host. You do not need to restart Docker. 对 MacOS 桌面版，把公钥放在 ~/.docker/certs.d 目录下，并重启 Docker ~/.docker/certs.d/\u0026lt;MyRegistry\u0026gt;:\u0026lt;Port\u0026gt;/ca.crt 对 curl，可以把公钥追加到 /etc/pki/tls/certs/ca-bundle.crt 中，也可以 \u0026ndash;cacert 指定证书，还可以加 -k 参数不安全访问 curl https://192.168.56.113:5000/v2/_catalog -k curl https://192.168.56.113:5000/v2/golang/tags/list -k  参考文献 [1] Nigel Poulton. 深入浅出 Dokcer. 版次：2019年4月第1版 [2] Lan Miell.等. Docker 实践. 版次：2018年2月第1版 [3] Explaining Docker Image IDs. https://windsock.io/explaining-docker-image-ids/\n","id":16,"section":"posts","summary":"目前，以容器技术为代表的应用形态和以虚拟化为代表的系统形态完美融合于 OpenStack 生态圈之上，Kubernetes 是用于自动部署，扩展和管理容器化应用程","tags":["Docker"],"title":"Docker Engine","uri":"https://blog.jemper.cn/2019/04/docker-engine/","year":"2019"},{"content":"有感而写，以后每月计划写一篇吧。\n关于多多益善的思考 看了这一篇文章 《从全栈工程师到全周期工程师》，全栈工程师是从前端到后端，从软件到硬件都懂的通才；之后 DevOps 流行，强调开发和部署的连贯性，所以逐渐有了全周期工程师，即一个人负责完整的软件生命周期：设计，开发，测试，部署，操作和支持。当然这两个概念的转变还只是限定于技术范畴，不知道哪天出现全能工程师，除了负责技术，还得负责运营和市场。近年来，语言不断从低级到高级，汇编、C语言、Go语言，那么代表门槛越来越低了吗？代表工程师更容易集多种技术于一身吗？这就如同以前关于技术广度和深度的讨论。宏观上看，从古到今有一个规律，即每18个月新增的数据量是18个月前数据量的总和，世界的科学知识（注意不是智慧）确实也在不断累积；对于一个单位的人其科学知识也在不断增长，但是不可能有宏观上的规律，毕竟人的时间是有限的，宏观上的时间是无限的（宇宙没有起点，也不会结束）。所以我更倾向于深度的学习，而且当某一项技术学到了较深的程度后，眼界就不一样了，再学其他的技术也容易达到类似的境界。然而现在的技术错综复杂，相互依赖，就以 Web 优化为例，运维和技术是高度融合的，这时候避免不了要有广度，从这一点，我也并不排斥广度学习，但是心中要有一把尺，知道达到多广就应该收敛学习。\n下面引用韩少功在深圳坪山图书馆开馆之际说的一段话： {% blockquote 韩少功, 深圳坪山图书馆开馆与读者分享与互动 2019/03/23 %} 读书越多越好这种说法并不可信，一个医生开出的处方不可能适合所有的病人，在知识爆炸的年代，不能浪费时间去“暴饮暴食”，要有针对性地选择知识，并与实践结合起来，知为行之始，行为知之成，要用我们的实践把书中的智慧、精神重新激活，才能变成有血有肉的东西，变成自身的智慧。 {% endblockquote %}\n关于学习阶段的思考 技术学习是有曲线周期的，我的理解是：指数增长 - 线性增长 - 水平停滞 - 重点突破，周期性的提升。\n 指数增长：当前技术门槛低的更低，高的不变或者更高，所以要入门其实越来越容易了，相关的学习资料也丰富。 线性增长：前期较为容易的技术阶段过去后，增长速度会放缓，第一可能会稍微进行广度学习，但是广度学习给人的技术感没有那么强烈；第二可能迷失了学习方向，比如后端人员花大量时间去学 JS、CSS等，未有体系的学习，没有高人指点；第三可能是工作性质的原因，公司要你学那些知识； 水平停滞：这个就是俗称的吃老本，大部分是重复性的工作，家庭生活，自身的懒惰。这些综合影响导致停滞不前，有时候你还能工作下去，但是遇到公司变动调整，你可以就在裁员的名单里面。而且这个阶段也比较不利于跳槽，如果不进行再学习，你很快会发现无处可跳了。 重点突破：能不能转折重新进入指数增长阶段，重点就在于能否有重点的突破，哪些是重点？这个很难界定，但是你能感觉到。比如一个 PHP 后端人员前期一直做业务编程，当进行总结得出编程模式的时候，当在 Linux 编程有所突破的时候，当翱翔于各种 RFC、密码学、算法的时候，你就会发现一下子知识有了长足的增长，有一种重点突破的感觉。  学习不在于一目十行，而在于人过留名，雁过留声。把你学的东西写出来，留给自己复习也好，当作他人阶梯也罢，你会发现这进步是实实在在的。\n关于时间换金钱的思考 最近有 996.icu 网站以 10 几万的 star 引起人们的关注，技术人员为自己的时间利益呐喊。其实这就是一个以时间换金钱的无奈之处。下面引用阮一峰的一段表述：\n{% blockquote 阮一峰,http://www.ruanyifeng.com/blog/2019/04/weekly-issue-50.html 2019/04/05 %} 前几天，我听一个广播节目。主持人问，现在很多人开网约车，这样能赚多少钱，能够赚到大钱吗？ 这个问题很容易回答，答案就是不能。出租车司机的收入，主要由营业时间的长短决定。基本上，一天开12个小时，就是比开6个小时，收入高出一倍。每天只有24个小时，因此收入存在上限，不可能偏离平均水平很远。 出租车是\u0026quot;时间换收入\u0026quot;的典型行业，投入的时间越多，收入越高，在家休息就没收入。很多行业都属于\u0026quot;时间换收入\u0026rdquo;，所有此类行业都赚不到大钱。因为你能用来交换的时间是有限的，而且进入中年以后，你就拿不出更多的时间来交换。开出租车赚零花钱，或者作为短期过渡，这是没问题的，但作为终身职业是很糟糕的。 我觉得，越来越多的程序员正在落入这个陷井，用编码的时间换取收入。只有不停地做项目，才能拿到钱。项目做得越多，收入越高。这个项目开发完了，公司又让他去干下一个项目。 忙了好几年，项目完成了一大堆，但是自己什么也没留下，以后的收入还要取决于从零开始的新项目。这样的话，你跟出租车司机有何两样，哪一天你不写代码了，不是照样没收入。 那些赚到大钱的人，没有一个是靠时间换取收入的。他们要么通过积累资产致富，要么购买他人的时间，为自己创造财富。你应该警惕，不要落入\u0026quot;时间换取收入\u0026quot;的陷井，不要只顾着为别人生产代码，而要注意积累自己的资产，以及适时开展属于自己的业务。 {% endblockquote %}\n这是一个新程序员要想的问题，更是一个老程序员要解决的问题。 当你在写代码的时候，别人在撰书写论文；当你在餐桌上只管品菜海侃的时候，别人在左右逢源，让所有宾客感到满意；你在内部会议上发言都显拘谨，他们面对突如其来的话筒也能侃侃而谈。总之，不是不够努力，就是努力不够；不是没有方向，就是斜风歪向。\n","id":17,"section":"posts","summary":"有感而写，以后每月计划写一篇吧。 关于多多益善的思考 看了这一篇文章 《从全栈工程师到全周期工程师》，全栈工程师是从前端到后端，从软件到硬件都懂的","tags":null,"title":"日记——几点思考（1）","uri":"https://blog.jemper.cn/2019/04/before-work/","year":"2019"},{"content":"摘要：在局域网需要靠物理地址发送给对应的主机，而到广域网是用的 IP 地址进行报文转发。IP 是点到点，负责发送给对应的主机，而 TCP 是端到端，负责发送给对应的应用程序。\n1 OSI 网络模型 一些英文缩写简称容易混淆，比如 MAC 有物理层的物理地址、数据链路层的介质访问控制、密码学的消息验证码、macOS苹果操作系统等，为了避免歧义，尽量要指明。\n 数据链路层 物理地址 每个网卡都有全球唯一的物理地址，路由器向同一个局域网的所有主机发送收到的数据包，本地的网卡比较一下包里指明的物理地址和自己的物理地址是否一致，如果一致则接收，否则丢弃。所以可以在局域网监听发给其它人的数据包，当然也有一些反监听手段。 物理地址由 6 字节表示，其中前三个字节由 IEEE 分配给厂商，后三个字节由厂商自己定。比如 XiaomiCo_9f:cb:9e (d8:ce:3a:9f:cb:9e)、Apple_b3:e2:83 (f4:5c:89:b3:e2:83)，厂商分配的物理地址段可以到 IEEE-SA - Registration Authority 查询。 物理地址前三字节是 IEEE 分配，但最终也是厂商决定，甚至用户在接入后可以自行篡改，所以很难全球唯一，但只要保证在局域网内唯一就可以了，设备在接入局域网的时候路由会检测，并把物理地址和内网 IP 进行绑定。 总之，由各个端点向上汇报唯一标识是不行的，得由一个中心机构向下分配唯一标识才能保证真正的唯一性。而外网 IP 就是由上往下分配的。  物理地址只应用于 Ethernet II（以太网 IEEE 802.3）层，它是一种局域网技术（以太网就是使用最广泛的局域网技术）。交换机等二层设备都是局域网内设备，交换机把端口与所连接适配器的物理地址关联起来（MAC 表），实现定点传输，其它适配器就不必查看网络上传输的全部帧。\n 网络层 ARP ARP（IP -\u0026gt; 物理地址） 和 RARP（物理地址 -\u0026gt; IP） 是地址解析协议，用于内网 IP 和物理地址的相互转换。ARP 是向局域网广播查询方式，RARP 是用查表方式。 可以通过 arp -an 查看计算机上的 arp 表。\n  网络层 traceroute 它可以追踪路由路径，原理是向目的主机发送 UDP 报文，目的主机返回 ICMP 报文，第 N 跳的 TTL 会设置为 N，每经过一个路由，TTL 就会减 1，当减到 0 时就不再继续转发直接丢弃并返回 ICMP 错误报文（code = 0，表示 TTL 达下限）。比如第一跳设置 TTL 为 1，由于它是 1，所以下一个路由器收到这个报文后，不会再继续转发了，会给源主机发送 ICMP 出错报文，就可以知道第一个路由的 IP 地址，同理，设置 TTL 为 2，就可以知道第二个路由的 IP 地址，依次类推。另外 traceroute 每一跳固定发三个 UDP 报文，三个 ICMP 返回的 IP 可能相同，也可能不同，若某个报文没有返回则输出 *。 另外，返回的 ICMP 也会设置 TTL，一般 64 跳的是 Linux、MacOS 系统，255 跳的是 Unix 系统，128 跳的是 Windows 系统。\n  traceroute 抓包\nTTL（time to live）是报文的生存时间，单位是秒（处理时间不到一秒算一秒，以现在的计算数度可以认为一秒等于一跳）,用于防止回环，位于 IP 报文的第 9 个字节，8bit，TTL的最大值是255，TTL的一个推荐值是 64；DNS TTL 是一条域名解析记录在DNS服务器上缓存时间。本质上都是表示存活时间。IPv6 首部采用了跳数限制，不再使用 TTL。\n 网络层 ping ping 是向目的 IP 发送 ICMP 报文，目的主机返回 ICMP 报文，打印列里有 ttl，就是返回时 TTL 减去经过的路由数剩下值，比如 49，那发出时 TTL 应该是 64，经过 15 跳（经过 15 个路由器）；比如 217，那发出时 TTL 应该是 255，经过 38 跳。\n  网络地址转换 NAT 如果 DHCP 服务器为客户端提供了一个 IP 地址，那么这个地址可能不是一个“公共的”、在 Internet 上唯一的 IP 地址。只要路由器自己具有在 Internet 上有效的 IP 地址，那么路由器就可以成为网络客户端的代理，从客户端接收请求，向 Internet 地址空间转发这个请求，进而接收来自于 Internet 地址空间的响应。许多路由器/DHCP 设备都可以提供 NAT 服务。NAT 设备从私有地址（10.0.0.0、172.16.0.0、192.168.0.0）来分配 IP 地址。这些地址一般意义上是不可路由的，只能通过地址转换来达到 NAT 客户端计算机。\n  跨网关需要默认网关转发（主机和路由器都有路由表），所以需要默认网关的物理地址。通过子网掩码计算是否属于不同的网关。对于本地网段上的主机来说，路由器的IP地址一般都是默认网关的地址。 对于转发，要区分路由和 NAT：\n NAT 地址转换，要区别接收和发送：1. 发送时目的 IP 不变，源 IP 改变；2. 接收时源 IP 不变，目的 IP 改变； 路由，则目的 IP 和源 IP 都不会改变。 当然 MAC 地址都是会改变的。  2 物理设备 一个设备工作在哪一层，关键看它工作时利用哪一层的数据头部信息。网桥工作时，是以介质访问控制（MAC） 头部来决定转发端口的，因此显然它是数据链路层的设备。具体说:\n 物理层：网卡、网线、集线器、中继器、调制解调器 数据链路层：网桥、交换机 网络层：路由器 网关工作在第四层传输层及其以上  常见设备如：\n 集线器是物理层设备,采用广播的形式来传输信息。 交换机就是用来进行报文交换的机器。多为链路层设备(二层交换机)，能够进行地址学习，采用存储转发的形式来交换报文。 路由器的一个作用是连通不同的网络，另一个作用是选择信息传送的线路。选择通畅快捷的近路，能大大提高通信速度，减轻网络系统通信负荷，节约网络系统资源，提高网络系统畅通率。    交换机和路由器的区别 交换机拥有一条很高带宽的背部总线和内部交换矩阵。交换机的所有的端口都挂接在这条总线上，控制电路收到数据包以后，处理端口会查找内存中的地址对照表以确定目的MAC（网卡的硬件地址）的NIC（网卡）挂接在哪个端口上，通过内部交换矩阵迅速将数据包传送到目的端口，目的MAC若不存在则广播到所有的端口，接收端口回应后交换机会“学习”新的地址，并把它添加入内部MAC地址表中。 使用交换机也可以把网络“分段”，通过对照MAC地址表，交换机只允许必要的网络流量通过交换机。通过交换机的过滤和转发，可以有效的隔离广播风暴，减少误包和错包的出现，避免共享冲突。 交换机在同一时刻可进行多个端口对之间的数据传输。每一端口都可视为独立的网段，连接在其上的网络设备独自享有全部的带宽，无须同其他设备竞争使用。当节点A向节点D发送数据时，节点B可同时向节点C发送数据，而且这两个传输都享有网络的全部带宽，都有着自己的虚拟连接。假使这里使用的是10Mbps的以太网交换机，那么该交换机这时的总流通量就等于2×10Mbps＝20Mbps，而使用10Mbps的共享式HUB时，一个HUB的总流通量也不会超出10Mbps。 总之，交换机是一种基于MAC地址识别，能完成封装转发数据包功能的网络设备。交换机可以“学习”MAC地址，并把其存放在内部地址表中，通过在数据帧的始发者和目标接收者之间建立临时的交换路径，使数据帧直接由源地址到达目的地址。 从过滤网络流量的角度来看，路由器的作用与交换机和网桥非常相似。但是与工作在网络物理层，从物理上划分网段的交换机不同，路由器使用专门的软件协议从逻辑上对整个网络进行划分。例如，一台支持IP协议的路由器可以把网络划分成多个子网段，只有指向特殊IP地址的网络流量才可以通过路由器。对于每一个接收到的数据包，路由器都会重新计算其校验值，并写入新的物理地址。因此，使用路由器转发和过滤数据的速度往往要比只查看数据包物理地址的交换机慢。但是，对于那些结构复杂的网络，使用路由器可以提高网络的整体效率。路由器的另外一个明显优势就是可以自动过滤网络广播。\n  集线器与路由器在功能上有什么不同? 首先说HUB,也就是集线器。它的作用可以简单的理解为将一些机器连接起来组成一个局域网。而交换机（又名交换式集线器）作用与集线器大体相同。但是两者在性能上有区别：集线器采用的式共享带宽的工作方式，而交换机是独享带宽。这样在机器很多或数据量很大时，两者将会有比较明显的。而路由器与以上两者有明显区别，它的作用在于连接不同的网段并且找到网络中数据传输最合适的路径。路由器是产生于交换机之后，就像交换机产生于集线器之后，所以路由器与交换机也有一定联系，不是完全独立的两种设备。路由器主要克服了交换机不能路由转发数据包的不足。 总的来说，路由器与交换机的主要区别体现在以下几个方面：\n    工作层次不同 最初的的交换机是工作在数据链路层，而路由器一开始就设计工作在网络层。由于交换机工作在数据链路层，所以它的工作原理比较简单，而路由器工作在网络层，可以得到更多的协议信息，路由器可以做出更加智能的转发决策。\n  数据转发所依据的对象不同 交换机是利用物理地址或者说MAC地址来确定转发数据的目的地址。而路由器则是利用IP地址来确定数据转发的地址。IP地址是在软件中实现的，描述的是设备所在的网络。MAC地址通常是硬件自带的，由网卡生产商来分配的，而且已经固化到了网卡中去，一般来说是不可更改的。而IP地址则通常由网络管理员或系统自动分配。\n  传统的交换机只能分割冲突域，不能分割广播域；而路由器可以分割广播域 由交换机连接的网段仍属于同一个广播域，广播数据包会在交换机连接的所有网段上传播，在某些情况下会导致通信拥挤和安全漏洞。连接到路由器上的网段会被分配成不同的广播域，广播数据不会穿过路由器。虽然第三层以上交换机具有VLAN功能，也可以分割广播域，但是各子广播域之间是不能通信交流的，它们之间的交流仍然需要路由器。\n  路由器提供了防火墙的服务 路由器仅仅转发特定地址的数据包，不传送不支持路由协议的数据包传送和未知目标网络数据包的传送，从而可以防止广播风暴。\n  3 网络测试 安装 telnet、telnet-server、xinetd，可以查看是否已经安装，有@前缀表示已经安装：\n\u0026gt; yum list | grep -E \u0026quot;xinetd|telnet\u0026quot; telnet.x86_64 1:0.17-64.el7 @base telnet-server.x86_64 1:0.17-64.el7 @base xinetd.x86_64 2:2.3.15-13.el7 @base systemctl enable telnet.socket systemctl start telnet.socket systemctl enable xinetd systemctl start xinetd\nless /etc/services | grep telnet\nfirewall-cmd \u0026ndash;add-service=telnet \u0026ndash;zone=public \u0026ndash;permanent\nhttps://sharadchhetri.com/2014/12/08/install-telnet-server-on-centos-7-rhel-7/\n","id":18,"section":"posts","summary":"摘要：在局域网需要靠物理地址发送给对应的主机，而到广域网是用的 IP 地址进行报文转发。IP 是点到点，负责发送给对应的主机，而 TCP 是端到端，负责发送","tags":["网络"],"title":"网络杂谈","uri":"https://blog.jemper.cn/2019/04/network/","year":"2019"},{"content":"HTTPS 整个知识体系非常庞大，我认识到的学习 HTTPS 的最佳步骤是：\n 基础部分：TCP/IP，可以参阅《TCP 传输原理》 安全部分：分别是学习密码学、OpenSSL命令行、TLS/SSL，可以参阅《密码学》和《SSL/TLS 协议》 应用部分：HTTP/2，可以参阅《HTTP/2 协议》  1 传输层优化 网络层性能关键点：延迟、带宽；\n1.1 三次握手 HTTP 需要 1.5*RTT，HTTPS 需要 2*RTT，建立一次连接的成本非常高，所以不管是 HTTP 还是 HTTPS，重用连接或者使用长连接是网站性能优化非常关键的一步骤；\n1.2 流量控制 接收方在每次发送 ACK 包的时候会告知发送方其接收窗口（rwnd）的大小，发送方看到接收方接收窗口比较大小，就会暂停或者发送少量的数据；Linux 系统默认接收窗口是 65535 字节（64KB），窗口缩放可以关闭，不过建议开启。\n1.3 拥塞控制  慢启动：慢启动的优点是在比较拥塞的网络，慢启动可以避免拥塞进一步加剧，但是它的缺点也是明显的，对于正常的网络，慢启动将降低传输的效率，例如本来一个 RTT 就可以传完的数据，现在要分成几个 RTT；比如 Linux 2 的 initcwnd 只有 3MSS，如果有 7MSS 数据要发送就不得不用 3RTT；如果把 initcwnd 改成 10，则 7MSS 并行发送，只需要 1RTT。 可以通过命令查看 ss -nli | fgrep cwnd； 修改某个网卡的 cwnd sudo ip route change default via 127.0.0.1 dev eth0 proto static initcwnd 10；  1.4 传输层队首阻塞 TCP 要保证数据包的正确传输，一个 HTTP/1.1、HTTP/2 或者 HTTPS 数据包发出后，会拆分为多个 TCP 包发送，对于接收方来说，收到所有 TCP 数据包后才能进行组装，然后才会发给应用层进行下一步处理。应用层无法解决传输层的问题，因此要完全解决队头阻塞问题，需要重新设计和实现传输层。目前而言，真正落地在应用的只看到 Google 的 QUIC。\n另外要特别注意 HTTP 队首阻塞： 对于同一个 TCP 连接，HTTP/1.1 通过 Pipelining 管道技术实现一次性发送多个请求，这样就解决了 HTTP/1.0 的客户端的队首阻塞。但是，HTTP/1.1 规定，服务器端的响应的发送要根据请求被接收的顺序排队，也就是说，先接收到的请求的响应也要先发送。这样造成的问题是，如果最先收到的请求的处理时间长的话，响应生成也慢，就会阻塞已经生成了的响应的发送。也会造成队首阻塞。可见，HTTP/1.1 的队首阻塞发生在服务器端。 如果队头阻塞的粒度是 http request 这个级别，那么 HTTP/2 over TCP 的确解决了 HTTP/1.1 中的问题。但是，HTTP/2 目前实现层面上都是基于 TCP（HTTP 从来没有说过必须通过 TCP 实现，你可以用它其他传输协议实现哟），因此 HTTP/2 并没有解决传输层 TCP 的队首阻塞问题，它仅仅是通过多路复用解决了以前 HTTP/1.1 管线化请求时的队首阻塞。\n目前的解决方案有：\n HTTP/2 over TCP(我们接触最多的 HTTP/2) 解决了 http request 级别的队头阻塞问题； HTTP/2 over QUIC 解决了传输层的队头阻塞问题（除去 header frame），是我们理解的真正解决了该问题；  2 应用层优化 http/1.1 于 1999发布，主要文档为 RFC 7230，提出了很多新的机制： （1）从面向文档的协议彻底转变为面向资源的协议（REST 理论） （2）支持 keepalive 并默认开启，需要注意的 TCP Keep-Alive 和 HTTP Keep-Alive 是不同层次上的概念，TCP 的 keep alive 是检查当前 TCP 连接是否活着；HTTP 的 Keep-alive 是要让一个 TCP 连接在 timeout 周期内永久存活。 （3）相比较于 1.0，必须得传 Host 头，支持同一IP的多虚拟机\nHTTP/1.1 的设计目标并没有重点关注性能，在高可用和高并发上的瓶颈越来越大，为了提升性能，基于 HTTP/1.1 出现了很多优化方案，比如长连接、HTTP pipelining(管道)、多 TCP、WebSocket 机制，这些优化方案能提升性能，但也带来很多负面的影响，比如 HTTP 管理技术基于 HTTP/1.1 并没有可行性（线头阻塞），而长连接技术给浏览器和服务器带来了很大的负载。 中间发展了 SPDY、RPC 等协议来解决性能问题，特别是 SPDY 是 HTTP/2 的灯塔。\n2.1 长连接 连接的创建成本非常高，所以使用长连接达到复用的目的。但是长连接可能会给服务器带来很大的负载，因为即使没有后续的请求，服务也必须保持一个连接，限制了服务器的并发处理能力，所以很多浏览器和 Web 服务器设置了长连接超时时间，比如 60 秒内没有任何新请求则关闭长连接，节省资源。\n需要注意的是并不是所有的应用场景都适合使用长连接：\n API 服务，API接口响应一般非常快速，也无须保存状态，所以处理完成后应该尽快关闭连接。客户端请求/响应模型一般很少使用长连接。 视频服务，视频服务传输的数据量非常大，一个连接只会发送一个请求，完成一个请求的时间比较长，没有必要采用长连接技术。  以下适合用长连接：\n Web 网站相对适合使用长连接技术，因为 Web 要加载很多子元素，一个连接可以并行发送多个子请求。 数据库持久连接，也就是连接池。  2.2 多个 TCP 连接 对于一个主机，浏览器一般会并行打开 6 个连接，当然打开多个 TCP 连接对服务器和浏览器是很大的消耗。如果还想避免单机 6 个连接的限制，出现了多主机优化手段，也就是静态元素拆分成多个主机，缺陷就更明显了，就是消耗过多的客户端和服务端资源。\n2.3 CDN 技术 通过 CDN 技术，网站可以在世界范围内部署 CDN 节点，某个用户访问该网站，可以选择最近的一个 CDN 节点，选择最短的路径。 其优势在于：\n 延迟减少，加快握手，HTTP/1.1 需要 1.5 次 RTT，HTTP/2 需要 2 次 RTT，效果比较明显。 CDN 厂商会用更好的技术方案来加速 TLS/SSL 连接。  3 HTTP/2 优化 4 SSL/TLS 优化 5 部署或升级 本文的 HTTPS 指的是构建在 HTTP/2 上，当然服务器是要向下兼容 HTTP/1.1。对于安全部分要定期检测，可以使用工具定期检测网站的 TLS 配置，例如 Qualys Lab 的 SSL Test.\n5.1 数字证书 证书的生成有多种方式，开发者熟悉以下两种生成方式就可以了。\n5.1.1 自签名证书 使用上面提到的 openssl 工具生成。 私钥：openssl genrsa -out privkey.pem 2048 证书：openssl req -new -x509 -sha256 -key privkey.pem -out cert.pem -days 365 -subj \u0026ldquo;/CN=blog.xxx.com\u0026rdquo;\n5.1.2 Let’s Encrypt 证书 Let’s Encrypt 官网推荐用 certbot 工具，macOS 上使用 brew install cerbot 安装即可。\n$ certbot certonly -d \u0026ldquo;*.xxx.com\u0026rdquo; -d \u0026ldquo;xxx.com\u0026rdquo; \u0026ndash;manual \u0026ndash;preferred-challenges dns-01 \u0026ndash;server https://acme-v02.api.letsencrypt.org/directory\n certonly 是插件； -d 支持的域名，*.xxx.com 是通配符证书，但不支持 xxx.com，所以后面还得再配置一个 xxx.com，多个 -d 参数需要配置多条 TXT 记录，注意 TXT 可以对同一主机配置多条值，等证书都生成了才可以删除 TXT 配置； \u0026ndash;preferred-challenges dns-01 域名的认证方式，这里的 dns-01 表示用设置域名 TXT 的方式来验证，也只有 dns-01 才支持通配符证书； acme 版本，只有 v02 才支持 通配符证书；  需要注意的可能要创建一些文件夹，如没权限时会提示手动创建，不用特别记。 \u0026ndash;logs-dir /var/log/letsencrypt \u0026ndash;config-dir /etc/letsencrypt \u0026ndash;work-dir /var/lib/letsencrypt sudo mkdir -p /usr/local/sbin sudo chown -R $(whoami) /usr/local/sbin\n域名 TXT 是否配置成功可以用 dig 进行验证： dig -t txt _acme-challenge.xxx.com，验证 txt 是否配置好。\n运行成功之后会有提示，最后一句表示更新证书的命令\n Congratulations! Your certificate and chain have been saved at: /etc/letsencrypt/live/xxx.com/fullchain.pem Your key file has been saved at: /etc/letsencrypt/live/xxx.com/privkey.pem Your cert will expire on 2019-06-19. To obtain a new or tweaked version of this certificate in the future, simply run certbot again. To non-interactively renew *all* of your certificates, run \u0026quot;certbot renew\u0026quot;    文件名 内容     cert.pem 服务端证书   chain.pem 浏览器需要的所有证书但不包括服务端证书，比如根证书和中间证书   fullchain.pem 包括了cert.pem和chain.pem的内容，服务端证书是第一张证书，接下来是中间证书   privkey.pem 证书的私钥    5.2 WEB 服务器 每种 Web 服务器配置 HTTPS 的方式都略有差异，这里以 nginx 为例。 通过 nginx -V 可以查看是否支持 ssl(\u0026ndash;with-http_ssl_module) 和 http2(\u0026ndash;with-http_v2_module) OpenSSL 1.0.2 版本及以上的都支持 ALPN。server 配置例如：\nserver { listen 80; listen [::]:80; server_name jemper.cn www.jemper.cn; return 301 https://www.jemper.cn$request_uri; } server { listen 443 ssl http2; server_name www.jemper.cn jemper.cn; ssl on; ssl_certificate /etc/nginx/jemper.cn/fullchain.pem; ssl_certificate_key /etc/nginx/jemper.cn/privkey.pem; ssl_trusted_certificate /etc/nginx/jemper.cn/chain.pem; ...... } 证书一般情况下 fullchain.pem 和 privkey.pem 就够用了，另外需要注意，如果服务器有修改证书，必须 reload nginx 才会重新加载密钥等。\n6 调试 6.1 nghttp2   nghttp2 工具集中的 nghttpd 服务器：nghttpd -v -d /usr/local/www 443 /Users/ada/.ssh/localhost/wpxkey.pem /Users/ada/.ssh/localhost/cert.pem\n  nghttp2 工具集中的 nghttp 命令行客户端， nghttp -ns https://www.jemper.cn nghttp -v \u0026ndash;no-dep -w 14 -H\u0026quot;header1: myhead\u0026rdquo; https://localhost 常用参数有\n   -v（打印 debug 信息），一般打印出来足以进行 HTTP/2的分析和学习 -n（丢弃下载的数据，如 HTML 内容） -a（下载在 HTML 中指明的、与 HTML 同一个域的引用资源，不过推送的资源不受此参数影响） -s（打印统计信息） -H \u0026lt;header\u0026gt;（给请求添加首部，如 -H\u0026rsquo;:method: PUT\u0026rsquo;） -w 设置窗口大小 -w 14 表示 2^14-1 \u0026ndash;no-dep Don't send dependency based priority hint to server  6.2 cURL命令 cURL命令 curl -w \u0026ldquo;@curl.txt\u0026rdquo; -so /dev/null https://www.jemper.cn curl -v \u0026ndash;http2 https://www.jemper.cn 常用参数： -w tells cURL to use our format file, 可以是文本 -w \u0026quot;TCP handshake: %{time_connect}, SSL handshake: %{time_appconnect}\\n\u0026quot;，也可以是文件 -w \u0026quot;@curl.txt\u0026quot;。 -o redirects the out of the request to /dev/null -s tells cURL not to show a progress meter，即不显示进度 -k, \u0026ndash;insecure turn off curl's verification of the certificate -H, \u0026ndash;header Pass custom header，可设置多次 -d, \u0026ndash;data HTTP POST data -v 显示更多关于你访问的 URL 的调度信息 \u0026ndash;http2 模拟 HTTP/2 的请求\n打印时间信息（单位秒）：\n time_namelookup: DNS解析时间,从请求开始到DNS解析完毕所用时间 time_connect: 连接时间,从请求开始到建立TCP连接完成所用时间,包括前边DNS解析时间，如果需要单纯的得到连接时间，用这个time_connect时间减去前边time_namelookup时间 time_appconnect: 从请求开始到连接建立完成时间，到SSL/SSH等建立连接时间。 time_pretransfer: 从请求开始到准备传输的时间。 time_redirect: 重定向时间 time_starttransfer: 从请求开始到 Web 服务器返回数据的第一个字节所用的时间 time_total: 总时间，按秒计  6.3 抓包解密 其实很多命令行工具如 openssl、curl 也都有请求日志，但不够详细。\n Chrome 会话密钥日志：Chrome 和 Firefox 都提供了记录 TLS 会话密钥的功能，这是 Mozilla 的规范。  SSLKEYLOGFILE = ~/tls/tls_master_secret.log open /Applications/Google\\ Chrome.app，这种打开方式保证 Chrome 能读取到 SSLKEYLOGFILE 环境变量。 接下来可以使用像 Wireshark 这种工具检查 HTTP/2 流量和观察 HTTP/2 帧。前提是需要在 Wireshark 做一些配置。 Wireshark/Preferences/Protocals/TLS 中设置 (Pre)-Master-Secret log filename 指向（1）设置的日志文件，最好将 TLS debug file 也配上，这样解密过程中的日志都会记录下来，便于调试分析。 要注意两个日志文件会起来越大，定期删除日志文件，否则 Wireshark 会运行得非常缓慢。 本地抓包工具推荐 Wireshark + npcap，过滤 ip.addr = www.xxx.cn \u0026amp;\u0026amp; (ssl || tcp)。 另外如果用的是 RSA 密钥协商，还可以用 Wireshark 的 RSA key list，不过 TLSv1.2+ 已经不支持 RSA 密钥协商了。\nwireshark 中的 TCP 有专家信息 Expert info，比如 TCP window update、TCP Zero Window Probe、TCP ACKed unseen segment 等，这些并不是 TCP 数据，而是 wireshark 根据一些规则得出的指导信息，每个信息的规则可以看 wireshark 文档 TCP Analysis\n chrome://net-export/ + https://netlog-viewer.appspot.com 可以进行 HTTP/2 帧的分析等。chrome 已经不再集成 chrome://net-internals/#http2 了，改用上面的组合，前者负责 save netlogs，后者负责 catapult netlog_viewer to view them。netlog-viewer 也可以本地化，但觉得没必要。\n  Fiddler、Charles 代理，也就是中间人方式，需要安装代理的证书，特别适合移动端抓包分析。\n  6.4 tcpdump 抓包 Linux 和 MacOS 都可以用该工具抓包，\nsudo tcpdump port 8080 -n sudo tcpdump -i eth0 src host 10.2.200.11 or dst host 10.2.200.11 sudo tcpdump -i eth0 -s 80 -w /tmp/tcpdump.cap  -i：网卡 -n：端口号用数字表示，很多命令都通用 -v -vv -vvv：显示详细的信息 -s 抓取的字节数，比如想要分析除应用层协议，可以设置为80，一般所有协议的首部大小加起来就 60B 左右 -w 保存 可以进行 or/and 等逻辑运行  tcpdump 显式格式和 Wireshark 有一些不一样，比如：\n win：tcpdump 是 TCP 16bit Window 首部值，而 Wireshark 是运算出来的实际窗口大小。 length：tcpdump 是 tcp 报文段数据长度， Wireshark 是帧长度 tcpdump 会标识出传输的段 seq 9577:10945，ack 一样是表示期待收到的一个数据序号 Flags [.] The \u0026lsquo;.\u0026rsquo; means the ACK flag was set  以下是抓包的样段，我去掉选项、日期等数据，window scale = 7，即乘积为128，服务器8090接收数据，客户端60950发送数据：\n... 1 16.223370 IP 8090 \u0026gt; 60950: Flags [.], ack 393065, win 3, length 0 2 21.726274 IP 60950 \u0026gt; 8090: Flags [.], seq 393065:393449, ack 1, win 2052, length 384 3 21.751857 IP 8090 \u0026gt; 60950: Flags [.], ack 393449, win 0, length 0 4 27.260046 IP 60950 \u0026gt; 8090: Flags [.], seq 393449:393450, ack 1, win 2052, length 1 5 27.281964 IP 8090 \u0026gt; 60950: Flags [.], ack 393449, win 0, length 0 6 29.130087 IP 8090 \u0026gt; 60950: Flags [.], ack 393449, win 117, length 0 7 29.130141 IP 60950 \u0026gt; 8090: Flags [.], seq 393449:394817, ack 1, win 2052, length 1368 8 29.130147 IP 60950 \u0026gt; 8090: Flags [.], seq 394817:396185, ack 1, win 2052, length 1368 9 29.130152 IP 60950 \u0026gt; 8090: Flags [.], seq 396185:397553, ack 1, win 2052, length 1368 10 29.130158 IP 60950 \u0026gt; 8090: Flags [.], seq 397553:398921, ack 1, win 2052, length 1368 11 29.130164 IP 60950 \u0026gt; 8090: Flags [FP.], seq 398921:400001, ack 1, win 2052, length 1080 12 29.153284 IP 8090 \u0026gt; 60950: Flags [.], ack 394817, win 107, length 0 13 29.205997 IP 8090 \u0026gt; 60950: Flags [.], ack 400002, win 67, length 0 ... 399 packets captured 1344 packets received by filter  第1行 确认，并更新 win = 3，实际窗口大小 = 3*128 = 384 第2行 就马上发送了 384 个数据长度的 TCP 报文段，当然这个报文段的长度是 384+TCP首部长度 = 450（这里没有标识，我是同时通过 Wireshark 抓包看的），可见 TCP 首部是 66 字节。在 Wireshark 会有专家信息“TCP Window Full” 第3行 确认，在 Wireshark 会有专家信息“TCP ZeroWindow” 第4行 TCP Zero Window Probe 第5行 ACK to a TCP Zero Window Probe，并且还是 TCP ZeroWindow 第6行 TCP window update 第7、8、9、10行 发送数据 第11行 也是发送数据，不过同时还有 FIN 和 PSH 标识 第12行 接收数据，下一序号是 394817，相比上一次确认 393449，相当于接收了 1368 个字节 第13行 也是接收数据，下一序号是 400002，相比上一次确认 394817，相当于接收了 5185 个字节，这就是 TCP 累积起来确认了  最后可以看到期望下一序号是 400002，所以一共发送了 400001 个字节数据，但因为握手阶段被算了 1 字节，所以实际发送量是 400000 个字节。 下面提供两个软件抓包数据对比(两者数据是一致的)：tcpdump TCP 抓包、Wireshark TCP 抓包\n6.5 Wireshark 抓包 6.5.1 过滤  ip 和 port：ip.addr eq \u0026amp;\u0026amp; tcp.port eq 直接用协议名：不过用此方式过滤要知道协议之间的依赖关系，否则经常会把某次访问的多种协议过滤掉。  6.5.2 自动分析 集中在 Analyze 和 Statistics 两个菜单。\n Analyze/Expert Infomation Statistics/Service Response Time Statistics/TCP Stream Graph 分析一个方向的传输情况，点击时间轴可以切换相对时间和绝对时间 Statistics/Summary Statistics/Conversations 显示协议流汇总信息  接下来做一分析，如下是 B 向 A 发送 400000 个字节的情况，这里是 抓包文件。\n1. Statistics/Conversations如下：\nAddress A | Address B | Packets | Bytes | Packets A → B | Bytes A → B | Packets B → A\n | - | - | - | - | - | - 120.77.37.121 | 192.168 .3.3 | 407 | 431090 | 97 | 6506 | 310  Bytes B → A | Rel Start | Duration | Bits/s A → B | Bits/s B → A\n | - | - | - | - 424584 | 24 | 69 | 750 | 48984  分析以上的数据可以得出：\n B 发了 310 个包，明显大于 97 个确认包，很多是合并确认的 B 消耗的首部字节 424584 - 400000 = 24584B，平均每个包首部为 24584 / 310 = 79B/frame，这么高的原因是因为有重传，我看了一下有 3 个 1368 包重传，那么 （24584 - 1368*3） / 310 = 66B/frame，这样就合理了。 A 消耗的首部字节 6506 / 97 = 67B/frame，查包没有重传的。 B 的传输速率 48984b/s = 6123B/s = 5KB/s，为什么这么慢，我们接着往下看。  2. Statistics/TCP Stream Graph(Stevens)：\n横坐标是相对时间（相对开启抓包的时间），27s-52s 是 0 窗口时期，共有三个长的 0 窗口时期（27s 前也有几个横线，那还算正常暂时不讨论），所以要解决网络就是要增加接收方的窗口。尽量不要出现 0 窗口时间。可是通过查包 A 的窗口是一直在增大的，最大到 120KB，仅发 400KB 的数据，窗口 120KB 完全足够了，基本可以达到秒传才对。\n其实真正的原因在 B 从缓存接收数据太慢了导致窗口快速缩小为 0，查看我的代码发现，数据是每秒 100KB 在发（发4次），接收是每秒 1KB 多在接收。所以 B 数据全部发送到 A 的缓冲区要 69 秒（当然 A 应用程序接收完就需要 400KB/1KB/s = 400s，但这跟 B 无关），可以计算出 A 缓冲数据峰值 400KB - 69KB = 331KB，如果强行要改正这种问题，只要把 A 的窗口增大到 400KB 就能秒传了（当然 A 应用程序读完其缓冲区还是需要 400s）。\n参考文献 [1] 虞卫东. 深入浅出 HTTPS 从原理一实战. 版次：2018年6月第1版 [2] 阮一峰. HTTPS 升级指南 http://www.ruanyifeng.com/blog/2016/08/migrate-from-http-to-https.html. 2016年8月19日 [3] 三种解密 HTTPS 流量的方法介绍 https://imququ.com/post/how-to-decrypt-https.html. 2016/03/28 [4] HTTP/2的历史、特性、调试、性能 https://www.jianshu.com/p/748c7ca7c50f. 2017.07.25 [5] HTTP队头阻塞. https://liudanking.com/arch/what-is-head-of-line-blocking-http2-quic/ [6] TCP 性能优化详解. https://www.zhuxiaodong.net/2018/tcp-performance-optimize-instruction/\n","id":19,"section":"posts","summary":"HTTPS 整个知识体系非常庞大，我认识到的学习 HTTPS 的最佳步骤是： 基础部分：TCP/IP，可以参阅《TCP 传输原理》 安全部分：分别是学习密码学、Open","tags":["HTTP","TLS","调试"],"title":"HTTPS 性能和调试","uri":"https://blog.jemper.cn/2019/04/https/","year":"2019"},{"content":"HTTP/1.1 与 HTTP/2 相隔了大概20年，但是从目前的研究和实验情况来看，我们不可能等上几十年才升级到下一个版本，h2 在逐渐的普及。 h2 和 h1 的最大差别在于在 http 层上增加了分帧层；把原来的 http 层的数据拆分成多种帧类型，并在每种类型前加上协议性的帧首部。基于二进制分帧的优点：\n 传输使用的编码方式改变（采用帧、流模式），不用创建多个 TCP 连接，单个连接就可以有多个流（处理主页面和所有子元素的请求），能够减少延迟，服务器和浏览器的 socket 负载也大大减少。 并行处理，乱序发送帧，不再采用 pipeline 阻塞方式 传输的功能得以扩展（如服务端推送、首部压缩、优先级、流量控制）。  学习 HTTP/2 最好的文档是 RFC 7540。\n1 连接 HTTP/2 对每一个域名只会开启一个连接（本质上就是 一个 TCP/Socket），HTTP/2 的设计思路是尽量在单个 TCP/IP socket 上通信。\n1.1 启用 http/2 对于浏览器来说，它无法知晓该服务端是否支持了 h2，有三种方式来了解是否支持。\n 为\u0026quot;http\u0026rdquo; URIs启用HTTP/2协议：客户端利用 Upgrade 首部来表明期望使用 h2；服务端返回 101 Swiching Protocols; 为\u0026quot;https\u0026rdquo; URIs启用HTTP/2协议：客户端在 ClientHello 中设置应用层协议协商（Application-Layer Protocol Negotiation，ALPN）扩展来表明期望使用 h2；服务端在 ServerHello 中同样返回，所以 h2 在创建 TLS 握手的过程中完成协商，不需要多余的网络通信。 先知情况下启用HTTP/2：使用 HTTP Alternative Services 或 Alt-Svc。  1.2 http/2 连接前奏 支持了之后还要再确认使用 h2，两端都需要发送前奏，作为对所使用协议的最终确认，并确定HTTP/2连接的初始设置。客户端和服务端各自发送不同的连接前奏。\n 客户端连接前奏以一个24字节的序列开始，用十六进制表示为： 0x505249202a20485454502f322e300d0a0d0a534d0d0a0d0a 即，连接前奏以字符串\u0026quot;PRI * HTTP/2.0\\r\\n\\r\\nSM\\r\\n\\r\\n\u0026quot;开始，一般称 Magic，注意它不是帧。这个序列后面必须跟一个可以为空的 SETTINGS 帧 服务端连接前奏包含一个可能为空的 SETTINGS 帧( 6.5节 )，它必须由服务端在HTTP/2连接中首先发送。 在发送完本端的连接前奏之后，必须对收到的作为对端连接前奏一部分的 SETTINGS 帧进行确认（ACK）。  1.3 SETTINGS 帧 SETTINGS 帧只能应用到整个连接，不能应用于单个流，即 SETTINGS 帧的 stream identifier 必须为 0x0；\nSETTINGS 帧包含了若干有序的键值对，每个键值对的格式如下：\n +-------------------------------+ | Identifier (16) | +-------------------------------+-------------------------------+ | Value (32) | +---------------------------------------------------------------+ SETTINGS 帧的标识符跟其它的不一样，只有 ACK 一个标识，取值 0x0 和 0x1，前者表示设置请求，后者表示确认响应；设置请求发送的 SETTINGS 帧包含了若干有序的键值对，确认响应没有负载数据。\nIdentifier 参数列表：\n   名称 默认值(octets) 描述      SETTINGS_HEADER_TABLE_SIZE (0x1) 4096 重新指定 HPACK 所用的首部表的最大尺寸    SETTINGS_ENABLE_PUSH (0x2) 1 如果设置为 0，当前端不会发送 PUSH_PROMISE    SETTINGS_MAX_CONCURRENT_STREAMS (0x3) 无限制 表明发送端能够并行接收的流的最大数量    SETTINGS_INITIAL_WINDOW_SIZE (0x4) 65535(64KB) 表明发送端流量控制的初始窗口尺寸    SETTINGS_MAX_FRAME_SIZE (0x5) 16384(16MB) 发送端希望接收的最大帧尺寸；这个值必须介于初始值和 2^24-1（16MB） 之间    SETTINGS_MAX_HEADER_LIST_SIZE (0x6) 无限制 该设置告诉通信的另一端，本端期望接收的最大首部的尺寸     1.4 PING 帧 PING 帧用以计算两端之间的往返时间 RTT，只有一个标识位 ACK，这个标识位和 SETTINGS 帧一样意识。\n1.5 关闭连接 GOAWAY 帧用于礼貌地关闭连接。这个是连接层的帧，也就是发送时流 ID 要设置为 0x0，\n2 帧 HTTP/2 是基于帧（frame）的协议，帧是 HTTP/2 最小传输单位；采用分帧是为了将重要的信息都封装起来，让协议的解析方轻松解析。基于帧的协议，所有的帧都固定用 9 个字节的帧首部加上帧负载数据组成。\n +-----------------------------------------------+ | Length (24) | +---------------+---------------+---------------+ | Type (8) | Flags (8) | +-+-------------+---------------+-------------------------------+ |R| Stream Identifier (31) | +=+=============================================================+ | Frame Payload (0...) ... +---------------------------------------------------------------+  帧首部字段解析：     名称 长度 描述     Length 3字节 帧负载长度，2^14（16KB）是默认的最大帧长度，如果需要更大的帧，必须在 SETTINGS 帧设置   Type 1字节 当前帧类型   Flags 1字节 具体帧类型的标识，影响负载的协议结构   R 1位 保留位   Stream Identifier 31位 每个流的唯一 ID   Frame Payload 长度可变 真实的帧内容，长度为 Length    帧类型：     名称 ID 描述     DATA 0x0 传输流的核心内容   HEADERS 0x1 包含 HTTP 首部，和可选的优先级参数   PRIORITY 0x2 指示或者更改流的优先级和依赖   RST_STREAM 0x3 允许一端停止流   SETTINGS 0x4 协商连接级参数   PUSH_PROMISE 0x5 提示客户端，服务端要推送些东西   PING 0x6 测试连接可用性和往返时延（RTT）   GOAWAY 0x7 告诉另一端，当前端已结束   WINDOW_UPDATE 0x8 协商一端将要接收多少字节（用于流量控制）   CONTINUATION 0x9 用以扩展 HEADERS 数据块    3 流 流（stream）的定义是：HTTP/2连接上独立、双向的帧序列交接。流代 ID 表示一次 HTTP 请求和响应所产生的一系列帧，流 ID 用来标识帧所属的流；单个 socket 上可以创建多个流，HTTP/2 的设计思路是尽量在单个 TCP/IP socket 上通信。 流 ID 在设计时就避免了客户端和服务端之间的流 ID 冲突，也可以轻地判断流的源头。\n 客户端会从 1 开始设置流 ID，之后每新开启一个流，就会增加 2，并一直使用奇数。 服务端开启在 PUSH_PROMISE 中标明的流时，设置的流 ID 从 2 开始，之后一直使用偶数。 0 是保留数字，用于连接级控制消息，不能用于创建新的流。  帧在流上发送的顺序非常重要，最后接收方会把相同 Stream Identifier (同一个流) 的帧重新组装成完整消息报文\n3.1 消息 帧消息的关注点在于 HEADERS 和 DATA 这两个帧类型，这也是由 h1 的头部和数据拆分而来。 帧首部必须以 +END_STREAM、+END_HEADERS 结束。\nh1 把 header 拆分成两部分：请求/状态行（GET / HTTP/1.1、HTTP/1.1 200 OK）、首部（Host、User-agent等）；而 h2 取消了这种拆分，一切都是 header，并引入了伪首部（Pseudo-Header）： （1）请求伪首部：:method、:scheme、:authority、:path （2）响应伪首部：:status\n HEADERS 帧   +---------------+ |Pad Length? (8)| +-+-------------+-----------------------------------------------+ |E| Stream Dependency? (31) | +-+-------------+-----------------------------------------------+ | Weight? (8) | +-+-------------+-----------------------------------------------+ | Header Block Fragment (*) ... +---------------------------------------------------------------+ | Padding (*) ... +---------------------------------------------------------------+ 帧字段的部分解析可以看 PRIORITY 帧。\n标识位（Flags）：\n   名称 位 描述     END_STREAM 0x1 表明这是流中最后的帧（流终止）   END_HEADERS 0x4 表明这是流中最后一个 HEADERS 帧；如果此标识未设置，表示随后会有 CONTINUATION 帧   PADDED 0x8 表明此帧添加了填充数据，要使用 Pad Length 和 Padding 字段   END_STREAM 0x20 设置了此标识，表明要使用 E、Stream Dependency 以及 Weight 字段    DATA 帧   +---------------+ |Pad Length? (8)| +---------------+-----------------------------------------------+ | Data (*) ... +---------------------------------------------------------------+ | Padding (*) ... +---------------------------------------------------------------+ 标识位（Flags）：\n   名称 位 描述     END_STREAM 0x1 表明这是流中最后的帧（流终止）   PADDED 0x8 表明此帧添加了填充数据，要使用 Pad Length 和 Padding 字段    3.3 流量控制 WINDOW_UPDATE 帧流量控制可以应用到单个流，也可以应用到连接承载的所有流（流 ID 为 0x0），这点与 SETTINGS 不一样。需要注意的是，在单个流上指定的 WINDOW_UPDATE 帧也会作用于连接层的流量控制。 在流建立的时候，窗口大小默认都是 2^16-1(64KB)；流量控制不能关闭，把窗口最大值设定为 2^31-1（2GB） 就等效于禁用它；\n +-+-------------------------------------------------------------+ |R| Window Size Increment (31) | +-+-------------------------------------------------------------+ 该帧没有标识符（Flags）。\n3.4 优先级 客户端拿到页面分析依赖关系的时候是通过声明依赖关系树和树里的相对权重实现的。\nindex.html - style.css - critical.js - less_critical.js(weight 20) - photo.jpg(weight 8) - header.jpg(weight 8) - ad.js(weight 4) 依赖树是客户端自己维护的，而权重则需要告诉服务端实现对象优先传输顺序，不过说到底，做什么以及如何处理优先级，最终还是得听服务器的，服务器仍有做它自己认为正确的事的权力。 那么客户端怎么告诉服务端的？通过 HEADERS 帧和 PRIORITY 帧，客户端可以明确的和服务沟通它需要什么，以及它需要这些资源的顺序。 PRIORITY 帧可以看成是 HEADERS 的子部分：\n +-+-------------------------------------------------------------+ |E| Stream Dependency (31) | +-+-------------+-----------------------------------------------+ | Weight (8) | +-+-------------+ 帧字段：\n E 标识当前的流是否为专用，是否不依赖其他流 Stream Dependency 流依赖，如果当前流依赖其他流，标识其所依赖的流 Weight 当前流的相对权重  该帧没有标识符（Flags）。\n3.5 CONTINUATON 帧 有些帧的帧负载很简单，比如 DATA，只有 Pad Length 后面就是 DATA；有些帧负载配置较多，即当 HEADERS、PUSH_PROMISE 帧首部块片段（header Block Fragment）较大（少见），需要分帧传输，有两种选择： （1）再次使用 HEADERS、PUSH_PROMISE 等帧，缺点是帧负载的配置重复传递，还得处理帧负载的配置有分歧的情况，可能引起麻烦。 （2）采用新的 CONTINUATON 帧，没有帧负载的配置，仅仅有 header Block Fragment，缺点是 CONTINUATON 和前面的帧必须是有序的，会减损多路复用的益处。\n最终协议开发者选择了处理较为简洁的新帧。\n +---------------------------------------------------------------+ | Header Block Fragment (*) ... +---------------------------------------------------------------+ 标识位（Flags）：\n   名称 位 描述     END_HEADERS 0x4 表明这是流中最后一个 HEADERS 帧；如果此标识未设置，表示随后会有 CONTINUATION 帧    3.6 关闭流 如果要终止一个流，可以将 RST_STREAM 加在该流的两端。帧字段只有 32 位的 Error Code。\n4 推送响应 PUSH_PROMISE 帧可以看成是服务端响应的首部，字段如下：\n +---------------+ |Pad Length? (8)| +-+-------------+-----------------------------------------------+ |R| Promised Stream ID (31) | +-+-----------------------------+-------------------------------+ | Header Block Fragment (*) ... +---------------------------------------------------------------+ | Padding (*) ... +---------------------------------------------------------------+ 有几个属性需要理解：\n Promised Stream ID 推送响应一定会对应到客户端已发送的某个请求； PUSH_PROMISE 和 HEADERS 帧字段是很相似的，比如 Promised Stream ID 和 Stream Dependency 是一样的意思，只是所处的角度不同所以描述不同； :method 首部的值必须确保安全。安全的方法就是”幂等“的那些方法； 会创建新的偶数的流 ID； 被发送的对象必须确保是可缓存的； 应该早于客户端接收到可能承载着推送对象的 DATA 帧推送，比如客户端请求 HTML，那么服务器应早于完整推送 HTML 前推送 PUSH_PROMISE。不过 h2 足够健壮，可以优雅地解决这类问题，但确实是浪费流量了。  客户端可以拒收，使用 RST_STREAM 帧。另外还有 PROTOCAL_ERROR 当 PUSH_PROMISE 涉及的协议不安全，或者客户端已经在 SETTINGS 帧中表明自己不接受推送时，仍然进行推送。不过在双方都了解对方想法前可能无法避免地推送大量资源。\n如果服务器接收到一个页面的请求，它需要决定是推送页面上的资源还是等客户端来请求。决定的过程需要考虑到如下方面：\n 资源已经在浏览器缓存中的概率； 从客户端来看这些资源的优先级； 可用的带宽，以及其他类似的会影响客户端接收推送的资源；  如果服务器选择正解，那就真的有助于提升页面的整体性能，反之则会损耗页面性能，这也是如今通用服务端推送解决方案非常少的原因。估计在 APP 上应用场景较多。\n5 首部压缩 HPACK 的知识点相对比较多，最好的方式是通读文档 RFC 7541，以下只是做一些归纳。 HPACK 没有定义可扩展性机制，比如 Integer Representation、静态表 等都是无法扩展的；只能通过定义完全的替代品来更改格式。估计这个是 HTTP/2 直接升级到 HTTP/3 的原因吧。\n5.1 学习的基础 5.1.1 术语 这 4 个术语是一定要理解的，才能读懂文档：\n Header Field：一个键值对；未编码； Header List：多个 Header Field 有序连接起来，未编码； Header Field Representation：单个 Header Field 的表示方法（即用 HPACK 压缩算法编码），解码后就是 Header Field； Header Block：多个 header field representations 有序连接起来，解码后就是 Header List；  也就是说 HTTP/2 的首部有序列表编码后就是 Header Block，解码后就是 Header List。且 Header List 中的顺序要和 Header Block 中的顺序一致。 比如：(数字是16进制) :method: GET HPACK 编码后是 82； :authority: localhost HPACK 编码后是 41 86 a0 e4 1d 13 9d 09； 单个表示方法：(括号表示这个键后面的值的字节数) :method(3)GET 就是 Header Field， 82 就是 Header Field Representation； 多个连接起来： :method(3)GET:authority(9)localhost 就是 Header List，82 41 86 a0 e4 1d 13 9d 09 就是 Header Block；\n除了术语，我们还要学习一下原始数据类型的表示和静态Huffman码。\n5.1.1 原始数据类型的表示 这部分很重要，一定要理解透，Primitive Type Representations 原始数据类型的表示：\n Integer Representation：表示对索引值（静态表或动态表索引条目）引用的表示法。 这部分可以看伪代码，N 可以灵活设置，后面的 Header Field Representation 使用了 N 等于 (1)7、(01)6、(001)5、(0000)4、(0001)4 这 5 种情况（前面括号是 octet(8位) 的二进制前缀）。要理解这段代码一定要看原文档，我就不引用了。  if I \u0026lt; 2^N - 1, encode I on N bits else encode (2^N - 1) on N bits I = I - (2^N - 1) while I \u0026gt;= 128 encode (I % 128 + 128) on 8 bits I = I / 128 encode I on 8 bits String Literal Representation：表示字符串字面量表示法，可以是直接地编码（ASCII），或使用静态Huffman码   0 1 2 3 4 5 6 7 +---+---+---+---+---+---+---+---+ | H | String Length (7+) | +---+---------------------------+ | String Data (Length octets) | +-------------------------------+ 其中 H 为 1 表示采用静态Huffman码，为 0 表示采用直接地编码。\n5.1.2 静态Huffman码 静态Huffman码本质上是一个查表的编码（静态Huffman码表）(其实 ASCII 也是一个查表编码)。原理就是一个对字符使用概率的重新编码，常规一个字符是用8位表示，但 Huffman 常用的字符采用5位、6位等表示。大概估算一般能够缩短1/3的长度（比如 9 个字符常规是 9*8 = 72 位表示，采用 Huffman 后最少可以只用 9*5 = 45 位表示，尾部填充后最少是 6 个字符的长度，少了 3 个字符）。\n5.2 索引表  \u0026lt;---------- Index Address Space ----------\u0026gt; \u0026lt;-- Static Table --\u0026gt; \u0026lt;-- Dynamic Table --\u0026gt; +---+-----------+---+ +---+-----------+---+ | 1 | ... | s | |s+1| ... |s+k| +---+-----------+---+ +---+-----------+---+ ⍋ | | ⍒ Insertion Point Dropping Point 请求端和响应端各维护了两张索引表：静态表和动态表。其中静态表的条数是固定的 s = 61，在所有的编码或解码上下文间共享的一套数据；动态表编号从 62 开始，每个动态表只针对一个连接，每个连接的压缩解压缩的上下文有且仅有一个动态表。\n5.3 编码解码 要理解这部分一定要理解透 5.1.1 节和 5.1.2 节。\n首部压缩最重要的就是实现 Header Field Representation（编码的头部字段）。一个编码的头部字段可由一个索引或一个字面量表示。解码就是反向过程。\n5.3.1 Indexed Representation 索引表示 索引的表示法将头部字段定义为对静态表或动态表中条目的引用， 这里就是使用了 N = (1)7 实现的。\n5.3.2 Literal Representation 字面量表示 字面量的表示通过描述头部字段的名称和值来定义头部字段。头部字段的名称可被字面地表示，或表示为对静态表或动态表中条目的引用。头部字段的值由字面量表示。\n定义了三种不同的字面的表示，每一种都分 Indexed Name 和 New Name 两种情况，表示头部字段的名称是否为对静态表或动态表中条目的引用，所以合起来算是六种：\n 将头部字段作为新条目添加到动态表的起始位置的字面地表示，Literal Header Field with Incremental Indexing；这里就是使用了 N = (01)6 实现的。 不向动态表添加头部字段的字面量表示，Literal Header Field without Indexing；这里就是使用了 N = (0000)4 实现的。 不向动态表添加头部字段的字面量表示，且这个头部字段总是使用字面的表示，Literal Header Field Never Indexed；这里就是使用了 N = (0001)4 实现的。  可按照 安全注意事项 的指导来选择这三种字面的表示中的一个，以保护敏感的头部字段值。\n头部字段的名字或值的字面表示的字节序列可以是直接地编码，或使用静态Huffman码。静态Huffman码可以看 5.1.2 节.\n上面三种不同的字面的表示中，只有第一种需要管理动态表（条目插入、条目被逐出）。\n5.3.3 Dynamic Table Size Update 动态表大小更新 这里就是使用了 N = (001)5 实现的。\n5.4 简单例子  :method: GET =\u0026gt; 82, 采用 N = (1)7 索引表示表，Representation: Indexed Header Field；采用了查静态表的方式实现，即 10000010(0x82)。 :authority: www.jemper.cn =\u0026gt; 41 8a f1 e3 c2 fd 0b 4d 65 b1 72 55, 采用 N = (01)6，Representation: Literal Header Field with Incremental Indexing - Indexed Name，看一下 RFC 文档即可找到该表示法：   0 1 2 3 4 5 6 7 +---+---+---+---+---+---+---+---+ | 0 | 1 | Index (6+) | +---+---+-----------------------+ | H | Value Length (7+) | +---+---------------------------+ | Value String (Length octets) | +-------------------------------+ （1）第一个 octets： :authority 这部分通过查静态表 Index = 1，首两位是01，所以是 01000001(41)； （2）第二个 octets： 第一个 bit 位表示是否是 Huffman 编码，这里当然要用 Huffman 编码，所以是 1，后面一共用 10 octets 长度表示值，所以是 10001010(8a)； （3）第三个 octets 到结束：www.jemper.cn 这个值的每一个字节通过查 Huffman 表得知分别为：1111000 1111000 1111000 010111 1110100 00101 101001 101011 00101 101100 010111 00100 101010；一共是 79 bit，后面用 1 填充到 octets 的倍数，即用 10 octets 能完全表示，连接起来就得到要传输的值：11110001 11100011 11000010 11111101 00001011 01001101 01100101 10110001 01110010 01010101(f1 e3 c2 fd 0b 4d 65 b1 72 55)。\n粗糙的结论：:method: GET 11 个字节只用了 1 个字节传输， :authority: www.jemper.cn 24 个字节只用了 12 个字节；即 35 个字节只用了 13 个字节传输，压缩率达 62.8%；其中静态表贡献了 94.7%，Huffman 贡献了 23%，可见一斑。  有趣轶事：\n 魔法字节流的 PRI 其实就是美国国家安全局 PRISM（棱镜）监控计划的一个笑话。 由早期的 HTTP/2.0 改成 HTTP/2 表示不能保证语义向后兼容，也就是不会有 2.1、2.2 之类的版本。  参考文献 [1] HTTP/2 RFC7540. https://httpwg.org/specs/rfc7540 [2] HPACK RFC7541. https://httpwg.org/specs/rfc7541 [3] HPACK RFC7541 中文文档. https://www.wolfcstech.com/2016/10/29/hpack-spec/ [4] Stephen Ludin, Javier Garza. HTTP/2 基础教程. 版次：2018年1月第1版 [5] 谈谈 HTTP/2 的协议协商机制. https://imququ.com/post/protocol-negotiation-in-http2.html. Apr 14, 2016 [6] HTTP2 详解. https://blog.wangriyu.wang/2018/05-HTTP2.html 2018.08.31 [7] 掌握 HTTP2.0 http://jartto.wang/2018/03/30/grasp-http2-0/\n","id":20,"section":"posts","summary":"HTTP/1.1 与 HTTP/2 相隔了大概20年，但是从目前的研究和实验情况来看，我们不可能等上几十年才升级到下一个版本，h2 在逐渐的普及。 h2 和 h1 的最大差别在于在 http 层","tags":["HTTP","协议"],"title":"HTTP/2 协议","uri":"https://blog.jemper.cn/2019/03/http2/","year":"2019"},{"content":"DevOps 是一套实践方法，在保证高质量的前提下缩短系统变更从提交到部署到生产环境的时间，它设想在开发组和运维组之间没有冲突。\n1 什么是 DevOps DevOps 是敏捷开发的延续，它将敏捷的精神延伸至运维（Operation）阶段。敏捷开发主要关注软件开发，主要目的是响应变化，快速交付价值，强调迭代增量的开发模式，没有自动化测试和部署，更没有监控等相关的流程。也就是敏捷 Scrum 可用在 DevOps 的开发过程中，但 DevOps 不仅仅是敏捷 Scrum。 一个明显的区别就是敏捷开发可能1-6周一次迭代，而 DevOps 一天几十次迭代都可以，这就是联合系统运维高效之处。 除外之外，DevOps 还关注团队协作与沟通，协作需要工具，沟通效果最好效率也最高的方式是面对面交谈。\n1.1 Scrum 敏捷框架 DevOps 中可以使用敏捷框架开发，常见敏捷框架有 Scrum、Kanban。大部分 IT 企业都使用 Scrum 框架，Scrum 采纳一种迭代和增量式的方法来优化对未来的预测和控制风险。作者著有《Scrum 指南》，有中文版可下载，下面简单介绍： {% img http://img.jemper.cn/2019/04/scrum_framework.png 700 %} Scrum 框架由 Scrum 团队以及与之相关的角色、事件、工件和规则组成，Scrum 的规则把角色、事件和工件组织在一起，管理它们之间的关系和交互。\n 团队以及与之相关的角色：Scrum 团队有三个角色：（1）一名 Scrum Master，负责确保成员都能理解并遵循过程，服务于产品负责人，服务于开发团队，服务于组织，类似项目经理；（2）一名产品负责人，负责最大化 Scrum 团队的工作价值；（3）开发团队，负责在每个 Sprint 结束时交付潜在可发布并且“完成”的产品增量。 事件：Scrum 的核心是 Sprint（短周期），其规定了 4 个正式事件，所有事件都是有时间盒限定的，也就是说每一个事件限制在最长的时间范围内，一旦 Sprint 开始，它的持续时间是固定的，不能缩短或延长，每个 Sprint 都可以被视为一个项目： Sprint 计划会议 每日 Scrum 站会 Sprint 评审会议 Sprint 回顾会议 工件：Scrum 采用了三个主要的工件：Product Backlog 是开发产品的所有需求的优先排列表，Sprint Backlog 包含在一个 Sprint 内的产品 Backlog，燃尽图用来衡量剩余的 Backlog 的工作量。  Scrum 的使用者必须经常检视 Scrum 的工件和完成 Sprint 目标的进展。\n1.2 DevOps 应用与研究现状 目前针对 DevOps 的研究主要集中在微服务、持续集成与持续部署、自动化工具的研发等内几个方面。\n  微服务架构风格是将单体系统程序划分为众多小而自治的服务进行开发的方式，其中每个服务都拥有自己的进程并利用轻量化机制（REST、RPC）进行通信，服务以业务为边界进行分解，可凭借自动化部署机制实现独立部署，不同服务可基于不同技术使用不同的编程语言实现并使用不同的数据存储技术（即去语言和数据中心化）。\n  持续集成、持续交付和持续部署\n  持续集成是代码构建完成\n  持续交付一般还不是最终的生产环境，可能是预生产环境，而且一般是手动部署\n  持续部署是持续交付的更高一个阶段，且通过了自动化测试，并自动化部署到生产环境\n  工具研究和开发 基础经济决定上层建筑，所以团队采用 DevOps 需要因地制宜。为了让DevOps落地，前期我们还是要从 DevOps 工具链开始实践，搭建一只小的 DevOps 团队来承载一项新的业务，等小团队适用了 DevOps 的管理节奏再推广到大团队。以下是随手记的工具链： 生产力工具：JIRA、Confluence、Swagger 部署工具：gitlab、jenkins、docker、k8s、ELK+自动报警机制 我们是这样进行工作的：产品经理在 JIRA 上撰写文档创建任务，研发 Leader 将任务分配给开发，Swagger 上撰写接口文档协作，开发完成后提交代码，并创建 MR，我们在本地部署了 Jenkins 进行持续集成进行构建和测试，再由其他工程师进行人工评审，通过后并到发布分支，进行预发布，再通过持续集成进行构建，自建 Docker registry 进行构建物管理。构建出的 Docker 镜像在测试环境和预发布环境上依次进行自动化测试及人工测试，测试通过后，使用我们运维自己搭建的工具进行部署管理。\n  注：为了降低工具的成本和使用门槛，包括 CODING 在内的很多云服务厂商都在做打通全流程的 DevOps 工具链的尝试，希望可以做到让企业开箱即用，低成本的践行 DevOps 理念。不过基本上还是不能满足企业真正的需要。还是得自己搭建。\n2 DevOps 流程 这里其实并非 DevOps 所定义的，而是为了用一个更结构化的方式来归类及最佳实践。大体有以下流程，每个流程可以进一步细分为一系列阶段、过程。 {% img http://img.jemper.cn/2019/04/devops_stream.png 500 %}\n 规划流程，确定业务需求，制定交付时间表，选定交付方案 编码流程，特性被细化为故事，特性是使用业务语言描述的需要交付的功能性或非功能性需求。特性分解后的故事就是需要完成的源代码和脚本的技术描述。 构建流程，编译代码并且没有错误日志，源码评审，单元测试通过，代码合并 测试流程，执行集成测试和系统测试，服务功能验收测试执行完成 发布流程，验收测试通过 部署流程，通过部署流水线尽可能实现以自动化的方式在生产环境中推出新服务 运维流程，保持服务的稳定可靠，保证业务的连续性 监控流程，提交的是产品，不是项目  3 软件架构 软件框架一般是指单个服务内部的组织形式，软件框架一般是分层的（展示层、服务层、模型层、持久层），并有对类的管理如依赖注入容器等。软件的交互方式有：时间驱动、事件驱动、请求驱动。\n而软件架构是一个或多个服务的交互和部署方式：\n从服务来看：\n 传统单体架构：一般是负载均衡调度服务器，服务器连接缓存、文件、数据库，属于单服务架构，但是单服务也可能集群 面向服务架构：SOA， 微服务架构：自动化部署、端点智能化、语言和数据去中心化控制。交互方式一般有事件驱动（消息队列）和请求驱动（REST、RPC）  从部署来看：\n 分布式架构： 底层基础架构： 日志系统 监控与报警 API 网关 持续集成持续部署服务 宕机自动切换 消息中间件服务  4 组织架构 今日头条、抖音、西瓜视频……字节跳动的每款 App 都基于三个核心部门（技术、Growth 和商业化，分别负责研发、推广和收入）来发展。在项目开始时，公司会为每个项目设置虚拟项目组，由三个核心部门抽调人员组成，试水成功后直接推广。所有产品共用一条技术线，快速试错。针对 App 类产品的快速迭代的业务特性，字节跳动依据 DevOps 理念对组织架构进行调整和优化，从结构上保证了技术支持业务创新的能力。\n描述云时，常用电网进行类比。当要使用电时，你将设备插到标准电路上并打开开关。你要为你所使用的电付费。大多数情况下，你不用关心各个电力公司是如何发电和传输电力的。\nPaaS：提供一组核心服务（例如，托管 Java Web 应用、Ruby Gem 包、Scala 应用等）。一般与 Ops 部门提供的一些服务相似，Ops 部门通常接管基础设施层的管理并向开发团队提供一组环境选项来托管他们的系统。相比 IaaS，PaaS 的额外抽象着你能够集中精力在系统最重要的部分——应用。你不必处理网络配置、负载均衡器、操作系统、底层的安全补丁等。但这了意味着你要放弃对底层的可见性和控制。如果你是可以接受的，那么使用 PaaS 就很值得。然而，当你之后最终需要额外的控制时，迁移可能非常困难。\n从内存顺序访问1M大概需要 12us，从旋转型磁盘访问项目大概需要 4ms 将磁盘头移动到正确的位置，然后，读取1M数据大约花费 2ms。消息在相同数据中心的一个来回大概需要500us，而在加州和荷兰之间的一个来回大约花费150ms。——DevOps 软件架构师行动指南\nAtlassian 开发两类工具：一类是团队生产力工具，如 JIRA(问题追踪和软件开发)、Confluence Wiki、HipChat 消息和 JIRA 服务台；另一类是开发工具，例如 Bamboo 持续集成服务器和 Bitbucket 托管的存储库。\n5 软件框架 软件框架一般是指单个服务内部的组织形式，软件本质上就是为了运行逻辑，这种逻辑最终由函数或者方法承担。框架可以认为是一个调度器，设计各种模块以期最适当地运行需求逻辑。\n5.1 web 框架 web 框架目的暴露方法给用户，用户调用后调用逻辑方法，所以 web 框架提供了 http 各种模块，以最合适的方式让用户编写逻辑方法并与显露的接口对应起来。所以使用 web 框架应用步骤一般就是：\n 路由的设计 路由指向的逻辑句柄的开发  其他像日志等就是扩展功能了。\n5.2 爬虫框架 爬虫框架目的是从一个网址开始，调用自己的下载、分析、条目处理的方法处理各种数据，包括请求数据、响应数据、条目数据。 下面是爬虫框架的一种设计，可以看出，框架最核心的就是一个调度器，框架的所有东西都被调度器封装成一个整体。\n 框架：说是一个调度器并不为过 实现：就是实现与能与框架对接的句柄 启动：启动框架，然后输入参数，运行，输出结果  参考文献 [1] what-is-scrum. https://www.scrum.org/resources/what-is-scrum\n","id":21,"section":"posts","summary":"DevOps 是一套实践方法，在保证高质量的前提下缩短系统变更从提交到部署到生产环境的时间，它设想在开发组和运维组之间没有冲突。 1 什么是 DevOps DevOps 是敏捷开发的","tags":null,"title":"DevOps","uri":"https://blog.jemper.cn/2019/03/devops/","year":"2019"},{"content":"SSL/TLS 是基于 TCP 之上，HTTP 之下的协议。从技术角度上看，HTTP/2 作为新一代的协议，虽然协议文本中并未强制要求加密，但主要的浏览器（Firefox、Chrome、Safari、Opera、IE、Edge）已共同宣布，他们只支持实现基于 TLS 的 HTTP/2，也就是说加密将是下一代协议的强制事实标准。\n网络通信存在三个风险：\n 窃听风险（eavesdropping）：机密性，需要机密性，用公钥分发\u0026quot;对话密钥\u0026rdquo;，并使用它对称加密数据； 篡改风险（tampering）：完整性，对数据的篡改，数字签名，需要权威的 CA 机构证明； 冒充风险（pretending）：身份验证，对公钥的冒充，数字签名，需要权威的 CA 机构证明；  TLS 便是要解决通信中存在的安全风险，并提供减少延迟可用方案。对于安全风险的处理方式，为了便于理解，可以看这一张简化的流程图： TLS 是一个分层协议，具体的文档可以查阅《RFC 5246》。\n协议-\u0026gt;消息类型-\u0026gt;子消息类型（如果有的话） 其中握手协议的子消息类型 HandshakeType(十六进制)常规有以下这些： hello_request(0x00), client_hello(0x01), server_hello(0x02),certificate(0x0b), server_key_exchange (0x0c),certificate_request(0x0d), server_hello_done(0x0e),certificate_verify(0x0f), client_key_exchange(0x10),finished(0x14), (0xff) 扩展消息还增加了一些子消息类型，后面涉及时再介绍。需要注意的是，对于 TLS 协议来说，多个子协议可以合并到一个 TLS 协议包中，减少延迟；\n下面主要以 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 密钥套件为例来分析 TLS。\n1 记录层协议 主要有四部分的逻辑处理： （握手完成，从应用层接收数据）\n 数据分块：每个块的大小小于2^14字节（16KB）；生成 TLSPlaintext 结构； 压缩：由于存在一些安全问题，一般不启用压缩；TLSPlaintext 结构转换为 TLSCompressed 结构，如果不压缩，可以认为 TLSPlaintext 和 TLSCompressed 一致，其实是 TLSPlaintext 和 TLSCompressed.fragment 一致； 加密和完整性保护：TLSCompressed 结构转换为 TLSCiphertext 结构，对 TLSCiphertext 解压后 其实就是 TLSCompressed.fragment， 添加记录层消息头：看格式协议图； （交付给 TCP 层传输）  接下来单独解释一下加密和完整性保护，这里以 AEAD 为例，AEAD 函数将 TLSCompressed.fragment 结构转换/逆转换为 AEAD TLSCiphertext.fragment 结构。\nadditional_data = seq_num + TLSCompressed.type + TLSCompressed.version + TLSCompressed.length; （1）加密\nAEADEncrypted = AEAD-Encrypt(write_key, nonce, plaintext, additional_data) 其中 nonce 是明文传输的；plaintext 为 TLSCompressed.fragment；这里的 AEAD 函数校验数据完整性是通过 additional_data 这个数据，所以 Mac 密钥已经不需要了。下面这段引自 rfc5246\nAEAD ciphers take as input a single key, a nonce, a plaintext, and \u0026ldquo;additional data\u0026rdquo; to be included in the authentication check, as described in Section 2.1 of [AEAD]. The key is either the client_write_key or the server_write_key. No MAC key is used.\n（2）解密\nTLSCompressed.fragment = AEAD-Decrypt(write_key, nonce, AEADEncrypted, additional_data) 2 握手协议 完整的握手消息流.\n Client Server ClientHello --------\u0026gt; ServerHello Certificate* ServerKeyExchange* CertificateRequest* \u0026lt;-------- ServerHelloDone Certificate* ClientKeyExchange CertificateVerify* [ChangeCipherSpec] Finished --------\u0026gt; [ChangeCipherSpec] \u0026lt;-------- Finished Application Data \u0026lt;-------\u0026gt; Application Data 2.1 密码套件协商  client_hello: 密码套件支持，曲线支持（ecc_curve）等。 server_hello: 协商出密码套件。  密码套件提供了身份认证、数据机密性和数据完整性，是 TLS/SSL 的核心，可以说只有充分理解密码套件才能真正理解 TLS/SSL 协议。以 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256(IANA 表示法，编号 0xC02F，OpenSSL 表示为 ECDHE_RSA_AES128_GCM_SHA256) 密钥套件为例，其总体上由三部分组成：(1)Key Exchange(ECDHE_RSA)、(2)Cipher(AES_128_GCM)、(3)Mac(SHA256)。\n 使用 ECDHE_RSA 进行身份验证和密钥协商，协商预备主密钥；在密码套件中，身份验证和密码协商是一起理解的（也包含了证书的信息），他们可以是同一个算法。 ECDHE 和 ECHD 密码套件差不多，但是不包含匿名的密码套件； 使用 AES_128_GCM 算法和密钥块进行会话保护；因为在数据加密过程中，是需要保证机密性和完整性的，所以需要对称加密算法 + Mac 算法（注意这里的 Mac 没有使用 HMac 算法），亦即实现 AD 或者 AEAD 的加密模式。 使用 SHA256 算法的 PRF 和 HMAC 算法推导出密钥块。在 TLSv1.2 以前版本采用的 Hash 算法是硬编码，TLSv1.2 及以后的版本取决于 TLS 版本和密码套件，如果密码套件指定的 Mac 算法安全级高于SHA256，则采用 SHA384。  ECDHE_RSA 密码套件中的证书包含一个 RSA 服务器的公钥（注意 CA 公钥算法是在证书里指定的，不需要在密码套件中指定），对 DH_XXX、ECDH_XXX、DHE_XXX、ECDHE_XXX 套件来说，套件的后半部分对应的公钥不会用来加密或者数字签名（如果要用于数字签名，前提是证书 Key Usage 扩展必须置为 digitalSignature），也不限制 CA 机构签发证书所用的数字签名，所以服务器的公钥可以看作普通的消息，服务器的非对称加密没有存在的必要性。ECDHE_RSA 这些名字的存在仅仅是历史原因。\n2.2 身份认证  certificate：一般只是证明服务端的身份，银行大额转账则要求双向认证，这时候就需要用到银行发的 Ukey。  身份认证就是密码学中数字签名的实际应用例子，由权威的 CA 机构签名背书给服务器发放数字证书（类似现实生活中身份证，驾驶证），身份认证就是对数字证书的身份求证，除了 TLS 使用的数字证书，银行 Ukey 里的数字证书都是一样的原理。SSL/TLS 通信流程图结合密码学的数字签名就可以讲清楚身份认证的过程。\n数字证书解决了服务器身份认证和公钥传输问题，服务器实体并不能自己证明自己，所以需要通过 CA 机构来进行认证，对服务端来说 CA 机构的私钥用来给服务器颁发证书，对客户端来说浏览器集成了 CA 机构的公钥可以验证服务器证书。**理解数字证书就要理解一次对话中需要两对非对称加密：CA 机构的公钥私钥和服务器的公钥私钥，且这两套的算法没有必然的联系。**目前密钥协商已不再使用 RSA 算法，所以服务器的公钥和私钥已经限于身份认证功能了。\n2.3 密钥协商  server_key_exchange: 一般 HTTPS 网站会部署 ECDHE_RSA、DHE_RSA、ECDHE_ECDSA、RSA 这四种密码套件，这些套件中，前三个一定需要发送 server_key_exchange 子消息才有足够的信息进行密钥协商，最后一个不允许服务器发送 server_key_exchange 子消息，因为单靠 RSA 已经足以进行密钥协商（密钥传输），除 RSA 之外 DH_DSS、DH_RAS 都不需要该子消息。 对 DH_RSA 来说，ServerDhParams 里包含了 dh_p、dh_g、dh_Ys，即 p 大质数、g 生成元、Ys 服务器 DH 公钥。 对 ECDHE_RSA 来说，ServerECDHParams 里包含了 curve_params、public，即椭圆曲线（一般是命名曲线，需要双方都支持）、ECC 公钥。 另外该消息会对发送的 DH/ECDH 参数和公钥进行签名，不过这不是重点。 server_hello_done: 其实就是一条空消息，表示服务端发送了足够的消息，接下来可以和客户端一起协商出预备主密钥。 client_key_exchanges: 收到 server_hello_done 后客户端立刻发送该消息。以 ECDHE_RSA 为例，把客户端 ECDH 公钥（ecdh_Yc）发送给服务端。注意的是，client_key_exchange 一定会发的，而 server_key_exchange 不一定会发，如果理解了密钥协商的要素，也就理解是否需要发送了。  到此密钥协商过程就算结束，客户端和服务端都计算出了预备主密钥，接下来双方就各自推导出密钥块。密钥块是通过密钥衍生算法计算出来的，而且需要经过两次运算，计算主密钥和密钥块是两个不同的过程。 首先我们先来解释一下 TLS 采用的密钥衍生算法（KDF）——PRF(scret, label, seed)，其中 scret 是输入值，label 是标识符（对固定的运算有固定的标识符），seed 其实就是 salt（随机数），如果输入值和 salt 一样，则输出也是一样的，为了保证输出结果一致，客户端和服务端持有的 salt 值是相同的，一般是客户端随机数加客户端随机数。《RFC 5246》——5. HMAC and the Pseudorandom Function 部分能更进一步理解 PRF，从下面的描述可以看出，PRF 就用到了密码套件的 Mac 算法。\n5. HMAC and the Pseudorandom Function The TLS record layer uses a keyed Message Authentication Code (MAC) to protect message integrity. The cipher suites defined in this document use a construction known as HMAC, described in [HMAC], which is based on a hash function. Other cipher suites MAY define their own MAC constructions, if needed. In addition, a construction is required to do expansion of secrets into blocks of data for the purposes of key generation or validation. This pseudorandom function (PRF) takes as input a secret, a seed, and an identifying label and produces an output of arbitrary length. In this section, we define one PRF, based on HMAC. This PRF with the SHA-256 hash function is used for all cipher suites defined in this document and in TLS documents published prior to this document when TLS 1.2 is negotiated. New cipher suites MUST explicitly specify a PRF and, in general, SHOULD use the TLS PRF with SHA-256 or a stronger standard hash function. First, we define a data expansion function, P_hash(secret, data), that uses a single hash function to expand a secret and seed into an arbitrary quantity of output: Dierks \u0026amp; Rescorla Standards Track [Page 14] RFC 5246 TLS August 2008 P_hash(secret, seed) = HMAC_hash(secret, A(1) + seed) + HMAC_hash(secret, A(2) + seed) + HMAC_hash(secret, A(3) + seed) + ... where + indicates concatenation. A() is defined as: A(0) = seed A(i) = HMAC_hash(secret, A(i-1)) P_hash can be iterated as many times as necessary to produce the required quantity of data. For example, if P_SHA256 is being used to create 80 bytes of data, it will have to be iterated three times (through A(3)), creating 96 bytes of output data; the last 16 bytes of the final iteration will then be discarded, leaving 80 bytes of output data. TLS's PRF is created by applying P_hash to the secret as: PRF(secret, label, seed) = P_\u0026lt;hash\u0026gt;(secret, label + seed) The label is an ASCII string. It should be included in the exact form it is given without a length byte or trailing null character. For example, the label \u0026quot;slithy toves\u0026quot; would be processed by hashing the following bytes: 73 6C 69 74 68 79 20 74 6F 76 65 73 （1）计算主密钥：一旦客户端和服务端协商出预备主密钥，就会立刻计算主密钥，在 Change Cipher Spec 协议发送之前，客户端和服务端计算出主密钥就行。通过 PRF 函数将预备主密钥转换为主密钥后，客户端和服务端应该立刻从内存中删除预备主密钥，避免被窃取。计算公式如下：\n master_secret = PRF(pre_master_secret, \u0026quot;master secret\u0026quot;, ClientHello.random + ServerHello.random) [0..47]; 其中，客户端和服务端的随机数组合起来就是 seed；主密钥的长度固定是48字节，而预备主密钥的长度取决于密码协商算法，对 RSA 密钥协商其预备主密钥的长度是48字节，对 DH/ECDH 密钥协商其预备主密钥长度取决于 DH/ECDH 公钥。 （2）计算密钥块： 客户端和服务端计算出主密钥后，立刻计算密钥块（key_block），TLS 记录层协议（实际上是记录层协议上的应用协议）需要使用这些密钥块进行密码学机密性和完整性保护。 主密钥的长度固定是48字节，而密钥块的个数和长度取决于密钥协商算法。计算如下：\nkey_block = PRF(SecurityParameters.master_secret, \u0026quot;key expansion\u0026quot;, SecurityParameters.server_random + SecurityParameters.client_random); 2.4 完成握手  Change Cipher Spec 协议：该协议并不是握手协议的一部分，但是握手过程依赖于该协议，所以 为了避免握手流程理解中断得把它放在握手过程中。它是可以和握手协议的组成一个 TLS 包一起发送的。客户端和服务端计算出预备主密钥、主密钥和密钥块后，接下来通知对端后续的消息都需要 TLS 记录协议加密和完整性保护了，连接状态由待读、待写状态切换为可读、可写状态。该协议只有一个字节 finished：在 Wireshark 中显示的是 encrypted_handshake_message。理论上只要发了 Change Cipher Spec 就可以对数据进行加密保护了，但因为握手协议过程中的包都没有加密和完整性保护，为了避免消息篡改，客户端和服务端需要校验对方发送的 Finished 子消息确保握手消息没有被篡改。该消息一定是在 Change Cipher Spec 子消息之后，否则会产生一个致命的错误。一旦客户端和服务端都校验了对方的 Finished 子消息，那么接下来就可以立刻加密保护应用层数据了。  3 会话恢复 上面讲的是完整的握手过程，接下来讲解通过会话恢复实现简短的握手过程。 会话恢复有两种形式，分别是基于 Session ID 的会话恢复和基于 Session ticker 的会话恢复。\n3.1 基于 Session ID 的会话恢复 《RFC 5246》文档对这一内容有描述。 基于 Session ID 的会话恢复是由服务端存储会话信息，TLS 协议只是规定 Session Cache 的存储方式，没有考虑如何实现 Session Cache，对分布式服务来说增加了复杂性，目前 Nginx 官方就没有支持分布式服务器 Session Cache。所以一般不采用。\n  基于 Session ID 进行完整的握手（会话建立）：Client Hello 的 Session ID 值为空，服务端创建新的 Session ID。建立完成后： (1) 客户端：仅保存 Session ID 值在内存中。 (2) 服务端：将会话信息保存 Session Cache 中，键值就是 Session ID，键值对应的内容就是会话信息。一个完整的会话信息包括： session id、证书（对端的，一般为空）、压缩算法（一般不启用）、密码套件、主密钥、会话可恢复标识（表示某个会话是否可恢复）。\n  基于 Session ID 进行简短的握手（会话恢复）：Client Hello 的 Session ID 值不为空。服务端检查是否可以恢复会话，如不能则进行完整的握手协议，同时生成一个新的 Session ID。如能恢复连接，则直接发送 ChangeCipherSpec 和 Finished，而不进行协商（主密钥存在 Session Cache 中）。\n  3.2 基于 SessionTicket 的会话恢复 详见文档《RFC 5077》——没有服务器端状态的会话恢复，它属于扩展，是 HTTPS 的一个重要知识点。 服务器将会话信息加密后以 ticket 的方式发送给客户端，服务端本身不存储会话信息，客户端收到 ticket 后存储在内存中，客户端只是存储和传输 ticket，不涉及 ticket 的解密。 添加了一个 HandshakeType 子消息类型 session_ticket（0x04），如果服务端 ServerHello 消息包含 SessionTicket TLS 扩展，则必须发送该子消息；如果服务端 ServerHello 不包含 SessionTicket TLS 扩展，则不能发送该子消息。\n 服务器返回 NewSessionTicket 完整握手的消息流  Client Server ClientHello (SessionTicket extension) --------\u0026gt; ServerHello (empty SessionTicket extension) Certificate* ServerKeyExchange* CertificateRequest* \u0026lt;-------- ServerHelloDone Certificate* ClientKeyExchange CertificateVerify* [ChangeCipherSpec] Finished --------\u0026gt; NewSessionTicket [ChangeCipherSpec] \u0026lt;-------- Finished Application Data \u0026lt;-------\u0026gt; Application Data  使用新 NewSessionTicket 进行简短握手的消息流   Client Server ClientHello (SessionTicket extension) --------\u0026gt; ServerHello (empty SessionTicket extension) NewSessionTicket [ChangeCipherSpec] \u0026lt;-------- Finished [ChangeCipherSpec] Finished --------\u0026gt; Application Data \u0026lt;-------\u0026gt; Application Data 注意，发送 NewSessionTicket 子消息来更新 ticket，ticket 也是有有效期的。\n4 总结 TLS 为了更安全必然造成一些损耗，但其的设计上已经做了优化，比如采用对称加密。如果采用 HTTP/2 协议传输，那么带来的性能提升完全可以抵消 TLS 带来的性能损耗。下面这些诀窍可供借鉴。\n 保持连接开启尽可能长的时间。TLS 成本最高的地方就是连接时的握手过程。如果连接时间能保持足够长，握手次数就可以减少。 使用基于 SessionTicket 的会话恢复，允许客户端复用上次的握手直接重连服务器。 使用内置加解密支持的芯片。  参考文献 [1] 虞卫东. 深入浅出 HTTPS 从原理一实战. 版次：2018年6月第1版 [2] 阮一峰. HTTPS 升级指南 http://www.ruanyifeng.com/blog/2016/08/migrate-from-http-to-https.html. 2016年8月19日\n","id":22,"section":"posts","summary":"SSL/TLS 是基于 TCP 之上，HTTP 之下的协议。从技术角度上看，HTTP/2 作为新一代的协议，虽然协议文本中并未强制要求加密，但主要的浏览器（Firef","tags":["TLS","协议"],"title":"SSL/TLS 协议","uri":"https://blog.jemper.cn/2019/03/tls/","year":"2019"},{"content":"近代加密都是加密算法公开的，自行设计算法要求编程阶段就要进行协商开发，且存可能是不严格的数学模型存在安全漏洞。一般企业内部也是用流行的密码学算法（语言支持的加密算法函数）。学习密码学有利于分析安全工具其背后的密码学原理，比如数字证书、SSH、AES的密码原理等。\n密码学四个目标：\n 机密性（隐私性）：对称加密算法和非对称加密算法都能够保证机密性，另外机密性并不能保证完整性； 完整性：数据完整且不能被篡改；消息验证码算法（MAC）保证完整性，MAC 值 = mac(消息，密钥)；另外完整性并不一定需要机密性，要看数据敏感度； 身份验证：数字签名，也就是用私钥加密数据（一般是加密比较短的 hash 消息摘要）； 不可抵赖性：也是由数字签名实现；  以下将分三部分简单归纳：基础算法、简单加密算法和组合加密算法。不过本文只讲解交互过程，不讲解具体实现。 需要强调的是，似乎每隔几个月，服务器和 HTTPS 就会曝出新的漏洞，针对这些漏洞及攻击摒弃老旧的加密算法，RC4、DES、MD5 等都已经强制丢弃。所以有些算法其实并不实用，但可了解其原理。\n1 基础算法 随机数生成器和密码学 Hash 算法都是密码学中的基础算法，很多其它的密码学算法选择这两个算法作为加密基元。\n1.1 随机数生成器算法    名称 生成类型 特性 内部状态     真正的随机数生成器 硬件生成 效率高、随机性、不可预测性、不可重现性 从物理设备获取，时间、温度、声音等熵源   伪随机数生成器 软件生成 效率高、随机性 种子（seed）   密码学伪随机数生成器 软件生成 效率高、随机性、不可预测性 用于密码学，是真正随机数生成器的一种    随机数生成器内部会维护一个状态，如果每次熵和种子是一样的，生成的随机数也是相同的，所以熵和种子对于随机数生成器非常重要。 密码学应用中很多场景会涉及随机数，不同的用途有不同的称呼，常见如下表：\n   名称 说明     密钥 对称加密算法、公开密钥算法、Mac算法，密钥本质上是一个随机数   初始化向量（IV） 块密码算法中很多迭代模式会使用IV   nonce 块密码算法中的CTR模式、AEAD加密模式也会用到nonce   salt 基于口令的加密算法会用到，通过salt生成一个密钥    其中密钥的作用如下表：\n   名称 作用 说明     对称加密算法密钥 加密解密 密钥不能泄露   非对称加密算法密钥 加密解密 公钥可以公开，私钥不能泄露   Mac 算法的密钥 消息加密和验证 密钥不能泄露   数字签名算法的密钥 身份验证 公钥可以公开，私钥不能泄露   会话密钥 加密解密 密钥不能泄露，该密钥一般配合对称加密算法进行加密解密   基于口令的密钥 进行权限校验，加密解密等 口令不能泄露    生成密钥一般用两种方式：\n 基于伪随机生成器生成密钥； 基于口令的加密（PEB）算法生成密钥，即使用同样的口令就能生成同样的密钥。  1.2 Hash 算法 是一种从任意文件中创造小的数字「指纹」的方法，因为 Hash 是单向性的，所以不是加密算法了。基于 Hash 算法产生的其它密码学算法有很多：Mac 消息验证码、伪随机数生成器、数字签名、块密码加密算法等。需要注意的是密码学 Hash 算法和普通 Hash 算法不是同一个概念，普通 Hash 算法实现的 Hash 表与检验和（checksum）不能用于密码学。 密码学 Hash 算法，有 MD5（128） 算法 和 SHA 族类，SHA 族类有 SHA-1（160）、SHA-2（SHA-256、SHA-512、SHA-224、SHA-384）、SHA-3（SHA3-256、SHA3-512、SHA3-224、SHA3-384） 三类算法（数字二进制数，除以4为16进制数），基于碰撞考虑建议用 SHA-2 类。\nHash 算法一般还用于文件比较和密码口令，密码口令为了防止攻击者对常用密码字典破解；推荐使用 密码衍生算法（KDF），利用盐值（随机数）增大密钥的搜索空间，简单的理解就是通过某些值可以生成任意长度的一个（多个）密钥，常见的有 PBKDF2、bcrypt、scrypt，还有 TLS 使用的 PRF 函数等。\n2 简单加密算法 2.1 对称加密 在日常开发中是最常用的保证数据机密性的方式；在 HTTP 中因为非对称加密计算量大，所以频繁数据传输采用了\u0026quot;对话密钥\u0026quot;进行对称加密。对称加密中密钥由双方保管。 对称加密算法有很多，如一次性密码本（XOR运算）、流密码算法（RC4_128、CHACHA20）、块密码算法（AES_256_CBC、3DES_EDE_CBC）、AEAD算法（AES_GCM（目前主流的AEAD）、AES_CCM、CHACHA20_Poly1305（谷歌提出的AEAD）），AEAD加密模式由于同时完成了机密性和完整性保护，在安全性上更有保障，服务器应该优先支持相关的密码套件。\n2.2 非对称加密 也叫公开密钥算法，因为计算量大，日常开发不会用来保证数据机密性，而是基于网络传输信息不安全事实，采用非对称加密进行身份认证。非对称加密的私钥由谁生成就由谁保管；对服务器来说，私钥是服务器生成的，由服务器保密，公钥给用户；对 github.com 来说，私钥由用户生成的，由用户保密，公钥上传配置到 github.com。\n公开密钥算法最重要和最广泛使用的算法就是 RSA 算法，它是 Ron Rivest、Adi Shamir、Leonard Adleman 三人创建的。 **RSA 算法是一个多用途的算法，可以进行加密解密、密钥协商、数字签名；**基础原理都是一样的：公钥加密私钥解密，私钥加密公钥解密。\n 加密解密：双向加密，如果不考虑性能，双方可以用各自的密钥对实现双向加密。 密钥协商：单向加密，即公钥加密密钥发给对方，对方用私钥解密得到密钥，实现对称加密。目前已经被淘汰。 数字签名：也是单向，不过和密钥协商相反，是用私钥加密数据（签名值），把签名值和数据发给对方，对方用公钥解密难签名。数字证书是数字签名一种例子，只是用 CA 机构的私钥给服务器的公钥这个数据进行签名而已。一言蔽之：数字签名是为了证明身份，不是为了加密数据。  RSA 原理 n = p*q, 其中p、q是大质数, φ(n) = (p-1)(q-1); 公钥 e、n: 1 \u0026lt; e \u0026lt; f(n), e 和 f(n) 互质; 私钥 d、n: e*d mod f(n) = 1. 签名：m^e mod n = c（密文） 解密：c^d mod n = m（明文） 安全性：传播 n、e、c; 要解密需要 n、d、c, 由 e 要推导出d, 必须先由n推导到p、q, 大因数分解很难. 基于算法是可以知道 RSA 如果加密的明文 m \u0026gt; n，就会有二义性，所以是不适合用来加密明文的，只用来加密密钥（比 n 小）。\n3 组合加密算法 组合基础算法和简单加密算法实现密码学的“机密性”、“完整性”、“身份验证”和“不可抵赖性”中的一个或者多个的算法；比如消息验证码需要组合 Hash 算法和加密算法、TLS 协议握手需要协商出密码套件。\n3.1 消息验证码(Message Authentication Code, Mac) 实现了完整性。 消息验证码适用于不需要对数据进行加密，只需要保证数据完整性的情形，当然也可以变通，既对数据进行加密同时也保证完整性。HTTP 应用最多的 Mac 算法是 HMAC 算法（使用 Hash 算法作为基元，更具体算法比如 HMAC-SHA256 算法使用了 SHA256 作为加密基元），还有 TLS 的 AEAD 使用了 GHASH 算法（也是一种 Mac 算法）、CMAC。Mac 不能进行身份验证，原因就在于消息发送方和接收方拥有同样的密钥，所以双方都可以抵赖。 MAC 采用一个消息和密钥作为输入，并且产生一个 MAC 标记作为输出。这个标记和消息一起，可以被任何拥有相同密钥的成员校验。 这类算法的变种很多，原理的步骤如下： （1）把消息先进行 Hash 运算，也就是使用 Hash 算法得到消息的摘要值； （2）双方都有相同的密钥，通过这个密钥对消息摘要值进行 MAC 运算，得到“MAC 值”；把消息和“MAC 值”传给对方； （3）接收方用相同的密钥对“MAC 值”进行解密得到消息摘要，对消息进行 Hash 运算得到消息摘要； （4）比较消息摘要；\n3.2 加密和验证模式 **实现了机密性和完整性。**对称加密算法可以保证消息的机密性，Mac 算法可以保证消息的完整性，将两者结合起来，就可以保证消息同时具备机密性和完整性。\n3.2.1 AD 加密模式 使用者结合对称加密算法和 Mac 算法，提供机密性和完整性的模式也叫 AE 加密模式（暂且理解 AE、AD 是一样的意思吧 ），主要有三种方式，只是加密算法和 Mac 算法顺序问题，总体上和消息验证码很相似，只是用同一个密钥对消息也进行加密。\n Encrypt-and-Mac Mac-then-Encrypt Encrypt-then-Mac（建议） 先对消息进行加密得到密文，然后对密文再计算 Mac 值，最终将密文和 Mac 值组合一起发送给接收方。   3.2.2 AEAD 加密模式 AEAD 是 AE 加密模式的一种变体，是在底层组合了加密算法和 Mac 算法，能够同时保证数据机密性和完整性，减轻了使用者的负担，与 AD 相比，AD 是使用者自己实现，AEAD 是底层实现（里面使用了结构，包含了密文和 用于 Mac 校验的属性）。主要有三种方式。\n CCM 模式，HTTPS 中使用得少，底层采用的是 Mac-then-Encrypt。 GCM 模式，流行的 AEAD 模式，采用了 GHASH 算法（一种 Mac 算法）进行 Mac 运算，不需要 Mac 密钥，使用块密码 AES 算法进行加密运算，在效率和性能上都很出色。 ChaCha20-Poly1305, 使用了 ChaCha20 流密码算法进行加密运算，使用 Poly1305 算法进行 Mac 运算。  注意，AEAD 的 Mac 算法是不用 Mac 密钥的，而是用 additional_data 属性来校验数据完整性的。 3.3 数字签名 实现了完整性和身份认证。 数字签名和消息验证码流程一样都不要求机密性，不同的地方仅仅在于数字签名用了非对称加密，而消息验证码用了对称加密，因此数字签名实现了防抵赖机制。数字签名具有以下特点：\n 防篡改：数据不会被修改，“MAC”算法也有这个特点； 防抵赖：消息签署者不能抵赖。MAC 因为用了双方共同拥有的密钥所以没有这个特点，而数字签名双方拥有不同的密钥（公钥和私钥）所以能防抵赖； 防伪造：发送的消息不能够伪造，MAC 算法也有这个特点；  目前使用较多的数字签名有 RSA、DSA、ECDSA 等。数字签名算法建议对消息摘要值进行签名，因为摘要值的长度是固定的，运算的时候速度会比较快。\n3.4 密钥协商算法（Key Exchange） **协商出预备主密钥。**客户端和服务端协商出预备主密钥，则协商结束（可以理解为主密钥和密钥块也可随时生成）；接下来就是通过密钥衍生算法（KDF）由预备主密钥推导出密钥块，KDF 有如 PBKDF2，但 PBKDF2 函数运算较慢，在 TLS 协议中没有可行性，TLS 使用一种称为 PseudoRandom Function(PRF)的函数进行密钥块推导，该算法和 HMac 一样使用 Hash 算法作为加密基元。主密钥和密钥块是 TLS 部分的内容，因此推导过程在《SSL/TLS 协议》一文中介绍。\n密钥的存储和传输分为静态密钥和动态密钥，在网络通信应用中，密钥的管理和分配是个难题，尤其是生成一个动态密钥更难，而密钥协商算法就可以解决密钥分配、存储和传输等问题。动态密钥存储在进程中，一旦客户端和服务端的连接关闭就会消失。\n3.4.1 RSA 密钥协商算法 客户端生成一个随机值，也就是会话密钥，并用 RSA 密钥的公钥进行加密并发给服务端，服务端用私钥解密得到会话密钥，到此双方完成连接，采用对称加密算法和会话密钥加密解密数据。服务器的公钥和私钥正是用于密钥协商过程中，只需要用到一次，后面的对话都不再会用到私钥了。 不足之处： （1）获取会话密钥过程其实并不能称为协商，完全是由客户端决定的，称为密钥传输更合适。攻击者不会云破解 RSA 加密算法的私钥，直接暴力破解会话密钥就能反解出明文。 （2）不能提供前向安全性。即长期使用的主密钥泄漏会导致过去的会话密钥泄漏。\n3.4.2 DH 密钥协商算法 DH（Diffie-Hellman）算法在进行密钥协商的时候，通信双方的任何一方无法独自计算出一个会话密钥，通信双方各自保留一部分关键信息。 在使用 DH 算法之前，先要生成一些公共参数，这个参数是公开的，一般由服务端生成，然后发给对方。 参数有两个，分别是 p 和 g，p 是一个很大的质数，g 表示为一个生成器，值很小；服务端通过 p 和 g 参数，生成一个 DH 密钥对 priv_key(b，b是一个随机数) 和 pub_key(Ys) =（g^b）mod p，私钥需要保密；p、g 和 pub_key 发给客户端，客户端也生成一个 DH 密钥对 priv_key(a，a是随机数)和 pub_key(Yc) = (g^a) mod p，并把 pub_key 也发送给服务端。服务端根据 Yc 和 b 计算 Z = (Yc^b) mod p，客户端根据 Ys 和 a 计算 Z = (Ys^a) mod p。Z 即预备主密钥，到此协商完成。 可以看到 DH 和 RSA 算法一样属于离散对数和因式分解问题。而 DH 协商中不需要用到服务器的私钥，服务器私钥的功能被削弱到用来身份认证，因此 DH 密钥协商不存在 RSA 密钥传输存在的问题。目前密码套件都是用 DH/ECDH 套件，所以服务器的密钥对已经被弱化了。\n在 TLS 中，密码协商都是依赖 client key exchange 和 server key exchange 两个子消息来传递的。\nDHE 和 ECDHE 密码套件实际上分别称为临时 DH 密码套件和临时 ECDH 密码套件，所谓临时（ephemeral）表示客户端和服务端每次生成的 DH 密钥和 ECC（椭圆曲线）密钥都是动态密钥。\n 静态 DH 算法（DH 算法） 静态 DH 算法，p 和 g 两个参数永远是固定的，而且服务器的公钥（Ys）也是固定的。和 RSA 密钥协商算法一样，一旦服务器对应的 DH 私钥泄露，就不能提供前向安全性。静态 DH 算法的好处就是避免在初始化连接时服务器频繁生成参数 p 和 g，因为该过程是非常消耗 CPU 运算的。 临时 DH 算法（DHE 算法） 在每次初始化连接的时候，服务器都会重新生成 DH 密钥对，DH 密钥对仅仅保存在内存中，不像 RSA 那样私钥是保存在磁盘中的，攻击者即使从内存中破解了私钥，也仅仅影响本次通信，因为每次初始化的时候密钥对是动态变化的。更安全的是，协商出会话密钥后，a 和 b 两个私钥可以丢弃，进一步提升了安全性。  只要理解DHE密钥交换原理，那么理解ECDHE密钥交换并不难。ECDHE的运算是把DHE中模幂运算替换成了点乘运算，速度更快，可逆更难。\n 客户端随机生成随机值Ra，计算Pa(x, y) = Ra x Q(x, y)，Q(x, y)为全世界公认的某个椭圆曲线算法的基点。将Pa(x, y)发送至服务器。 服务器随机生成随机值Rb，计算Pb(x,y) - Rb x Q(x, y)。将Pb(x, y)发送至客户端。 客户端计算Sa(x, y) = Ra x Pb(x, y)；服务器计算Sb(x, y) = Rb x Pa(x, y) 算法保证了Sa = Sb = S，提取其中的S的x向量作为 Premaster Key（预备主密钥）。  客户端和服务端进行协商算法的时候，服务器使用 Server Key Exchange 子消息传递相关的参数（DH 参数和 ECC 命名曲线）和服务器的公钥；客户端收到服务器发送的相关参数后，需要使用这些参数生成客户端的公钥（DH 公钥和 ECDH 公钥），客户端和服务端结合双方的密钥才能协商出一致的预备主密钥。\n4 OpenSSL 库或命令工具 程序开发、证书生成都需要使用实现了各种密码学算法的 openssl 库或命令工具，目前 openssl 已经被大部分系统替换成 LibreSSL（PHP 大部分内置的密码学函数也是基于底层的 LibreSSL 库），只是工具名还是习惯地称 openssl。 常见系统如 OpenBSD 自 5.6 起，macOS 自 10.11 El Capitan 起，Alpine Linux 自 3.5.0 起。在 macOS 上测试如下：\nadadeMBP:~ ada$ openssl version LibreSSL 2.6.5 参考文献 [1] 虞卫东. 深入浅出 HTTPS 从原理一实战. 版次：2018年6月第1版 [2] Mrpre. SSL中的RSA、DHE、ECDHE、ECDH流程与区别 https://blog.csdn.net/mrpre/article/details/78025940. 2017年09月19日 [3] TLS Perfect Forward Secrecy 之 DH/ECDHE https://blog.nlogn.cn/pfs-ecdhe/\n","id":23,"section":"posts","summary":"近代加密都是加密算法公开的，自行设计算法要求编程阶段就要进行协商开发，且存可能是不严格的数学模型存在安全漏洞。一般企业内部也是用流行的密码学","tags":["TLS","密码学"],"title":"密码学","uri":"https://blog.jemper.cn/2019/03/cryptology/","year":"2019"},{"content":"本文谈到的并发是指单程序、单节点并发，区别于并发系统，并发系统的一个更加流行的词是分布式系统，并发系统更有可能是并行的，因为其中的多个程序一般可以同时在不同的硬件环境上运行。分布式系统可以看我另一篇文章《微服务架构的复杂性》。 并发指的是多个任务几乎被同时发起运行，但是在同一时刻这些任务不一定都处于运行状态，这取决于 CPU 核心或者 CPU 数量。并行指的是在同一时刻可以有多个任务真正地同时运行。并行运行的必要条件是多 CPU 核心或者多 CPU 的计算环境。 在功能开发中，非并发程序往往未能充分利用服务器的性能，为用户提供服务基本都是排队处理。而并发程序有时可以代替集群，其性能提高对整个系统是至关重要的作用，如果我们将单个节点的性能提高 30%，或者甚至超过 100%，那么我们可以节省多少台机器呢？不过同时并发却带来了编程的复杂性。不同程序单元之间的远程过程调用可以参考另一篇文章《go 网络编程》。Go 语言的特点是通过内部调度可以最大限度地利用单机的计算能力。然而在分布式计算方面，它本身其实并没有提供什么现成的东西，还需要使用一些第三方的框架或工具，或者自己编写和搭建。\n1 为什么并发很难？ 并发的困难在于通信，通信有两个要保证：\n 数据竞争：临界区 主要原因是临界区引起的，任何空间，只要被同时访问，都可能发生问题。洗手间就是现实世界中的临界区，互斥量就是洗手间的使用规则。 (1) 进去时锁上门，出来时再解锁门； (2) 其它人需要在门外等待； (3) 等待的人可能很多，需要排队进入。 需要根据不同程序酌情考虑究竟是扩大还是缩小临界区，临界区大了其它被阻塞线程等待时间较长，临界区小了频繁调用互斥量也是缺点。 顺序竞争：即使保证了同步问题，也不一定就保证了顺序执行的问题，所以这个问题也要重视，不过这个问题容易解决，所以不展开谈。  本文主要讲解线程间并发，但这里也简单的列举进程间的通信方式：\n 管道、消息队列 信号：它是唯一异步的 IPC 方法，我们可以通过 kill 给进程发送信号，进程采用 notice 异步监听信号。有50几种信号，不同系统有细微差别，但总体上是一致的。对 Go 要提醒一点的是运行时要 build 成执行文件直接运行，不要用 go run 间接（包装）运行，否则发送的信号可能被外层程序截获。  可以直接 kill -s sigkill 58148 或 kill -n 9 58148 方式发送信号 ctrl-c 发送 SIGINT 信号给前台进程组中的所有进程。常用于终止正在运行的程序。 ctrl-z 发送 SIGTSTP 信号给前台进程组中的所有进程，常用于挂起一个进程。 ctrl-\\ 发送 SIGQUIT ctrl-d 不是发送信号，而是表示一个特殊的二进制值，表示 EOF。 SIGKILL 和 SIGSTOP 两种信息不能自行处理   共享内存：虚拟内存和物理内存的原理 socket、Streams：不同主机之间就只能选择这类通信  2 原子性 如果某个东西是原子的，隐含的意思就是它在并发环境中是安全的。 谈论原子性必然要谈“上下文（context）”这个词，上下文的概念很多，有函数层级的上下文（函数栈），有程序界限的上下文（php和redis），有用户程序和系统程序或CPU的上下文（程序的运算和内核的运算），原子性可能在某个上下文中有些东西是原子性的，而在另一个上下文中却不是。在考虑原子性时，经常第一件需要做的事就是定义上下文或范围然后再考虑这操作是否是原子性的。一切都应该遵循这个原则。 比如 go 中 i++ 是由三步组成的，是不可分割的，但是可中断的。\n 检索 i 的值 增加 i 的值 存储 i 的值 在非并发程序中，或者并发程序中但没有把 i 暴露给其它 goroutine，那么它是原子性的。 redis的原子性是指位于redis服务器上下文中的原子性。当然多个原子性一起并不会产生更大的原子性。以前写php程序，对redis某个值加1，用incr操作是原子性的，用检索、增加、存储三部走对单个php-fpm也是原子性的，但对多个php-fpm就不是原子性的。 要实现并发安全，必须是变量在所有上下文中都是原子性的。  3 互斥量 Mutex 3.1 C 语言的互斥量函数： pthread_mutex_t mutex; int pthread_mutex_init(pthread_mutex_t * mutex, const pthread_mutexattr_t * attr); 成功时返回 0，失败时返回其他值。 int pthread_mutex_destroy(pthread_mutex_t * mutex); 成功时返回 0，失败时返回其他值。 int pthread_mutex_lock(pthread_mutex_t * mutex); 成功时返回 0，失败时返回其他值。 int pthread_mutex_unlock(pthread_mutex_t * mutex); 成功时返回 0，失败时返回其他值。\n4 条件变量 Conditions 是对互斥量的补充，因为互斥量只有两种状态。条件变量进入阻塞，等待通知。\n5 信号量 Semaphore 信号量和互斥量很相似，只是用 0 和 1 （二进制信号量）控制，信号量不能为负数，否则便阻塞。 sem_t sem; int sem_init(sem_t * sem, int pshared, unsigned int value); 成功时返回 0，失败时返回其他值。value 为初始信号量。 int sem_destroy(sem_t * sem); 成功时返回 0，失败时返回其他值。 int sem_wait(sem_t * sem); 成功时返回 0，失败时返回其他值。相当于 lock，信号量 value 值 -1。 int sem_post(sem_t * sem); 成功时返回 0，失败时返回其他值。相当于 unlock，信号量 value 值 +1。\n6 并发模式 ","id":24,"section":"posts","summary":"本文谈到的并发是指单程序、单节点并发，区别于并发系统，并发系统的一个更加流行的词是分布式系统，并发系统更有可能是并行的，因为其中的多个程序一","tags":["Golang","并发"],"title":"并发的复杂性","uri":"https://blog.jemper.cn/2019/03/go-concurrency-difficult/","year":"2019"},{"content":"理解 I/O 的一些概念问题可以先看另一篇文章《C Socket 编程》 Go 语言把 I/O 操作抽象成为 Reader 和 Writer 接口，并在 C 语言的基础上又设置了一层缓存操作。\n1 简介 先进行接口设计，后进行实现设计，即对每一个接口方法进行设计。\n1.1 接口设计 定义一个接口，是要设计一种方法集。在做接口设计的时候，每一个方法的具体功能一定要能表述出来；并且根据所有方法，其中可以汇总出核心必备的属性，当然接口不能拥有属性，这只是一种脑补的属性，有利于实现设计。\n可以分析 io 的接口实现，包括所有接口的功能描述和脑补核心属性：\n io.Reader：只有一个 Read 方法   Read 方法的功能：可以从中读取数据，可以分多次读取 脑补核心属性：有两个，一个是源比如字符串、文件等；另一个是对源的当前读取到的指向   io.Writer：只有一个 Write 方法   Write 方法的功能：可以把数据写进去，能写多少依据源的能力 脑补核心属性：直接一个源就可以了  1.2 实现设计 根据要实现的方法集的一种设计，并可以由此知道需要的属性。原则上对每一个方法，如果有输出的话，则无论采用哪种设计，对同样的输入（或者同样的无输入），应该要有同样的输出。 实现其实就是一种设计（当然也可以扩充其功能），比如 strings.Reader 基本上就是比较原始的设计，而 bufio.Reader 采用了缓冲的设计实现了同样的功能。接下来我们列举一些常用的实现，并分析其原理。\n1.3 io 包 io 包中定义有：\n 大量的接口 一些通用函数 小量的接口实现，在原io中的实现并不常用，strings、bufio 等包对 io 的实现更常用一些。  总体来看，io.go 主要是进行接口设计，实现设计是次要的。\n2 bufio 实现设计 bufio.Reader 封装了 io.Reader，bufio.Writer 封装了 io.Writer，接下来就分别从这两个类型进行讲解。\n2.1 bufio.Reader bufio 封装了对应 io 的同时多了 buf 属性，和对 buf 的控制r、w这两个属性，读和写其实就会优先从 buf 进行，\nbufio.Reader 主要有buf、r、w、rd(io.Reader)等属性（其它属性对原理的理解相对次要所以不提），把读操作优化成“rd-\u0026gt;buf-\u0026gt;变量”，除了一些特殊情况还是保留“rd-\u0026gt;变量”的读取方式。 读取主要分两类：1、确定读取长度，如bufio.Read，bufio.ReadByte等；2、确定读取到某个字符，如bufio.ReadSlice，bufio.ReadLine，bufio.ReadBytes，bufio.ReadString等\n bufio.Read：   如果 buf 不为空，则从 buf 取数据尽可能多的把 p 填满（可能填不满，这种情况下即使io.Reader还有未读数据，也不会再去取，也就是只取 buf 数据）。 如果 buf 为空，即 r==w，则判断要取的长度是否大于buf，如果大于等于buf，再直接从io.Reader取，也只有这种情况下是直接取的即“rd-\u0026gt;变量”；如果小于 buf，则把buf填满，然后回到1；   ReadSlice 或 ReadLine 该两个方法比较底层，不建议使用，这里需要注意的，返回的其实是指向 bufio.buf 属性的切片，因为 bufio.buf 底层的数组指针一直不变，而值却在变，因为返回的切片区域可能会因为 bufio.buf 值的改变而改变，特别在多次调用时要注意。\n  ReadBytes 或 ReadString 这两个方法通过调用 ReadSlice 实现，并在最终 copy 到新创建的切片返回，所以多次读取很安全。\n  WriteTo 它的功能是把全部数据写到 Writer，对于 bufio，必须分两步：\n   第一步把当前的缓冲写到 Writer，内部函数 writeBuf 就是单纯把缓冲区写到 Writer 然后把还没有缓冲的问题写到 Writer，判断逻辑是：源 Reader 有 WriterTo 就调用其 WriterTo，没有就走 fill 填充（也就是调用源 Reader 的 Read 读取）  2.2 bufio.Writer 如果写入的数据大于缓冲区，则直接写入。 写入只是写到缓冲区，注意需要调用 Flush 方法写入，否则即使是程序运行结束也不会写入。\n ReadFrom   如果缓冲为空且源 ReaderFrom 存在，则直接从源里调用 否则循环的取到缓冲中，并 Flush  3 bytes.buffer 实现设计 主要属性有 buf []byte 和 off：read at \u0026amp;buf[off], write at \u0026amp;buf[len(buf)]\n4 Socket 实现设计 首先我们看一下一段 TCP 代码 socker.go。\nnet.Dial 和 net.Listen().Accept() 返回 socket，可读可写，本质上是经过以下步骤取得文件句柄的： net.Listen().fd.accept() 取得 fd 句柄，然后创建 net.conn（实际上 socket 还要经过协议一层包装）\ntype conn struct { fd *netFD } 抓包分析：以代码的逻辑，TCP 数据一定是完整传输的，且每次发送的时间都有 PSH 标识；但因为并发的原因，服务端仅一次且不确定时间地读取 TCP 数据，导致在服务端读取之后发送的数据没有继续读取。 4 ioutil 工具 ","id":25,"section":"posts","summary":"理解 I/O 的一些概念问题可以先看另一篇文章《C Socket 编程》 Go 语言把 I/O 操作抽象成为 Reader 和 Writer 接口，并在 C 语言的基础上又设置了一层缓存操作。 1 简介 先进行接口","tags":["Golang","io"],"title":"go I/O 操作","uri":"https://blog.jemper.cn/2019/03/go-io/","year":"2019"},{"content":"本篇主要内容是 Go HTTP 原理和 RPC 相关知识。 在进行原理讲解之前我觉得有必要熟悉一下 TCP 传输原理和 C Socket 编程，因为 Go 是在 C 的基础上进行封装的，这样对哪些属于 C，哪些属于 Go 有一个清晰的了解，也更能避繁就简。\n1 Go HTTP 原理 1.1 服务端 1.1.1 三层逻辑 先说明一下，这种分类仁者见仁，智者见智，能说清楚脉络就是好方法。\n ServeMux 类型：有两大类方法： (1) HandleFunc()、Handle() 创建其属性，m: map[string]muxEntry 路由和函数执行体； (2) ServeHTTP()、Handler()、handler() 接收到请求后对 m 的路由查找，找到函数执行体； Server 类型：包含了 ServeMux 这个 Handler 接口类型，ListenAndServe() 和 Serve() 方法主要负责监听和接口请求，并启用 goroutine 调用下面 conn 类型的 serve()； conn 类型：包含了 Server 类型，serve() 方法，先通过 serverHandler 类型的 ServeHTTP() 方法 查找到 ServeMux，调用 ServeMux 的（2）相关方法找到函数执行体（也是一个 ServeHTTP() 方法）并执行。 这一下就提到三个 ServeHTTP() 方法，功能都是不一样的，第一个是用于查找 ServeMux，第二个是用于查找函数执行体，第三个是要执行的函数执行体，注意不要混淆了。  一言以蔽之，先保存路由和函数执行体映射（ServeMux 准备阶段）；开启监听和等待连接，接受连接请求并新开一个 goroutine（Server 等待连接阻塞，承上启下）；通过查找映射找到函数执行体（conn 执行阶段）。 其中的监听、等待连接、接受连接是 Go 语言的实现，底层调用了 C 的接口。\n1.1.2 细节剖析 上面的三个类型中其实就是一层一层包含，里面通过一些重点内核函数（如监听连接等）或者辅助函数（如 serverHandler 等）实现该层的功能或逻辑。 在调用 func ListenAndServe(addr string, handler Handler) 函数的的时候，第二个就是可以自定义 Handler，也可以传 nil 表示使用 http.DefaultServeMux 进行处理，而下面是就设置 http.DefaultServeMux 中的 m 属性的方式：\n//句柄pattern1 通过一个结构体 type Mux struct{} func (Mux) ServeHTTP(w http.ResponseWriter, r *http.Request) { fmt.Fprintln(w, \u0026quot;welcome to china\u0026quot;) } //句柄pattern2 通过一个函数 type HandlerFunc func(http.ResponseWriter, *http.Request) func (f HandlerFunc) ServeHTTP(w http.ResponseWriter, r *http.Request) { f(w, r) } //句柄pattern3 这其实和pattern2一样，只是调用的不是自定义的HandlerFunc，而是http.HandlerFunc func IndexHandler(w http.ResponseWriter, r *http.Request) { fmt.Fprintln(w, \u0026quot;hello world\u0026quot;) } func httpServe(){ http.Handle(\u0026quot;/pattern1\u0026quot;, new(Mux)) http.Handle(\u0026quot;/pattern2\u0026quot;, HandlerFunc(IndexHandler)) http.Handle(\u0026quot;pattern3\u0026quot;, http.HandlerFunc(IndexHandler)) http.HandleFunc(\u0026quot;/pattern4\u0026quot;, IndexHandler) http.ListenAndServe(\u0026quot;:8000\u0026quot;, nil) } 1.2 客户端 客户端的实现方式有很多种，下面我只实现两种。另外，理解 go 的 HTTP 最佳的方式是理解原代码，理解原代码的方式是进行服务端调试，要进行服务端的调度需要从客户端请求开始。\n1.2.1 基于 TCP 的 HTTP 客户端 tcpAddr, err := net.ResolveTCPAddr(\u0026quot;tcp4\u0026quot;, \u0026quot;localhost:8000\u0026quot;) conn, err := net.DialTCP(\u0026quot;tcp\u0026quot;, nil, tcpAddr) _, err = conn.Write([]byte(\u0026quot;GET /pattern2 HTTP/1.1\\r\\nHost: localhost:8000\\r\\n\\r\\n\u0026quot;)) _ = conn.CloseWrite() //发送 FIN result, err := ioutil.ReadAll(conn) 1.2.2 基于 http.Client 的 HTTP 客户端 req, _ := http.NewRequest(\u0026quot;GET\u0026quot;, \u0026quot;http://localhost:8000/pattern2\u0026quot;, nil) client := \u0026amp;http.Client{} resp, err := client.Do(req) result, err := ioutil.ReadAll(resp.Body) 2 RPC 2.1 概述 RPC 是远程过程调用，或者说是远程方法调用、远程函数调用，多了一个“远程”，区别于同一进程空间内的方法或者函数调用，服务调用方须以网络的形式进行远程调用，是服务化框架的核心之一。 TCP/IP 和 HTTP 我把他们定位为实现数据传输，是协议设计。 RPC 和 Restful 我把他们定位为在数据传输基础上提供服务，是框架设计。 Restful 是基于 HTTP 的，但 RPC 可以基于 HTTP，也可以基于 TCP/IP。换句话说，HTTP 是面向“事先没有沟通”的普通用户，Restful 一般是面向“事先有简单沟通”的程序员（有后端的，但更多是前端），相对来说还是存在通用性设计；而 RPC 是面向“事先有良好沟通”的程序员（后端），所以可以去掉一些通用设计而采用精准功能设计，从而提高效率。 打个比喻，Restful 或者 HTTP 犹如有界面操作系统，事先用户简单学习甚至完全不需要学习其使用方式，用户自己摸索就能使用它；RPC 犹如命令操作系统，直接提供命令给你操作，前提是你得来学习命令或者函数操作方式，去掉“中间商赚差价”，所以效率提高了。\nGo 语言标准包中已经提供了对 RPC 的支持，而且支持三个级别的 RPC：TCP(使用的标准包 net/rpc)、HTTP(使用的标准包 net/http、net/rpc)、JSONRPC(使用的标准包 net/rpc、net/rpc/jsonrpc)。但 Go 语言的 RPC 包是独一无二的 RPC，只支持 Go 语言开发的服务器与客户端之间的交互，因为在内部，它们采用了 Gob 来编码。所以如果是跨语言的，可以采用开源的 RPC，比如 gRPC，基于 HTTP，它采用 protocol 编码，支持很多语言，如 Java、Go、C++ 和 PHP 等。 下面我只讲解实现基于 TCP 的 Go 标准包 RPC（说明这个 RPC 是用 TCP 传输，且用了 Gob 编码），服务端和客户端都使用标准包 net/rpc。\n2.2 基于 TCP 的标准包 RPC ","id":26,"section":"posts","summary":"本篇主要内容是 Go HTTP 原理和 RPC 相关知识。 在进行原理讲解之前我觉得有必要熟悉一下 TCP 传输原理和 C Socket 编程，因为 Go 是在 C 的基础上进行封装的，这样对哪些属","tags":["Golang","网络","Socket","RPC","HTTP"],"title":"go 网络编程","uri":"https://blog.jemper.cn/2019/03/go-network/","year":"2019"},{"content":"一切皆文件，I/O 操作无处不在，文件、设备、管道、Socket等都是 I/O 操作。C 语言对文件 I/O 操作分两种，一种是无缓冲的（用户层无缓存区），返回的是文件描述符（int 整型），代表函数是 open、read、write 和 socket 等；另一种是有缓冲的（用户层设计了缓存区），返回是的数据流 Stream（FILE 结构体），代表函数是 fopen、fread、fwrite、putc、getc、fputs、fgets 和 fprintf等；不过 C 标准已经不再支持对文件（这里没有包括 socket）的无缓冲操作。\n“文件句柄”是 windows 中的术语，因为在 windows 中 socket 和文件是不一样的；而 Linux 不区分文件与 socket, 所以用“文件描述符”术语，其中0、1、2就是我们熟悉的标准输入、标准输出和标准错误，自定义的描述符从 3 开始由小到大顺序编号。 以下是在 linux 的输出：\n\u0026gt; lsof -p 4501 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME a.out 4501 feixin10 cwd DIR 8,1 101 52704 /home/feixin10 a.out 4501 feixin10 rtd DIR 8,1 224 96 / a.out 4501 feixin10 txt REG 8,1 13384 609203 /home/feixin10/a.out a.out 4501 feixin10 mem REG 8,1 2151672 25190121 /usr/lib64/libc-2.17.so a.out 4501 feixin10 mem REG 8,1 163400 25166793 /usr/lib64/ld-2.17.so a.out 4501 feixin10 0u CHR 136,0 0t0 3 /dev/pts/0 a.out 4501 feixin10 1u CHR 136,0 0t0 3 /dev/pts/0 a.out 4501 feixin10 2u CHR 136,0 0t0 3 /dev/pts/0 a.out 4501 feixin10 3u IPv4 34600 0t0 TCP *:8090 (LISTEN) a.out 4501 feixin10 4u IPv4 34601 0t0 TCP docker-1.c.suishou-01.internal:8090-\u0026gt;112.97.60.187:20530 (ESTABLISHED) 1 套接字 服务端和客户端都需要通过 socket() 函数建立套接字，了解套接字的结构可以通过下面的命令，*（macOS） 或者 0.0.0.0（Linux） 表示任意 IP：\nmacOS \u0026gt; netstat -an | grep tcp4 Proto Recv-Q Send-Q Local Address Foreign Address (state) tcp4 628000 0 127.0.0.1.8090 127.0.0.1.54044 ESTABLISHED tcp4 0 10000 127.0.0.1.54044 127.0.0.1.8090 ESTABLISHED tcp4 0 0 *.8090 *.* LISTEN Linux \u0026gt; netstat -an | grep tcp4 tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN tcp6 0 0 :::22 :::* LISTEN tcp 0 0 0.0.0.0:8090 0.0.0.0:* LISTEN tcp 0 0 10.170.0.4:22 112.97.60.187:20524 ESTABLISHED tcp 0 0 10.170.0.4:22 112.97.60.187:20526 ESTABLISHED tcp 176656 0 10.170.0.4:8090 112.97.60.187:20530 ESTABLISHED 上面前三行输出内容中，LISTEN 是服务器监听 socket，第一个 ESTABLISHED 是服务器的连接 socket，第二是 ESTABLISHED 是客户端的连接 socket。根据上面列的信息，其实就可以知道套接字结构的主要属性： {% img http://img.jemper.cn/2019/03/%E5%A5%97%E6%8E%A5%E5%AD%97%E7%BB%93%E6%9E%84.png 300 %} 套接字结构\n接收（Recv-Q）和发送（Send-Q）的缓冲区大小由系统默认设置，也可以调用 setsockopt() getsockopt() 进行设置，两个缓冲区都是 FIFO（先进先出），主动关闭套接字后该套接字将只能 read，不能 send。 数据流虽然是数据报形式发送，到存放到缓冲区后就没有数据报概念了，因此只能读取到没有数据为止，不能假设写到连接一端的数据大小与从连接另一端读取的数据大小之间存在任何一致性，换句话说，在发送端通过调用一次 send() 传入的数据可以通过在另一端调用 recv() 多次来获取；而调用 recv() 一次可能返回调用 send() 多次所传入的数据。 Local Address 和 Foreign Address 也没什么虽然特别说的。 state 这个却需要深入理解，这部分可以参考《TCP 传输原理》一文。\n需要注意的是，一个连接的 socket 往往不是直接对外的，而是经路由（网络层）进行内外网转换： 2 Socket 编程 2.1 TCP 2.1.1 服务端 接下来我们来看几个核心的函数，需要注意返回值。我以打电话来勾勒其轮廓。\n 电话机：int socket(int domain, int type, int protocal); 成功时返回文件描述符，失败时返回 -1。从参数上看，就可以知道，domain 是选择网络层协议（IPv4、IPv6 等），type 和 protocol 是选择传输层协议（TCP、UDP 等），所以就知道 TCP/IP 这两个是整个网络协议集合里面最重要的要素，即“协议”，选择了这两个，其它的协议也就随之确定了，比如选择了 TCP 就会有窗口协议等。 分配自己的电话号码：int bind(int sockfd, struct sockaddr *myaddr, socklen_t addrlen); 成功时返回 0，失败时返回 -1。主要是设置端口，IP 地址一般自动获取。这一步是 socket 另外两个要素，IP 地址和端口。 等待别人来电：int listen(int sockfd, int backlog); 成功时返回0，失败时返回 -1。backlog 指定监听套接字的完成连接队列的最大长度。 (1)未完成连接队列：未完成 3 次握手，如果完成连接队列已满，将忽略客户机新发来的 SYN，而不发 RST，原因参考《TCP 传输原理》； (2)完成连接队列：已完成 3 次握手，但未被应用程序的 accept 接受； 别人打来电话，接听：int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen); 成功时返回文件描述符，失败时返回 -1。这一步生成了新的 socket，这个新的 socket 状态马上变成 ESTABLISHED，而参数里的 sockfd 还是原来的 socket，继续保持监听。  2.1.2 客户端 还是以打电话来比喻。\n 电话机：int socket(int domain, int type, int protocal); 这个和上面没任何区别。 打电话：int connect(int sockfd, struct sockaddr *serv_addr, socklen_t addrlen); 成功是返回 0，失败时返回 -1。客户端不用设置自己的密码。  2.1.3 数据传输  ssize_t send(int sockfd, const void *buf, size_t nbytes, int flags); 成功返回发送字节数，失败返回 -1，并置相应的 errno；注意要对返回的字节数进行验证。nbytes：要传输的数据字节数，flags：传输数据时指定的可选项信息。 ssize_t recv(ind sockfd, const void *buf, size_t nbytes, int flags); 成功返回接收字节数，失败返回 -1，如果通信对端正常关闭，则返回 0；nbytes：要传输的数据字节数，flags：接收数据时指定的可选项信息。  这两个比 write 和 read 多了 flags 选项。可选项可查阅资料。\n{% img http://img.jemper.cn/2019/03/TCP%E5%A5%97%E6%8E%A5%E5%AD%97%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png 300 %} TCP 套接字的工作流程\n参考 C 语言代码实现： TCP 服务端 C 语言实现 TCP 客户端 C 语言实现\n2.2 UDP UDP 不需要连接，bind socket 可以参照 TCP 部分。\n2.2.1 数据传输  ssize_t sendto(int socket, void *restrict buffer, size_t length, int flags, const struct sockaddr * dest_addr, socklen_t dest_len); 成功返回发送的字节数，失败返回 -1； ssize_t recvfrom(int socket, const void *message, size_t length, int flags, struct sockaddr *restrict address, socklen_t *restrict address_len); 成功返回发送的字节数，失败返回 -1；并置相应的 errno；  {% img http://img.jemper.cn/2019/03/UDP%E5%A5%97%E6%8E%A5%E5%AD%97%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png 300 %} UDP 套接字的工作流程\n参考 C 语言代码实现： UDP 服务端 C 语言实现 UDP 客户端 C 语言实现\n2.3 linux 文件操作  ssize_t write(int fd, const void * buf, size_t nbytes); 成功返回写入的字节数，失败时返回 -1；nbytes：要传输的数据字节数。 ssize_t read(int fd, void * buf, size_t nbytes); 成功时返回接收到的字节数（但遇到文件结尾则返回0），失败时返回 -1。nbytes：要接收数据的最大字节数。  以 _t 为后缀的数据类型都是元数据类型，在 sys/types.h 头文件中一般由 typedef 声明定义，算是给大家熟悉的基本数据类型起别名。原因是系统和计算机位数从 16、32 到 64 位升级，为让内置函数的操作和存储能力也同步提升，同时避免在开发者层面上对应改动，内置函数和开发者都使用 size_t（unsigned int）、ssize_t（signed size_t 或 signed int）类型，这样随着系统的提升，只要修改并编译 size_t、ssize_t 的声明即可，整体上内置函数提供更大的数值存储，而这一些对开发者而言是不需要任何改动的。\n2.4 Socket 选项  int setsockopt(int sockfd, int level, int optname, const void *optval, socklen_t optlen); 成功返回 0，失败返回 -1，并给 errno 设置对应的错误值。 SO_SNDBUF：设置发送缓冲区的大小，2048 \u0026lt;= val \u0026lt;= 256*(sizeof(struct sk_buff)+256)。该操作将 sock.sk.sk_sndbuf 设置为 2*val，防止因大数据量的发送突然导致缓冲区溢出； SO_RCVBUF：设置接收缓存区的大小，256 \u0026lt;= val \u0026lt;= 256*(sizeof(struct sk_buff)+256)。该操作将 sock.sk.sk_rcvbuf 设置为 val； SO_REUSEADDR：当 Server 端重启或者崩溃的时候，它就是主动关闭的一方，会进入 TIME_WAIT 状态，导致服务器不能立刻重启，该选项就是为了解决这个问题的，在调用 bind 前设置就可以了。 SO_KEEPALIVE：keep-alive int getsockopt(int sockfd, int level, int optname, void *optval, socklen_t optlen); 成功返回 0，失败返回 -1，并给 errno 设置对应的错误值。  3 并发服务器 简单的循环服务器同一时刻只可以响应一个客户端的请求，只到一个客户端请求结束才开始接受下一个客户，并不实用，接下来讲几种并发模型。\n3.1 多进程 缺点是需要IPC通信和分时使用 CPU 产生的上下文切换（不同进程切换导致相关信息移出或者移入内存或 CPU 寄存器）开销。\n3.2 多线程 解决了多进程两个问题，但带来了同步问题，不过现在同步问题已经很多的解决方案，特别是 Go 更是在解决同步这一问题下足了功夫。 int pthread_create(pthread_t * restrict thread, const pthread_attr_t * restrict attr, void * (* start_routine)(void *), void * restrict arg); 成功时返回 0，失败时返回其它值。 int pthread_join(pthread_t thread, void ** status); 成功时返回 0，失败时返回其它值。让调用函数的线程进入阻塞状态。 int pthread_detach(pthread_t thread); 成功时返回 0，失败时返回其他值。调用该函数不会引起阻塞，可以通过该函数引导销毁线程创建的内存空间。\n3.3 I/O 复用 不要过于依赖该模型，该方案并不适用于所有情况，应当根据目标服务器的特点采用不同实现方法。 复用：为了提高物理设备的效率，用最少的物理要素传递最多数据时使用的技术。有“时（time）分复用技术”和“频（frequency）分复用技术”。 select() 函数是最具代表性的实现复用服务器的方法，它将多个文件描述符集中到一起统一监视。 int select(int maxfd, fd_set * readset, fd_set * writeset, fd_set * exceptset, const struct timeval * timeout); 发生错误时返回 -1，超时返回时返回 0；因发生关注的事件返回时，返回大于 0 的值，该值是发生事件的文件描述符数。 (1) 文件描述符监视范围 maxfd = 最大的文件描述符值 + 1，加 1 是因为文件描述符的值从 0 开始。 (2) 对 fd_set 位变量的注册或者更改值的操作由宏完成，除了下面的宏，还有 FD_ISSET(int fd, fd_set *fdset) 用于验证 select 函数的调用结果。 (3) select 函数调用后，fd_set 位值仍为 1 的位置上的文件描述符即是发生了变化的，监听但是没有变化的位会从 1 改变为 0，正因为 readset、writeset、exceptset 这三个集合在调用 select 函数后会发生变化，因此为了记住初始化，必须先复制保存起来以便后续继续监听；包括超时时间也需要先复制初始值，因为调用 select 后会更新为剩余时间。\n4 字节流处理 TCP 协议底层操作的只是字节流，那么对应用程序而言，我们必须依据自己的应用定制私有的协议。如同设计新的应用层协议，常见的应用层协议如 RPC、HTTP/1.1、HTTP/2 都是协议化来规划数据包的大小和属性。 以 HTTP/2 为例进行分析： 缓冲区 TCP 报文段数据是指去掉 TCP 首部后的数据字节流，数据字节流是连续的，没有边界的，承载的是 HTTP/2 帧数据片段，且一个 TCP 报文段数据和一个 HTTP/2 帧是没有对应关系的，可能一个 TCP 报文段数据包含多个 HTTP/2 帧，也可能多个 TCP 报文段数据承载一个 HTTP/2 帧。如果把 TCP 报文段数据整体来看，就是多个 HTTP/2 帧首尾相连。因为帧是协议化的，所以 HTTP/2 才能够识别帧，并取出负载数据，并按流 ID 组成各个请求响应。 总之，TCP 层接收到数据去掉首部信息后才放入缓冲区，所以缓冲区仅仅是应用层的数据，应用层协议最重要的就是把字节流协议化，以便能从字节流中识别出来数据。\n参考文献 [1] 程国钢.等. Linux C 编程从基础到实践 [2] 尹圣雨（韩）. TCP/IP网络编程. 版次：2014年7月第1版 [3] 吕雪峰.等. 嵌入式 Linux 软件开发——从入门到精通. 版次：2014年9月第1版\n","id":27,"section":"posts","summary":"一切皆文件，I/O 操作无处不在，文件、设备、管道、Socket等都是 I/O 操作。C 语言对文件 I/O 操作分两种，一种是无缓冲的（用户层无缓存区），返回","tags":["C","Socket","编程","io"],"title":"C Socket 编程","uri":"https://blog.jemper.cn/2019/03/c-socket/","year":"2019"},{"content":"TCP的主要目的是在一个比较不可靠的相互通信上提供可靠的，安全的逻辑回路或者连接服务。为了实现这个服务，TCP 的设计 体系非常庞大，我对其理解只是冰山一角，此文只是将我学过的重点知识进行汇总。\n1 连接 国家规定新建小区必须设置信箱，这无形之中就让家家户户都建立起了连接，你想要给任何一家发信件都是可以的，总之连接是抽象的，而信箱是具体的。对应到网络，socket 就是信箱；网络连接是无形的，而 sockets 对各自的机器是具体的数据结构。每个连接被一套指定两端的 sockets 唯一指定，建立连接其实就是双方建立绑定的 socket 的过程，断开连接就是销毁各自的 socket 的过程。当然上面的 socket 是被 accept 的连接 socket 而非主动或被动（倾听）的 socket，因为像 UDP 是主动 socket，是不需要连接的。\n1.1 三次握手  套接字 程序在创建 socket 并 bind 后成为主动套接字，可以用它来进行主动连接，但不能接收连接；进行 listen 时转变为被动 socket（倾听套接字）。 倾听套接字维护两个队列：   未完成连接队列：每个未完成 3 次握手操作的 TCP 连接在这个队列中占有一项，当完成连接将移出该队列，到完成连接队列中； 完成连接队列：每个已经完成 3 次握手操作，但尚未被应用程序接收（调用函数 accept）的 TCP 连接在这个队列中占有一项；当被 accept 接收的时候，将移出该队列，到新建的 socket 中；  标识   ACK：标识表明确认号是否合法，为 0 表示数据报确认号是无效数据。 PSH：接收方因此请求数据报一到便可送往应用程序而不必等到缓冲区满时才传送。 RST：用于复位由于主机崩溃或其它原因而出现的错误的连接。还可以用于拒绝非法的数据报或者拒绝连接请求。  选项 可选项大小 0 - 32 byte。下面这些选项一般只在握手阶段设备。如果是抓包分析，错过了握手阶段的包，抓包工具就不知道该如何计算，所以我们有时候会很莫名地看到一些极小的接收窗口值。   MSS（0x2）: Max Segment Size 是 TCP 报文段里数据的最大长度（不包括 TCP 首部），握手中两端之间选择一个较小的值投入使用,一般是 1460B。原因是 MTU（Max Transmission Unit），由于以太网传输电气方面的限制，每个以太网帧最小为 64B，最大为 1518B，而 Ethernet II 帧最长占用 18B，剩下的 1500B 称为 MTU，它相当于 MSS + TCP 包头（约 20 字节）+IP 包头（20 字节）= 1500 字节；网络层协议如 IP 协议会根据 MTU 这个值来决定是否把上层传下来的数据进行分片。当然网络中不媒介的 MTU 是不同的，本地甚至可以达上万字节，大部分网络 MTU 大于等于 1500。 Window Scale (0x3): 窗口比例选项使 TCP 的窗口定义从 16bit(64KB) 增加到 30bit(1GB)，通过这个值计算窗口大小值的乘数 Multiplier，比如 Shift count（取值 0 - 14） 值为 6，表示 Multiplier = 2^6。窗口大小等于 Window(TCP 首部) * Multiplier(选项)，单位是 octets。TCP 刚被发明的时候，全世界网络带宽很小，所以最大接收窗口被定义为 64KB，随着硬件的革命性进步，64KB 已经成为瓶颈，只能进行扩展。 另外要注意，客户端和服务端的 Window Scale 是各自设置运算互不干扰；这点和 MSS 取两端最小值不同。  1.2 四次挥手   四次挥手比三次握手多一次的原因就是有一个半关闭状态，关闭的一方只能接收，不能再发送。\n  当TCP连接中途突然断掉，使用RST标志位指出连接被异常中止或拒绝连接请求，但是有一点例外，即客户机向服务机发送SYN时，因为服务机倾听套接字backlog已完成连接队列已满时，不会发送RST，有两个原因：\n   完成队列满的情况是暂时的，客户机应当继续发送几次SYN数据段； 即使发送了客户机也不知道具体是什么原因，只能认为服务机出异常而停止握手；  以下情况会发送RST：\n 客户端尝试与服务器未对外提供服务的端口建立TCP连接，服务器将会直接向客户端发送reset报文； 客户端和服务器的某一方在交互的过程中发生异常（如程序崩溃等），该方系统将向对端发送TCP reset报文，告之对方释放相关的TCP连接； 接收端收到TCP报文，但是发现该TCP的报文，并不在其已建立的TCP连接列表内，则其直接向对端发送reset报文； 在交互的双方中的某一方长期未收到来自对方的确认报文，则其在超出一定的重传次数或时间后，会主动向对端发送reset报文释放该TCP连接； 有些应用开发者在设计应用系统时，会利用reset报文快速释放已经完成数据交互的TCP连接，以提高业务交互的效率。  2 确认机制 ACK TCP数据包中的序列号（Sequence Number）不是以报文段来进行编号的，而是将连接生存周期内传输的所有数据当作一个字节流，序列号就是整个字节 流中每个字节的编号。一个TCP数据包中包含多个字节流的数据（即数据段），而且每个TCP数据包中的数据大小不一定相同。在建立TCP连接的三次握手 过程中，通信双方各自已确定了初始的序号x和y，TCP每次传送的报文段中的序号字段值表示所要传送本报文中的第一个字节的序号。\nTCP的报文到达确认（ACK），是对接收到的数据的最高序列号的确认，并向发送端返回一个下次接收时期望的TCP数据包的序列号（Ack Number）。例如， 主机A发送的当前数据序号是400，数据长度是100，则接收端收到后会返回一个确认号是501的确认号给主机A。\nTCP提供的确认机制，可以在通信过程中可以不对每一个TCP数据包发出单独的确认包（Delayed ACK机制），而是在传送数据时，顺便把确认信息传出， 这样可以大大提高网络的利用率和传输效率。同时，TCP的确认机制，也可以一次确认多个数据报，例如，接收方收到了201，301，401的数据报，则只 需要对401的数据包进行确认即可，对401的数据包的确认也意味着401之前的所有数据包都已经确认，这样也可以提高系统的效率。\n如果收到乱序的数据包，那么就应该立即发送 ACK，以确保对方知道重复的确认(DUPACK)，然后及时重传。\n若发送方在规定时间内没有收到接收方的确认信息，就要将未被确认的数据包重新发送。接收方如果收到一个有差错的报文，则丢弃此报文，并不向发送方 发送确认信息。因此，TCP报文的重传机制是由设置的超时定时器来决定的，在定时的时间内没有收到确认信息，则进行重传。这个定时的时间值的设定非 常重要，太大会使包重传的延时比较大，太小则可能没有来得及收到对方的确认包发送方就再次重传，会使网络陷入无休止的重传过程中。接收方如果收到 了重复的报文，将会丢弃重复的报文，但是必须发回确认信息，否则对方会再次发送。\nTCP协议应当保证数据报按序到达接收方。如果接收方收到的数据报文没有错误，只是未按序号，这种现象如何处理呢？TCP协议本身没有规定，而是由TCP 协议的实现者自己去确定。通常有两种方法进行处理：一是对没有按序号到达的报文直接丢弃，二是将未按序号到达的数据包先放于缓冲区内，等待它前面 的序号包到达后，再将它交给应用进程。后一种方法将会提高系统的效率。例如，发送方连续发送了每个报文中100个字节的TCP数据报，其序号分别是1， 101，201，…,701。假如其它7个数据报都收到了，而201这个数据报没有收到，则接收端应当对1和101这两个数据报进行确认，并将数据递交给相关的应用 进程，301至701这5个数据报则应当放于缓冲区，等到201这个数据报到达后，然后按序将201至701这些数据报递交给相关应用进程，并对701数据报进行 确认，确保了应用进程级的TCP数据的按序到达。\n3 流量控制 Flow Control 流量控制方法主要有两种：滑动窗口控制和速率控制；TCP 使用前者作为流量控制方式，并且是可变长的滑动窗口。滑动窗口协议定义了在缓存上的窗口，即窗口大小利用缓冲区实现的，但两个是不同的概念。\nTCP数据包的TCP头部有一个window字段，它主要是用来告诉对方自己能接收多大的数据（注意只有TCP包中的数据部分占用这个空间），这个字段在通信双方建立连接时协商确定，并且在通信过程中不断更新，故取名为滑动窗口。有了这个字段，数据发送方就知道自己该不该发送数据，以及该发多少数据了。发送方的发送速率高于接收端的接收速率，就需要接收方通过滑动窗口控制发送方的发送速率，保证通信双方的接收缓冲区不会溢出，数据不会丢失。\n由于窗口大小在TCP头部只有16位来表示，所以它的最大值是65536，但是对于一些情况来说需要使用更大的滑动窗口，这时候就要使用扩展的滑动窗口，如光纤高速通信网络，或者是卫星长连接网络，需要窗口尽可能的大。这时会使用扩展的32位的滑动窗口大小。参见选项 Window Scale。\n3.1 滑动窗口移动规则   窗口合拢：在收到对端数据后，自己确认了数据的正确性，这些数据会被存储到接收缓冲区，等待应用程序获取。但这时候因为已经确认了数据的正确性，需要向对方发送确认响应ACK，又因为这些数据还没有被应用进程取走，这时候便需要进行窗口合拢，缓冲区的窗口左边缘向右滑动。注意响应的ACK序号是对方发送数据包的序号，一个对方发送的序号，可能因为窗口张开会被响应（ACK）多次。\n  窗口张开：窗口收缩后，应用进程一旦从缓冲区(滑动窗口区或接收缓冲区)中取出数据，TCP的滑动窗口需要进行扩张，这时候窗口的右边缘向右扩张，实际上窗口这是一个环形缓冲区，窗口的右边缘扩张会使用原来被应用进程取走内容的缓冲区。在窗口进行扩张后，需要使用ACK通知对端，这时候ACK的序号依然是上次确认收到包的序号。\n  窗口收缩，窗口的右边缘向左滑动，称为窗口收缩，HostRequirement RFC强烈建议不要这样做，但TCP必须能够在某一端产生这种情况时进行处理。\n  3.2 send 行为 默认情况下，send的功能是拷贝指定长度的数据到发送缓冲区，只有当数据被全部拷贝完成后函数才会正确返回，否则进入阻塞状态或等待超时。如果你想修改这种默认行为，将数据直接发送到目标机器，可以将发送缓冲区大小设为0（或通过TCP_NODELAY禁用Nagle算法），这样当send返回时，就表示数据已经正确的、完整的到达了目标机器。注意，这里只表示数据到达目标机器网络缓冲区，并不表示数据已经被对方应用层接收了。\n协议层在数据发送过程中，根据对方的滑动窗口，再结合MSS值共同确定TCP报文中数据段的长度，以确保对方接收缓冲区不会溢出。当本方发送缓冲区尚有数据没有发送，而对方滑动窗口已经为0时，协议层将启动探测机制，即每隔一段时间向对方发送一个字节的数据，时间间隔会从刚开始的30s调整为1分钟，最后稳定在2分钟。这个探测机制不仅可以检测到对方滑动窗口是否变化，同时也可以发现对方是否有异常退出的情况。\npush标志指示接收端应尽快将数据提交给应用层。如果send函数提交的待发送数据量较小，例如小于1460B（参照MSS值确定），那么协议层会将该报文中的TCP头部的push字段置为1；如果待发送的数据量较大，需要拆成多个数据段发送时，协议层只会将最后一个分段报文的TCP头部的push字段置1。\n3.3 recv 行为 默认情况下，recv的功能是从接收缓冲区读取(其实就是拷贝)指定长度的数据。如果将接收缓冲区大小设为0，recv将直接从协议缓冲区(滑动窗口区)读取数据，避免了数据从协议缓冲区到接收缓冲区的拷贝。recv返回的条件有两种：\n recv函数传入的应用层接收缓冲区已经读满 协议层接收到push字段为1的TCP报文，此时recv返回值为实际接收的数据长度  协议层收到TCP数据包后(保存在滑动窗口区)，本方的滑动窗口合拢（窗口值减小）；当协议层将数据拷贝到接收缓冲区(滑动窗口区—\u0026gt;接收缓冲区)，或者应用层调用recv接收数据(接收缓冲区—\u0026gt;应用层缓冲区，滑动窗口区—\u0026gt;应用层缓冲区)后，本方的滑动窗口张开(窗口值增大)。收到数据更新window后，协议层向对方发送ACK确认。\n协议层的数据接收动作完全由发送动作驱动，是一个被动行为。在应用层没有任何干涉行为的情况下（比如recv操作等），协议层能够接收并保存的最大数据大小是窗口大小与接收缓冲区大小之和。Windows系统的窗口大小默认是64K，接收缓冲区默认为8K，所以默认情况下协议层最多能够被动接收并保存72K的数据。\n4 拥塞控制 Congestion Control 发送方一开始便向网络发送多个报文段，直至达到接收方通告的窗口大小为止，这种方式在局域网是可以的；但是在互联网上链路层复杂，三次握手后，设备无法知道目前的网络状况，如果发送过多的网络包，可能会导致整个网络阻塞且数据被丢弃。所以需要一些机制在面临拥塞时遏制发送方。 发送方始终保持两个窗口：接收方确认窗口和拥塞窗口；取两个窗口的最小值作为可以发送的字节数：实际发送窗口 = min(cwnd, rwnd)；当 cwnd \u0026gt; rwnd 的时候是对方的接收能力限制了我的发送速度，而当 rwnd \u0026gt; cwnd 的时候是网络情况造成了发送比较慢。\n假设高速链路带宽是 10Gbps，分组大小为 1500 字节（byte）,回路延时 RTT 为 100ms，则发送端达到 10Gbps吞吐量时，发送端拥塞窗口大小为 83 333 个分组。\n4.1 慢启动算法 TCP 发送端在初始段的发送速率以指数型增长，直至发生超时或者收到冗余的确认报文时，或者达到接收端窗口大小时停止增长。 每个 TCP 连接完成的时候会设置拥塞窗口（cwnd）的大小，拥塞窗口默认大小是 10 个 MSS，这是相对保守的一个值，也就是说完成 TCP 连接后，发送方第一次发送的数据量最多是 10 个 MSS。一旦发送方在定时器超时之前接收到接收方发送的 ACK 包，拥塞窗口大小就增加一倍。当拥塞是 N 个数据报的大小时，如果发送 N 个数据报都被及时确认，那么将拥塞窗口大小增加 N 个数据报对应的字节数目。也就是发送的数据包数量保持指数增长。直到数据传输超时或者达到接收方设定的窗口大小。拥塞窗口便设置为恰好不造成超时或达到接收方的窗口大小的字节数。拥塞窗口这种处理机制称为慢启动，实际上慢启动并不慢。\n慢启动的优点是在比较拥塞的网络，慢启动可以避免拥塞进一步加剧，但是它的缺点也是明显的，对于正常的网络，慢启动将降低传输的效率，例如本来一个 RTT 就可以传完的数据，现在要分成几个 RTT；比如 Linux 2 的 initcwnd 只有 3MSS，如果有 7MSS 数据要发送就不得不用 3RTT；如果把 initcwnd 改成 10，则 7MSS 并行发送，只需要 1RTT。 修改某个网卡的 cwnd sudo ip route change default via 127.0.0.1 dev eth0 proto static initcwnd 10； 可以通过命令查看 ss -nli | fgrep cwnd 查看拥塞窗口初始化数据报的数量； 关于 initcwnd 可以查看 Tuning initcwnd for optimum performance\n4.2 拥塞避免  拥塞避免：该算法主要用来将 TCP 连接的数据传输速率维持在一个比较接近网络带宽上限，但又不至于引起严重拥塞的水平上。慢启动的过程中，拥塞窗口会以指数倍增长，一直增长到拥塞阈值 ssthresh，然后再以线性递增的方式增加拥塞窗口，这个阶段叫拥塞避免。也就是说当 cwnd \u0026lt; ssthresh 时是慢启动的过程，而当 cwnd \u0026gt; ssthresh 时是拥塞避免。一直增长到合适的带宽大小。  4.3 超时重传 4.4 快速重传和快速恢复 4.5 Nagle 算法 5 定时器管理 Timer 为了实现 TCP，对每个连接 TCP 管理 4 个不同的定时器。\n5.1 重传定时器 Retransmession TCP 提供可靠的传输所使用的方法之一是确认和重传。重传定时器是用于处理重传时间的，TCP 每发送一个报文段后就启动重传定时器；如果超时前收到确认报文，则报文重传定时器被停止；否则该报文将被重传，且定时器被复位。 TCP 采用了一种动态重传时间策略，它能根据往返时间（RTT），不断地调整修正重传时间。RTT 是 TCP 维护的变量，每次进行测量的时候，RTT 将得到修正。\n5.2 持续定时器 Persistance 其作用有两个：\n 零窗口死锁 探测报文段  当接收端向发送端发送一个零窗口报文段（报文段首部中窗口值的大小设置为0），发送端就停止向接收端发送报文段。后来接收端想通知发送端，让其接着发送数据时，便向发送端发送一个非零窗口报文段。该报文段在路上丢失了。而接收端以为该非零报文段已经发送给发送端，而发送端由于没有收到接收方的非零窗口报文段，于是两端都等待，陷入“死锁”的状态。\nTCP为发送端设置一个持续计时器，当发送端收到零窗口报文段时，启动该持续计时器，便等待着接收方的非零通知。持续计时器又超时时还没收到，发送端便向接收方发送一个“探测报文段”，该探测报文仅携带1B的数据，该探测报文会消耗一个序号，但特殊的是，该探测报文段的序号永远不需要确认。目的是促使 TCP 接收端重传一个确认，该确认的内容包括接收端希望发送端的发送窗口的大小作为回复，即:如果接收端希望发送端的窗口大小仍然是0，重置续计时器；如果不是0，则该僵局打破了。\nProbe ACK 只是对 Probe 的应答，而不是确认序列号，接收端对探测报文段的响应是必须重传确认报文段，并将窗口大小告诉发送方，也就是窗口大小不为零时对探测报文段的序号进行确认。 注意：即便发送发收到了零窗口设置的报文段，发送端也能接收两种报文段：一个字节数据的探测报文段和携带紧急数据的报文段。\n我们查看 Wireshark 看探测报文的“专家信息”，其实就是 Probe 和 Probe ACK 两个包在循环:\n TCP Zero Window Probe：探测报文段，报文段数据为 1； ACK to a TCP Zero Window Probe：对 Probe 的应答，但不确认序列号； \u0026hellip;\u0026hellip;  5.3 保活定时器 keep-alive 保活功能可以保护长连接，也可以让服务端能检测到半开放的连接。这个要注意区别持续定时器，触发启动的方式是不一样的。\n TCP keep-alive segment：报文段数据为 0； ACK to a TCP keep-alive segment：保活应答； \u0026hellip;\u0026hellip;  5.4 时间等待定时器 time-wait 主动关闭方在关闭连接后处于 TIME_WAIT 状态，启用该定时器。本质原因是 TCP 需要兼顾对服务端和客户端开启和结束地处理，换句话说就是不能只顾一方关闭而不管另一方是否关闭。 1）为了保证A发送的最后一个ACK报文能够到达B。这个ACK报文段有可能丢失，因而使处在LAST-ACK状态的B收不到对已发送的FIN+ACK报文段的确认。B会超时重传这个FIN+ACK报文段，而A就能在2MSL时间内收到这个重传的FIN+ACK报文段。如果A在TIME-WAIT状态不等待一段时间，而是在发送完ACK报文段后就立即释放连接，就无法收到B重传的FIN+ACK报文段，因而也不会再发送一次确认报文段。这样，B就无法按照正常的步骤进入CLOSED状态。 2）A在发送完ACK报文段后，再经过2MSL时间，就可以使本连接持续的时间所产生的所有报文段都从网络中消失。这样就可以使下一个新的连接中不会出现这种旧的连接请求的报文段。\n正常在 TIME_WAIT 情况下不能再创建 ip:port 相同的 socket，原因就是上面分析的旧的 socket 滞留的报文段可能给新的 socket 接收了而造成干扰；不过这就导致 2MSL(一般30秒)不能马上重启服务，可以指定 SO_REUSEADDR。\n6 设置和调试 6.1 系统参数 linux 和 unix 系统都是使用 sysctl -A 进行查看，有关 tcp 的配置可以使用命令 sysctl -A | grep net.inet.tcp 查看，以下例举一些常用的参数，后台的值是 macOS 系统的默认值。\n net.inet.tcp.msl: 15000；此时 2msl = 30 秒； net.inet.tcp.keepidle: 7200000；若连接空间2h时间后，保活定时器启动发送探测报文段来查看对方是否还存在； net.inet.tcp.keepintvl: 75000；间隔 75s 发送探测报文 net.inet.tcp.keepinit: 75000； net.inet.tcp.keepcnt: 8；探测报文一共发送的次数 net.inet.tcp.always_keepalive: 0；  6.2 调试 学习的时候可以自己写服务端和客户端进行，抓包工具可以用 wireshark，内网可以安装插件 npcap。不过建议把服务端部署在外网，这样才能正常模拟网络传输；局域网内抓包，数据干扰因素太多，比如 MSS 设置成了夸张的 16344！TCP Zero Window Probe 那一个字节竟然会被确认！还多出了 TCP Previous segment not captured、TCP ACKed unseen segment 等专家信息提示，很容易逻辑混淆。\n下面是我用 C 语言实现的客户端和服务端，并把服务端分别部署在外网和本地进行抓包： tcp 服务端 C 语言实现、tcp 客户端 C 语言实现 wireshark 外网抓包分析、wireshark 本地抓包分析\n变更窗口由算法运算，通过 socket 状态可以看一下何时变更窗口；处于发送探测包的时候 socket 形式如下：\nProto Recv-Q Send-Q Local Address Foreign Address (state) tcp4 560949 0 127.0.0.1.8090 127.0.0.1.64196 ESTABLISHED tcp4 0 22243 127.0.0.1.64196 127.0.0.1.8090 FIN_WAIT_1 8090 端是读取，64196 端是发送，当 8090 端的接收窗口为零，持续定时器被启用，第一行的值一直在减少（读取），第二行的值保持不变。 当 8090 端剩下 131404 字节时更新窗口大小为 26220 字节，于是马上变成：\nProto Recv-Q Send-Q Local Address Foreign Address (state) tcp4 152384 0 127.0.0.1.8090 127.0.0.1.64196 CLOSE_WAIT tcp4 0 0 127.0.0.1.64196 127.0.0.1.8090 FIN_WAIT_2 除此之外，本系列 HTTPS 一文有提供更多的抓包调试工具。\n参考文献 [1] 杨延双.等. TCP/IP 协议分析及应用. 版次：2010年1月第1版 [2] 忆常. TCP报文到达确认（ACK）机制. https://blog.csdn.net/wjtxt/article/details/6606022 [3] 徐永士.等. 大数据时代下的通信需求——TCP传输原理与优化. 版次：2015年8月第1版 [4] 李银城. 高效前端 Web高效编程与优化实践. 版次：2018年3月第1版 [5] TCP 性能优化详解. https://www.zhuxiaodong.net/2018/tcp-performance-optimize-instruction/ [6] RFC793 TCP标准 最初的TCP标准定义，但不包括TCP相关操作细节 RFC813 TCP窗口与确认策略 讨论窗口确认机制，以及描述了在使用该机制存在的问题及解决方法 RFC879 TCP最大分段大小及相关主题 讨论MSS参数在控制TCP分组大小的重要性，以及该参数与IP分段大小的关系等 RFC896 IP/TCP网络互联拥塞控制 探讨拥塞问题与TCP如何控制拥塞 RFC2525 已知TCP的问题 描述当前已知的部分TCP问题 RFC2581 TCP拥塞控制 描述用于拥塞控制的四种机制：慢启动、拥塞防御、快重传和快恢复 RFC2988 TCP重传计时器计算 讨论与TCP重传计时器设置相关话题，重传计时器控制报文在重传前应等待多长时间\n","id":28,"section":"posts","summary":"TCP的主要目的是在一个比较不可靠的相互通信上提供可靠的，安全的逻辑回路或者连接服务。为了实现这个服务，TCP 的设计 体系非常庞大，我对其理解","tags":["TCP","协议"],"title":"TCP 传输原理","uri":"https://blog.jemper.cn/2019/03/tcp-protocol/","year":"2019"},{"content":"整个 context 包原码就有一个文件即 context.go，我估计 context 的设计就是由 http 引出来的，学习 context 其实也是学习 go 的接口设计范式。\n1.1 Context 接口 type Context interface { // 取消树和过期时间的实现 Deadline() (deadline time.Time, ok bool) // 过期时间点 Done() \u0026lt;-chan struct{} // 过期或者手动取消时关闭通道 Err() error // 可以判断是过期还是手动取消 // 键值对的实现，valueCtx类型真正实现 Value(key interface{}) interface{} } 从这点可以看出，Context 库设计的目的： (1) 退出通知机制 (2) 传递数据\n1.2 四个重要的类型  emptyCtx 类型：无实际作用的 int 型，这是最原始的 Context 接口实现； cancelCtx 类型：Context + 取消树（这里的“取消树”是一个完整的名词，“取消树”上的键是 canceler 类型，说明它可能是 cancelCtx，也可能是 timerCtx）； timerCtx 类型：cancelCtx + 过期时间；在上下级的超时时间中，下级超时时间一定要比上级早，否则即使用 WithDeadline() 创造出来的也是 cancelCtx，这还是很容易理解的。有一个 timer 属性，是用于提前取消定时，即调用 Stop() 方法； valueCtx 类型：Context + 键值对。  实例化上述四个类型的API函数：\n Background()、TODO()：构造 emptyCtx； WithCancel()：构造 cancelCtx； WithDeadline()、WithTimeout()：构造 timerCtx；一个是时间点，一个是时间段，时间段会转化为时间点；定时用了 time.AfterFunc 函数，返回值存储在 timer 属性； WithValue()：构造valueCtx。  在实例化中，有两条主线： (1) 第一条线是context，上面四个类型是一个衔接一个。后期由 removeChild() 负责找到父节点并从其 children 移除该节点。所以其特点就是向上追溯父节点。 Background(a框) \u0026gt; WithCancel(b框) \u0026gt; WithTimeout(c框，超时3秒) \u0026gt; WithTimeout(d框，超时6秒) \u0026gt; WithValue(e框)；需要注意一点的是 d 框不是 timerCtx，因为其超时比父晚，所以会直接生成 cancelCtx。 (2) 第二条线是 children，只有 cancelCtx 和 timerCtx 能串成链，如果上级不是这两个类型，再继续向上上级找，直到找到才挂靠。由 propagateCancel() 和 parentCancelCtx() 这两个函数负责该逻辑。所以其特点是就向下一级一级取消子节点。\n1.3 重要的内部函数  cancel()：无论是超时还是手动取消都会调用，DeadlineExceeded 和 Canceled 分别是超时取消和动手取消； (1) cancelCtx 的 cancel()：关闭 done 通道，cancel “取消树”上的节点（也就是取消所有子节点），判断要不要从父节点上移除； (2) timerCtx 的 cancel()：会先调用 cancelCtx 的 cancel(false, err)，判断要不要从父节点上移除，最后 c.timer.Stop()，这也是建议无论是否设置超时都习惯性调用 defer cancel()，否则 c.timer 计时器到时之前不会被回收。 propagateCancel()：“取消树”的繁衍，或者是“构造”也可以； parentCancelCtx()：propagateCancel()调用时会调用到该函数；从当前节点往上查找到最近的父级 cancelCtx 节点，并作为子节点挂靠在其下面。 removeChild()：从父节点上移除；cancel 是自上而下的，当前的节点 cancel 了，那么其所有子节点也要 cancel，但父节点按原先的逻辑走；先从父节点上移除，等父节点 cancel 的时候就不用再去 cancel 那个已经 cancel 掉的子节点了。  ","id":29,"section":"posts","summary":"整个 context 包原码就有一个文件即 context.go，我估计 context 的设计就是由 http 引出来的，学习 context 其实也是学习 go 的接口设计范式。 1.1 Context 接口 type Context interface { // 取消树","tags":["Golang","Go包"],"title":"go context 包","uri":"https://blog.jemper.cn/2019/03/go-context/","year":"2019"},{"content":"时间概念 时间对所有程序语言来说原理都是一样的：时间点、时间段、时区和显示格式。\n 时间点：时间点没有时区差异，也就是说全世界无论哪个时区都是同一个时间点，时间戳就是时间点的表达方式，所以时间戳没有时区差异。 时间段：时间段其实就是一个整数型，从小到大依次是皮秒（ps）、纳秒（ns）、微秒（μs）、毫秒（ms）、秒（s），在 go 程序中 1s == 1e9。 时区：时间点有时区差异，同一个时间戳在不同时区具有时差区别。 显示格式：这部分没有逻辑问题，纯粹是视图显示。  time 包使用  time.Time：它封装了时间点和时区  type Time struct { // 时间点的实现 wall uint64 ext int64 // 时区的实现 loc *Location } 所以，可以对时间点进行的操作包括（不限于）：\n 取得时间点，Now() 返回 Time，其实它是有时区的；Unix() 返回整数型，没有时区概念； 加减时间段 Add(时间段)，可以使用正负； 减时间点算时间差 Sub(时间点)、After(时间点)、Before(时间点)、Equal(时间点)，注意没有加时间点的方法；另外还提供了 Since(时间点)、Until(时间点) 等函数对参数时间点和当前时间点做减法运算； 设置时区，默认是 local，UTC() 和 In() 等都是设置时区的。  定时相关的接口和函数   Timer 接口，包含有 Stop()、Reset() 方法，包含的函数有 After()，返回通道（阻塞），所以无法提前停止；AfterFunc() ，返回 Timer，后期可以用 Stop() 提前停止；还有 NewTimer() 函数也是返回 Timer，故也可以提前停止； Ticker 接口，也是包含有 Stop() 方法，包含有函数有 Tick() 和 NewTicker()，前者返回通道（阻塞），无法提前停止，如果需要提前停止，只能调用后者，因为它返回 Ticker。   时间段的操作比较简单，暂不举例。\n  格式化的操作也比较简单，暂不举例。\n  ","id":30,"section":"posts","summary":"时间概念 时间对所有程序语言来说原理都是一样的：时间点、时间段、时区和显示格式。 时间点：时间点没有时区差异，也就是说全世界无论哪个时区都是同一","tags":["Golang","Go包"],"title":"go time 包","uri":"https://blog.jemper.cn/2019/03/go-time/","year":"2019"},{"content":"Go Conversions 和 类型断言是两种类型转换方式。\n1 Go Conversions *Point(p) // same as *(Point(p)) (*Point)(p) // p is converted to *Point \u0026lt;-chan int(c) // same as \u0026lt;-(chan int(c)) (\u0026lt;-chan int)(c) // c is converted to \u0026lt;-chan int func()(x) // function signature func() x (func())(x) // x is converted to func() (func() int)(x) // x is converted to func() int func() int(x) // x is converted to func() int (unambiguous) 1 类型断言 type ( Bar interface { Bar() } Foobar interface { Bar() Foo() } fbStr string ) func (fbStr)Foo() {} func (fbStr)Bar() {} func main() { var test fbStr = \u0026quot;test\u0026quot; var a Bar = test //这里可以隐式转换 var b Foobar b = a.(Foobar)\t//这里不是子集，变量需要断言，也可以 b = a.(fbStr) var a2 Foobar = test var b2 Bar b2 = a2\t//这里 Bar 是 Foobar 的子集，不需要断言 fmt.Println(b, b2) } 编译时会检查当前变量所属类型的方法，如果是子集或相同就可以隐式转换\n运行时会检查当前变量的方法，如果是子集或相同就可以断言转换\n总结：可以用类型判断转换的，就可以隐式转换；只能用变量判断转换的，就不可以隐式转换。\n断言还可以断言空方法的接口（比如 interface{}）为简单类型，或者直接用类型转换实现而不用断言\nvar kn interface{} = \u0026quot;test\u0026quot; tkn := kn.(string) fmt.Println(tkn) ","id":31,"section":"posts","summary":"Go Conversions 和 类型断言是两种类型转换方式。 1 Go Conversions *Point(p) // same as *(Point(p)) (*Point)(p) // p is converted to *Point \u0026lt;-chan int(c) // same as \u0026lt;-(chan int(c)) (\u0026lt;-chan int)(c) // c is converted to \u0026lt;-chan int func()(x) // function signature func() x (func())(x) // x is converted to func() (func() int)(x) // x is converted to func() int","tags":["Golang"],"title":"go 类型转换","uri":"https://blog.jemper.cn/2018/08/go-conversions/","year":"2018"},{"content":"今天利用 docker 服务器来搭建 Joomla 公司官网和 Magento 商城。它们都是采用了 php 语言写的应用，所以直接用我相关的文章来配置服务器即可：docker 入门与 docker-compose 编配工具。 说点题外话，如果现在还有小公司在开发什么商城或者官网类型的应用，赶紧放弃，投入到 Joomla 和 Magento 的怀抱吧。这两个已经很成熟，也是开源的，要开发也可以参考这两者或者基于它们去开发，省时省力，也利于成功。\nJoomla 先来看看 Joomla!3x (Joomla! versions 3.5 and later) 推荐的硬件要求：\n PHP (Magic Quotes GPC off) 5.6 + or 7 + MySQL (InnoDB support required) 5.5.3 + Nginx 1.8 +  解压 Joomla 压缩包到 /var/www/joomla，配置目录权限；创建 joomla 数据库。 nginx 配置文件：\nserver { listen 80; server_name kl.jemper.cn; server_name_in_redirect off; access_log /var/log/nginx/joomla.access_log; error_log /var/log/nginx/joomla.error_log info; root /var/www/joomla; index index.php index.html index.htm default.html default.htm; # Support Clean (aka Search Engine Friendly) URLs location / { try_files $uri $uri/ /index.php?$args; } # deny running scripts inside writable directories location ~* /(images|cache|media|logs|tmp)/.*\\.(php|pl|py|jsp|asp|sh|cgi)$ { return 403; error_page 403 /403_error.html; } location ~ \\.php$ { fastcgi_pass alpine; fastcgi_index index.php; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; } # caching of files location ~* \\.(ico|pdf|flv)$ { expires 1y; } location ~* \\.(js|css|png|jpg|jpeg|gif|swf|xml|txt)$ { expires 14d; } } 其安装很简单，直接打开域名就可以了，点击下一步就基本可以安装成功。\nJoomla 的思想 网站结构：  一个网站由各个菜单组成，一个菜单即是一个页面，比如首页页面，业务页面，集团页面，集团下的子页面等等； 一个菜单由各个模块组成，一个模块即是一块内容，比如首页从上到下由“苦干实干提质量 建设一流能源公司”、四栏信息栏、轮播图、码头|供油船|储油库、头版文章列表、产品与服务和辐射区域等模块组成； 一个模块由各种图片，文章，文字组成，比如轮播图由多张图片组成，头版文章列表由文章组成； **所以，管理菜单即是管理网站结构。**后台导航处有菜单下拉列表，分为菜单（这里的菜单即是菜单的分类）和各菜单分类下包含的菜单项。  网站内容：  有了网站结构，相当于建好了一座楼的框架，接下来就是填充内容，内容分为两种：文章和图片。 **所以，管理内容即是管理网站显示的信息。**后台导航处有内容下拉列表，分为分类管理（即文章的分类）、各文章分类下包含的文章（即文章管理）和媒体管理（即是图片管理），其中各篇文章可以标星，就变成了头版文章。  相关的认识  存在一个菜单其菜单项类型是文章列表相关的排版，则该文章分类下的所以有文章都会在该菜单页面中打开。 一套模板，选择某一 layout 或所有，每个 layout 有固定好的 position。  Magento 先来看看 Magento 推荐的硬件要求： Magento 需要一些额外的扩展，在安装的时候进行检测，可以根据提示安装扩展。\nrm -rf /var/www/magento/* tar -xzf Magento-CE-2.1.5_sample_data-2017-02-20-05-42-11.tar.gz -C /var/www/magento/ cp source_zh_CN.csv /var/www/magento/ chmod -R 777 /var/www/magento 语言文件下载 执行\nbin/magento i18n:pack -m replace -d source_zh_CN.csv zh_Hans_CN 然后更新缓存却可，要注意区别 zh_Hans_CN、zh_Hant_HK、zh_Hant_TW\n相关连接 官方文档 Installation stops at about 70%\n","id":32,"section":"posts","summary":"今天利用 docker 服务器来搭建 Joomla 公司官网和 Magento 商城。它们都是采用了 php 语言写的应用，所以直接用我相关的文章来配置服务器即可：docker 入门与 docker-compose 编配工具","tags":["工具"],"title":"Joomla + Magento 官网加商城","uri":"https://blog.jemper.cn/2017/03/joomla-magento/","year":"2017"},{"content":"用习惯了windows下高版本2.11.0的git，再到centos7自带的1.8.5版本觉得操作还是不方便，所以今天就把它升级到最新版。\n必须安装的dependencies\n# yum install gcc perl openssl-devel libcurl-devel expat-devel perl-ExtUtils-MakeMaker 这里不同环境具体也是不同的，缺什么就安装什么。上面只是我在阿里云服务器上安装时缺少的工具和头文件。 可选安装的dependencies，安装文档套件时所需要的，安装说明文档有写到：\n $ make prefix=/usr all doc info ;# as yourself\n# make prefix=/usr install install-doc install-html install-info ;# as root\n\u0026hellip;\nTo build and install documentation suite, you need to have the asciidoc/xmlto toolchain. Because not many people are inclined to install the tools, the default build target (\u0026ldquo;make all\u0026rdquo;) does not build them.\n\u0026quot;make doc\u0026rdquo; builds documentation in man and html formats; there are also \u0026ldquo;make man\u0026rdquo;, \u0026ldquo;make html\u0026rdquo; and \u0026ldquo;make info\u0026rdquo;. Note that \u0026ldquo;make html\u0026rdquo; requires asciidoc, but not xmlto. \u0026ldquo;make man\u0026rdquo; (and thus make doc) requires both.\n\u0026quot;make install-doc\u0026rdquo; installs documentation in man format only; there are also \u0026ldquo;make install-man\u0026rdquo;, \u0026ldquo;make install-html\u0026rdquo; and \u0026ldquo;make install-info\u0026rdquo;.\n 即安装 man 手册，需要 asciidoc 和 xmlto，用以下命令\n# yum install asciidoc.noarch xmlto 接下来就是安装了，我一般都是查网络文档，所以没有安装任何man、info、html文档，下面是安装man文档的命令\n# make prefix=/usr install install-doc 如果不需要man，就不用安装 asciidoc.noarch、xmlto 了。\n官方的git 安装说明\n另外，可参考另一篇写得更详细的文章Git服务器安装详解及安装遇到问题解决方案\n","id":33,"section":"posts","summary":"用习惯了windows下高版本2.11.0的git，再到centos7自带的1.8.5版本觉得操作还是不方便，所以今天就把它升级到最新版。 必","tags":["工具"],"title":"git 源码安装","uri":"https://blog.jemper.cn/2017/02/git-source/","year":"2017"},{"content":"Hexo 是一个很不错的静态博客管理工具的，快速发布，Markdown 支持，插件支持。官网现在列出了100来个主题都很好看，第一篇文章就来写 Hexo 的部署工作吧。 我个人比较喜欢用 Centos，所以我是基于 Centos7 环境来写 Hexo 博客的，Win10 用户可以用 Linux 子系统来部署，这样可以在 Linux 上部署，在 Win10 上写文章；网上也有很多类似的文章，我自己喜欢折腾，大家没事可以粗略看一看。\nHexo 环境  git 和 ssh nodejs  1、Centos7 下 yum 安装的 git 都是1.8^，如果想安装 2.11^ 得用源码安装，可以在这里看关于Git源码安装的文章。ssh避免了每次输入密码，如果是只有一个ssh密钥，也就没有必要用ssh-agent管理工具了，直接放在 ~/.ssh/ 下，使用ssh时默认就会去查找 ~/.ssh/id_rsa（可以先用ssh -T git@github.com测试一下连接）。\n2、node.js 在 Hexo 文档中有介绍，但是我试了安装不了，还是直接到 nvm源 里查看安装方法吧。\nHexo 使用 这部分主要参考官方文档。注意命令可以缩写，比如hexo cl、hexo d，和ip address 缩写成ip a或者ip add一样的道理。\n文档中命令的参数并没有完全写出来，比如hexo server 还有其它的参数，hexo server -i 192.168.2.15 中的参数 -i 可加 ip 地址，用hexo help server 查看完整的命令参数。\n常用命令  hexo init 当前文件夹列出的hexo所有命令跟是否已经在该文件夹init有关系，这点做得挺好的； hexo new 新建文件，会去scaffolds复制模板，如果使用 hexo new drafts \u0026ldquo;article title\u0026rdquo;，就需要用 hexo publish drafts \u0026ldquo;article title\u0026rdquo; 发布； hexo generate 生成静态网站； hexo deploy 根据_config.yml文件的deplay type类型部署静态网站文件到服务器； hexo clean 清除 public，缩写 hexo cl，不能缩写成 hexo c； hexo server 可以在本地浏览器预览，可见即可得，无需 generate，方便样式调试；hexo s \u0026ndash;drafts 可以把草稿变成文章；  分类和标签  tags: [Sports,Baseball]，也可以用 - 数组形式 categories:   多级分类，文章会放在 Sports/Baseball 目录下  categories: - Sports - Baseball categories: [Sports,Baseball] 多个子分类，会分别放在 Sports 和 Baseball 两个目录下  categories: - [Sports] - [Baseball] GitHub Pages Hexo 和 GitHub 是代码和代码库的关系，Hexo 和 GitHub Pages 是代码和服务器的关系。 GitHub 创建一个库，名字一样要是{yourname}.github.io,这样才能正常用{yourname}.github.io域名访问。 在代码库里设置 setting 里的 Options/GitHub Pages 栏配置域名和分支，我这里是用master分支。如果有配置域名，相当于分支下 Create a CNAME file，为了不影响每次 deploy 后被 CNAME 文件删除，安装插件npm install --save hexo-generator-cname3。域名做 cname 记录到{yourname}.github.io。 然后配置 Hexo 根目录下的 _config.yml 的 deploy 属性，type 如果是用 git，相应的就需要安装npm install hexo-deployer-git --save；branch 和 setting 的 master 分支保持一致，有域名的话要加上一行 cname: [YOUR.DOMAIN]。\ndeploy: type: git repo: git@github.com:wpxun/wpxun.github.io.git branch: master message: '站点更新:{{now(\u0026quot;YYYY-MM-DD HH/mm/ss\u0026quot;)}}' cname: jemper.cn 引入 Disqus 评论模块 到 Disqus 网站注册并添加站点，获得 disqus_shortname，在根目录 /_config.yml 中添加行disqus_shortname: yourDisqusShortname，至于怎么在 Disqus 新建站点，网上教程好多。\n引入目录模块 Hexo 已经有 辅助函数，形式: \u0026lt;%- toc(page.content) %\u0026gt;，根据以下的配置，默认就会开启目录，如需关闭，只需要在文章 item（判断了 item.toc） 处设置 toc: false。\n在 /themes/light/layout/_partial/article.ejs 修改 entry 部分为：\n \u0026lt;div class=\u0026quot;entry\u0026quot;\u0026gt; \u0026lt;% if (item.excerpt \u0026amp;\u0026amp; index){ %\u0026gt; \u0026lt;%- item.excerpt %\u0026gt; \u0026lt;% } else { %\u0026gt; \u0026lt;% if (item.toc !== false){ %\u0026gt; \u0026lt;div id=\u0026quot;toc\u0026quot; class=\u0026quot;toc-article\u0026quot;\u0026gt; \u0026lt;%- toc(item.content, {list_number: false}) %\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;%- item.content %\u0026gt; \u0026lt;% } %\u0026gt; \u0026lt;/div\u0026gt; 在 themes/light/source/css/_partial/article.styl 添加：\n.toc-article margin 0.5em border-left 10px solid color-border padding 0.5em strong .toc-article li list-style none 随处写文章解决方案 上面已经有一个 master 主分支，用来放博客静态文件；还需要一个 source 分支，用来放博客的源文件。原理是先拉代码下来，创建空的 source 分支，把博客源文件放进入提交到 source 分支，推到 Github。需要注意的一点是主题不要用git clone https://github.com/ahonn/hexo-theme-even themes/even，而是在其它地方git拉取，再复制到theme目录下，git嵌套不了。操作如下\n$ cd hexo $ hexo clean \u0026amp;\u0026amp; hexo deploy //这是单纯的hexo，deploy到github的wpxun.github.io分支了 $ cd .. $ git clone git@github.com:wpxun/wpxun.github.io.git //在新的目录下拉取下来，之后就在这个分支书写了，上面的 hexo 目录就可以删除了 $ cd wpxun.github.io $ git co --orphan source //创建新分支 $ git rm -rf . $ cp -r ../hexo wpxun.github.io $ hexo clean $ touch .gitignore //里面写上[.deploy_git、public、db.json]这三行 $ git add . $ git ci -am\u0026#34;add source branch\u0026#34; \u0026amp;\u0026amp; git push -u origin source 至此，以后在新的机器上写文章的操作如下：\n  拉取代码切换到 source 分支下；\n  因为 node_modules 跟 node 版本或者操作系统版本有关（这也是.gitignore要排除掉它的原因），所以不同操作系统或不同版本的 node，node_modules 文件夹并不能共用。所以需要在该系统中其它文件夹 hexo init，并安装相应的依赖（npm install --save hexo-generator-cname3，npm install hexo-deployer-git --save）再把 node_modules 文件夹再复制过来\n  $ git clone git@github.com:wpxun/wpxun.github.io.git $ cd wpxun.github.io $ git co source //这步操作后需要进行第2点的复制文件操作，否则hexo不能正常执行，相关操作以上已经说明，就不赘述了 $ hexo new \u0026#34;add-post\u0026#34; $ hexo cl \u0026amp;\u0026amp; hexo d $ hexo cl $ git add . $ git ci -am\u0026#34;add post\u0026#34; \u0026amp;\u0026amp; git push SEO 搜索   验证所有权：一般采用文件验证，把文件直接放在 themes/light/source\n  生成 Sitemap：安装 Baidu/Google Sitemap 插件\n  npm install hexo-generator-baidu-sitemap --save npm install hexo-generator-sitemap --save 修改站点配置文件_config.yml，添加以下内容：\n# 自动生成 sitemap sitemap: path: sitemap.xml baidusitemap: path: baidusitemap.xml ","id":34,"section":"posts","summary":"Hexo 是一个很不错的静态博客管理工具的，快速发布，Markdown 支持，插件支持。官网现在列出了100来个主题都很好看，第一篇文章就来写 Hexo 的部署","tags":null,"title":"博客实践","uri":"https://blog.jemper.cn/2017/02/blog/","year":"2017"}],"tags":[{"title":"C","uri":"https://blog.jemper.cn/tags/c/"},{"title":"Docker","uri":"https://blog.jemper.cn/tags/docker/"},{"title":"Golang","uri":"https://blog.jemper.cn/tags/golang/"},{"title":"Go包","uri":"https://blog.jemper.cn/tags/go%E5%8C%85/"},{"title":"HTTP","uri":"https://blog.jemper.cn/tags/http/"},{"title":"io","uri":"https://blog.jemper.cn/tags/io/"},{"title":"Jenkins","uri":"https://blog.jemper.cn/tags/jenkins/"},{"title":"Kubernetes","uri":"https://blog.jemper.cn/tags/kubernetes/"},{"title":"RPC","uri":"https://blog.jemper.cn/tags/rpc/"},{"title":"Service Mesh","uri":"https://blog.jemper.cn/tags/service-mesh/"},{"title":"Socket","uri":"https://blog.jemper.cn/tags/socket/"},{"title":"TCP","uri":"https://blog.jemper.cn/tags/tcp/"},{"title":"TLS","uri":"https://blog.jemper.cn/tags/tls/"},{"title":"协议","uri":"https://blog.jemper.cn/tags/%E5%8D%8F%E8%AE%AE/"},{"title":"容器","uri":"https://blog.jemper.cn/tags/%E5%AE%B9%E5%99%A8/"},{"title":"密码学","uri":"https://blog.jemper.cn/tags/%E5%AF%86%E7%A0%81%E5%AD%A6/"},{"title":"工具","uri":"https://blog.jemper.cn/tags/%E5%B7%A5%E5%85%B7/"},{"title":"并发","uri":"https://blog.jemper.cn/tags/%E5%B9%B6%E5%8F%91/"},{"title":"架构","uri":"https://blog.jemper.cn/tags/%E6%9E%B6%E6%9E%84/"},{"title":"编程","uri":"https://blog.jemper.cn/tags/%E7%BC%96%E7%A8%8B/"},{"title":"网络","uri":"https://blog.jemper.cn/tags/%E7%BD%91%E7%BB%9C/"},{"title":"调试","uri":"https://blog.jemper.cn/tags/%E8%B0%83%E8%AF%95/"}]}